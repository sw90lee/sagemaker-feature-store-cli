"""Main CLI module for SageMaker FeatureStore Online CLI"""

import click
from typing import Optional
from .config import Config
from .commands import list_cmd, get_cmd, put_cmd, bulk_get_cmd, bulk_put_cmd, clear_cmd, migrate_cmd, create_cmd, delete_cmd, export_cmd, analyze_cmd, add_features_cmd, batch_update_cmd


@click.group()
@click.option('--profile', help='ì‚¬ìš©í•  AWS í”„ë¡œí•„')
@click.option('--region', help='ì‚¬ìš©í•  AWS ë¦¬ì „')
@click.pass_context
def cli(ctx, profile: Optional[str], region: Optional[str]):
    """SageMaker FeatureStore CLI - ì˜¤í”„ë¼ì¸ í”¼ì²˜ ìŠ¤í† ì–´ ì „ìš© ê´€ë¦¬ ë„êµ¬"""
    ctx.ensure_object(dict)
    ctx.obj['config'] = Config(profile=profile, region=region)


@cli.command('list')
@click.option('--output-format', '-o', type=click.Choice(['table', 'json']), default='table',
              help='ì¶œë ¥ í˜•ì‹')
@click.pass_context
def list_feature_groups(ctx, output_format: str):
    """ëª¨ë“  í”¼ì²˜ ê·¸ë£¹ ëª©ë¡ ì¡°íšŒ (ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´)
    
    \b
    ì˜ˆì‹œ:
      # í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
      fs list
      
      # JSON í˜•íƒœë¡œ ì¶œë ¥
      fs list --output-format json
    """
    config = ctx.obj['config']
    list_cmd.list_feature_groups(config, output_format)


@cli.command('get')
@click.argument('feature_group_name')
@click.argument('record_identifier_value')
@click.option('--feature-names', help='ì¡°íšŒí•  í”¼ì²˜ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
@click.option('--output-format', '-o', type=click.Choice(['table', 'json']), default='json',
              help='ì¶œë ¥ í˜•ì‹')
@click.pass_context
def get_record(ctx, feature_group_name: str, record_identifier_value: str, 
               feature_names: Optional[str], output_format: str):
    """ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(Athena)ì—ì„œ ë‹¨ì¼ ë ˆì½”ë“œ ì¡°íšŒ
    
    \b
    ì˜ˆì‹œ:
      # ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì—ì„œ ê¸°ë³¸ ì¡°íšŒ
      fs get my-feature-group record-id-123
      
      # íŠ¹ì • í”¼ì²˜ë§Œ ì¡°íšŒ
      fs get my-feature-group record-id-123 \\
        --feature-names "feature1,feature2,feature3"
      
      # í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥
      fs get my-feature-group record-id-123 --output-format table
    """
    config = ctx.obj['config']
    feature_list = feature_names.split(',') if feature_names else None
    get_cmd.get_record(config, feature_group_name, record_identifier_value, feature_list, output_format)


@cli.command('put')
@click.argument('feature_group_name')
@click.option('--record', required=True, help='ì €ì¥í•  ë ˆì½”ë“œì˜ JSON ë¬¸ìì—´')
@click.pass_context
def put_record(ctx, feature_group_name: str, record: str):
    """ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(S3)ì— ë‹¨ì¼ ë ˆì½”ë“œ ì €ì¥
    
    \b
    ì˜ˆì‹œ:
      # ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì— ë‹¨ì¼ ë ˆì½”ë“œ ì €ì¥
      fs put my-feature-group \\
        --record '{"feature1": "value1", "feature2": "value2", "record_id": "123"}'
    """
    config = ctx.obj['config']
    put_cmd.put_record(config, feature_group_name, record)


@cli.command('bulk-get')
@click.argument('feature_group_name')
@click.argument('input_file')
@click.option('--output-file', '-o', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
@click.option('--feature-names', help='ì¡°íšŒí•  í”¼ì²˜ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
@click.option('--current-time', '-c', is_flag=True, help='ì¡°íšŒëœ ê²°ê³¼ì˜ Time í•„ë“œë¥¼ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ êµì²´')
@click.pass_context
def bulk_get_records(ctx, feature_group_name: str, input_file: str, 
                    output_file: Optional[str], feature_names: Optional[str], current_time: bool):
    """ì…ë ¥ íŒŒì¼(JSON/CSV)ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(Athena)ì—ì„œ ëŒ€ëŸ‰ ë ˆì½”ë“œ ì¡°íšŒ
    
    \b
    ì˜ˆì‹œ:
      # ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì—ì„œ JSON íŒŒì¼ì˜ ë ˆì½”ë“œ ID ëª©ë¡ìœ¼ë¡œ ì¡°íšŒ
      fs bulk-get my-feature-group input_ids.json
      
      # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
      fs bulk-get my-feature-group input_ids.json \\
        --output-file results.json
      
      # CSV íŒŒì¼ ì‚¬ìš©
      fs bulk-get my-feature-group input_ids.csv \\
        --output-file results.csv
      
      # íŠ¹ì • í”¼ì²˜ë§Œ ì¡°íšŒ
      fs bulk-get my-feature-group input_ids.json \\
        --feature-names "feature1,feature2"
      
      # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ Time í•„ë“œ êµì²´
      fs bulk-get my-feature-group input_ids.json --current-time
    """
    config = ctx.obj['config']
    feature_list = feature_names.split(',') if feature_names else None
    bulk_get_cmd.bulk_get_records(config, feature_group_name, input_file, output_file, feature_list, current_time)


@cli.command('bulk-put')
@click.argument('feature_group_name')
@click.argument('input_file')
@click.option('--output-file', '-o', help='ê²°ê³¼ ë¡œê·¸ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ')
@click.option('--batch-size', default=100, help='ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 100, ìµœëŒ€ 1000 ê¶Œì¥)')
@click.pass_context
def bulk_put_records(ctx, feature_group_name: str, input_file: str, output_file: Optional[str], batch_size: int):
    """ì…ë ¥ íŒŒì¼(JSON/CSV)ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(S3)ì— ëŒ€ëŸ‰ ë ˆì½”ë“œ ì €ì¥
    
    \b
    ì˜ˆì‹œ:
      # ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì— JSON íŒŒì¼ì˜ ë ˆì½”ë“œë“¤ì„ ì—…ë¡œë“œ
      fs bulk-put my-feature-group records.json
      
      # CSV íŒŒì¼ ì‚¬ìš©
      fs bulk-put my-feature-group records.csv
      
      # ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
      fs bulk-put my-feature-group records.csv --batch-size 500
      
      # ê²°ê³¼ ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
      fs bulk-put my-feature-group records.json \\
        --output-file logs.txt --batch-size 200
    """
    if batch_size <= 0 or batch_size > 1000:
        click.echo("ë°°ì¹˜ í¬ê¸°ëŠ” 1-1000 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    config = ctx.obj['config']
    bulk_put_cmd.bulk_put_records(config, feature_group_name, input_file, output_file, batch_size)


@cli.command('clear')
@click.argument('feature_group_name')
@click.option('--online-only', is_flag=True, help='ì˜¨ë¼ì¸ ìŠ¤í† ì–´ë§Œ ì‚­ì œ')
@click.option('--offline-only', is_flag=True, help='ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ë§Œ ì‚­ì œ')
@click.option('--force', is_flag=True, help='í™•ì¸ ì—†ì´ ì¦‰ì‹œ ì‚­ì œ')
@click.option('--backup-s3', help='ì‚­ì œ ì „ S3 ë°±ì—… ê²½ë¡œ')
@click.option('--dry-run', is_flag=True, help='ì‹¤ì œ ì‚­ì œ ì—†ì´ ê³„íšë§Œ í™•ì¸')
@click.option('--deduplicate-only', is_flag=True, help='ì‚­ì œ ëŒ€ì‹  ì¤‘ë³µëœ record_idë§Œ ì œê±° (EventTime ê¸°ì¤€ ìµœì‹ ë§Œ ìœ ì§€)')
@click.pass_context
def clear_feature_group(ctx, feature_group_name: str, online_only: bool, offline_only: bool, 
                       force: bool, backup_s3: Optional[str], dry_run: bool, deduplicate_only: bool):
    """í”¼ì²˜ ê·¸ë£¹ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
    
    \b
    ì˜ˆì‹œ:
      # ëª¨ë“  ë°ì´í„° ì‚­ì œ (í™•ì¸ í”„ë¡¬í”„íŠ¸ í¬í•¨)
      fs clear my-feature-group
      
      # ì˜¨ë¼ì¸ ìŠ¤í† ì–´ë§Œ ì‚­ì œ
      fs clear my-feature-group --online-only
      
      # ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ë§Œ ì‚­ì œ
      fs clear my-feature-group --offline-only
      
      # ë°±ì—… í›„ ì‚­ì œ
      fs clear my-feature-group \\
        --backup-s3 s3://my-backup/fg-backup/
      
      # ê°•ì œ ì‚­ì œ (í™•ì¸ ì—†ìŒ)
      fs clear my-feature-group --force
      
      # ê³„íšë§Œ í™•ì¸ (ì‹¤ì œ ì‚­ì œ ì—†ìŒ)
      fs clear my-feature-group --dry-run
      
      # ì¤‘ë³µëœ record_idë§Œ ì œê±° (ì‚­ì œí•˜ì§€ ì•ŠìŒ)
      fs clear my-feature-group --deduplicate-only
      
      # ì¤‘ë³µ ì œê±° ë¯¸ë¦¬ë³´ê¸°
      fs clear my-feature-group --deduplicate-only --dry-run
    """
    if online_only and offline_only:
        click.echo("--online-onlyì™€ --offline-only ì˜µì…˜ì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    config = ctx.obj['config']
    clear_cmd.clear_feature_group(config, feature_group_name, online_only, offline_only, 
                                 force, backup_s3, dry_run, deduplicate_only)


@cli.command('migrate')
@click.argument('source_feature_group')
@click.argument('target_feature_group')
@click.option('--clear-target', is_flag=True, help='íƒ€ê²Ÿ í”¼ì²˜ê·¸ë£¹ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ')
@click.option('--batch-size', default=100, help='ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ì´ì¦ˆ (ê¸°ë³¸: 100)')
@click.option('--max-workers', default=4, help='ë™ì‹œ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 4)')
@click.option('--dry-run', is_flag=True, help='ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì—†ì´ ê³„íšë§Œ í™•ì¸')
@click.option('--filter-query', help='ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë°ì´í„° í•„í„°ë§ (SQL WHERE ì ˆ)')
@click.pass_context
def migrate_feature_group(ctx, source_feature_group: str, target_feature_group: str, 
                         clear_target: bool, batch_size: int, max_workers: int, 
                         dry_run: bool, filter_query: Optional[str]):
    """í”¼ì²˜ ê·¸ë£¹ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
    
    \b
    ì˜ˆì‹œ:
      # ê¸°ë³¸ ë§ˆì´ê·¸ë ˆì´ì…˜
      fs migrate source-fg target-fg
      
      # íƒ€ê²Ÿ ë°ì´í„° ì‚­ì œ í›„ ë§ˆì´ê·¸ë ˆì´ì…˜
      fs migrate source-fg target-fg --clear-target
      
      # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •
      fs migrate source-fg target-fg \\
        --batch-size 50 --max-workers 8
      
      # ê³„íšë§Œ í™•ì¸ (ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì—†ìŒ)
      fs migrate source-fg target-fg --dry-run
      
      # íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„°ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜
      fs migrate source-fg target-fg \\
        --filter-query "WHERE created_date >= '2024-01-01'"
    """
    if batch_size <= 0:
        click.echo("ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    if max_workers <= 0:
        click.echo("ì›Œì»¤ ìˆ˜ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    config = ctx.obj['config']
    migrate_cmd.migrate_feature_group(
        config, source_feature_group, target_feature_group,
        clear_target=clear_target, batch_size=batch_size, 
        max_workers=max_workers, dry_run=dry_run, 
        filter_query=filter_query
    )


# create, delete, export, analyze ëª…ë ¹ì–´ ë“±ë¡
cli.add_command(create_cmd.create)
cli.add_command(delete_cmd.delete)
cli.add_command(export_cmd.export)

@cli.command('analyze')
@click.argument('feature_group_name', required=False)
@click.option('--bucket', help='S3 ë²„í‚· ì´ë¦„')
@click.option('--prefix', help='S3 í”„ë¦¬í”½ìŠ¤ ê²½ë¡œ')
@click.option('--export', help='ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚¼ ê²½ë¡œ')
@click.option('--output-format', '-o', type=click.Choice(['table', 'json']), default='table',
              help='ì¶œë ¥ í˜•ì‹')
@click.pass_context
def analyze_feature_store(ctx, feature_group_name: Optional[str], bucket: Optional[str], 
                         prefix: Optional[str], export: Optional[str], output_format: str):
    """í”¼ì²˜ ìŠ¤í† ì–´ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(S3) ìš©ëŸ‰ ë° ë¹„ìš© ë¶„ì„
    
    âš ï¸  ì£¼ì˜: ì´ ëª…ë ¹ì–´ëŠ” ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(S3)ë§Œ ë¶„ì„í•©ë‹ˆë‹¤. 
         ì˜¨ë¼ì¸ ìŠ¤í† ì–´(DynamoDB)ëŠ” ë¶„ì„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    
    \b
    ì˜ˆì‹œ:
      # íŠ¹ì • í”¼ì²˜ ê·¸ë£¹ì˜ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ë¶„ì„
      fs analyze my-feature-group
      
      # S3 ìœ„ì¹˜ ì§ì ‘ ë¶„ì„
      fs analyze --bucket my-bucket --prefix path/to/data
      
      # ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°
      fs analyze my-feature-group --export analysis_report.csv
      
      # JSON í˜•íƒœë¡œ ì¶œë ¥
      fs analyze my-feature-group --output-format json
    """
    if not feature_group_name and (not bucket or not prefix):
        click.echo("í”¼ì²˜ ê·¸ë£¹ ì´ë¦„ ë˜ëŠ” --bucketê³¼ --prefixë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    config = ctx.obj['config']
    analyze_cmd.analyze_feature_store(config, feature_group_name, bucket, prefix, export, output_format)


@cli.command('add-features')
@click.argument('feature_group_name')
@click.argument('features_file', required=False)
@click.option('--feature', '-f', multiple=True,
              help='Feature ì •ì˜ (í˜•ì‹: name:type[:description] ë˜ëŠ” name:type:list:dimension[:description])')
@click.option('--json', '-j', multiple=True,
              help='JSON í˜•íƒœì˜ feature ì •ì˜')
@click.option('--dry-run', is_flag=True, help='ì‹¤ì œ ë³€ê²½ ì—†ì´ ê³„íšë§Œ í™•ì¸')
@click.option('--wait/--no-wait', default=True, help='ì—…ë°ì´íŠ¸ ì™„ë£Œê¹Œì§€ ëŒ€ê¸° ì—¬ë¶€ (ê¸°ë³¸ê°’: True)')
@click.pass_context
def add_features_command(ctx, feature_group_name: str, features_file: str, feature: tuple, json: tuple, dry_run: bool, wait: bool):
    """í”¼ì²˜ ê·¸ë£¹ì— ìƒˆë¡œìš´ featureë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤ (í†µí•© ëª…ë ¹ì–´).
    
    ì„¸ ê°€ì§€ ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:
    
    \b
    1. JSON íŒŒì¼ ë°©ì‹:
       fs add-features my-fg features.json
    
    \b
    2. CLI í”Œë˜ê·¸ ë°©ì‹:
       fs add-features my-fg -f "name:String:ì„¤ëª…" -f "score:Fractional"
    
    \b
    3. JSON ë¬¸ìì—´ ë°©ì‹:
       fs add-features my-fg -j '{"FeatureName": "name", "FeatureType": "String"}'
    
    \b
    Feature ì •ì˜ í˜•ì‹ (ë²¡í„°/ì§‘í•©ì€ Iceberg ë§Œ ì§€ì›):
      ê¸°ë³¸ í˜•ì‹: name:type[:description]
      ë²¡í„° í˜•ì‹: name:type:list:dimension[:description]  
      ì§‘í•© í˜•ì‹: name:type:set[:description]
    
    \b
    ì˜ˆì‹œ:
      # JSON íŒŒì¼ë¡œ ì¶”ê°€
      fs add-features my-fg new_features.json
      
      # CLI í”Œë˜ê·¸ë¡œ ì¶”ê°€
      fs add-features my-fg -f "user_score:Fractional:ì‚¬ìš©ì ì ìˆ˜" -f "status:String"
      
      # JSON ë¬¸ìì—´ë¡œ ì¶”ê°€
      fs add-features my-fg -j '{"FeatureName": "conversion_rate", "FeatureType": "Fractional"}'
      
      # ë¯¸ë¦¬ë³´ê¸°ë§Œ í™•ì¸
      fs add-features my-fg features.json --dry-run
      
      # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
      fs add-features my-fg -f "new_field:String" --no-wait
    """
    # ì…ë ¥ ë°©ì‹ ê²€ì¦
    input_methods = sum([
        bool(features_file),
        bool(feature),
        bool(json)
    ])
    
    if input_methods == 0:
        click.echo("âŒ Feature ì •ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:", err=True)
        click.echo("  - JSON íŒŒì¼: fs add-features my-fg features.json", err=True)
        click.echo("  - CLI í”Œë˜ê·¸: fs add-features my-fg -f 'name:type:ì„¤ëª…'", err=True)
        click.echo("  - JSON ë¬¸ìì—´: fs add-features my-fg -j '{\"FeatureName\": \"name\", \"FeatureType\": \"String\"}'", err=True)
        raise click.Abort()
    
    if input_methods > 1:
        click.echo("âŒ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì…ë ¥ ë°©ì‹ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", err=True)
        click.echo("  ì‚¬ìš©ëœ ë°©ì‹:", err=True)
        if features_file:
            click.echo(f"    - JSON íŒŒì¼: {features_file}", err=True)
        if feature:
            click.echo(f"    - CLI í”Œë˜ê·¸: {len(feature)}ê°œ", err=True)
        if json:
            click.echo(f"    - JSON ë¬¸ìì—´: {len(json)}ê°œ", err=True)
        raise click.Abort()
    
    # ì…ë ¥ ë°©ì‹ì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ í˜¸ì¶œ
    if features_file:
        # JSON íŒŒì¼ ë°©ì‹
        add_features_cmd.add_features(feature_group_name, features_file, dry_run, wait)
    elif feature:
        # CLI í”Œë˜ê·¸ ë°©ì‹
        add_features_cmd.add_features_from_flags(feature_group_name, list(feature), dry_run, wait)
    elif json:
        # JSON ë¬¸ìì—´ ë°©ì‹
        add_features_cmd.add_features_from_json_strings(feature_group_name, list(json), dry_run, wait)


@cli.command('schema')
@click.argument('feature_group_name', required=False)
@click.option('--output-format', '-o', type=click.Choice(['table', 'json']), default='table',
              help='ì¶œë ¥ í˜•ì‹ (ìŠ¤í‚¤ë§ˆ ì¡°íšŒìš©)')
@click.option('--template', is_flag=True, help='Feature definition í…œí”Œë¦¿ íŒŒì¼ ìƒì„±')
@click.option('--template-output', default='feature_template.json', help='í…œí”Œë¦¿ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
@click.pass_context
def schema_command(ctx, feature_group_name: str, output_format: str, template: bool, template_output: str):
    """í”¼ì²˜ ê·¸ë£¹ì˜ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ë˜ëŠ” í…œí”Œë¦¿ ìƒì„± (í†µí•© ëª…ë ¹ì–´).
    
    ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤:
    
    \b
    1. ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ëª¨ë“œ:
       fs schema <feature-group-name> [ì˜µì…˜]
    
    \b  
    2. í…œí”Œë¦¿ ìƒì„± ëª¨ë“œ:
       fs schema --template [ì˜µì…˜]
    
    \b
    ì˜ˆì‹œ:
      # ìŠ¤í‚¤ë§ˆ ì¡°íšŒ (í…Œì´ë¸” í˜•íƒœ)
      fs schema my-feature-group
      
      # ìŠ¤í‚¤ë§ˆ ì¡°íšŒ (JSON í˜•íƒœ)
      fs schema my-feature-group --output-format json
      
      # í…œí”Œë¦¿ ìƒì„± (ê¸°ë³¸ íŒŒì¼ëª…)
      fs schema --template
      
      # í…œí”Œë¦¿ ìƒì„± (ì‚¬ìš©ì ì •ì˜ íŒŒì¼ëª…)
      fs schema --template --template-output my_features.json
    """
    # ì…ë ¥ ëª¨ë“œ ê²€ì¦
    if template and feature_group_name:
        click.echo("âŒ ìŠ¤í‚¤ë§ˆ ì¡°íšŒì™€ í…œí”Œë¦¿ ìƒì„±ì„ ë™ì‹œì— í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", err=True)
        click.echo("  - ìŠ¤í‚¤ë§ˆ ì¡°íšŒ: fs schema <feature-group-name>", err=True)
        click.echo("  - í…œí”Œë¦¿ ìƒì„±: fs schema --template", err=True)
        raise click.Abort()
    
    if not template and not feature_group_name:
        click.echo("âŒ Feature Group ì´ë¦„ì´ í•„ìš”í•˜ê±°ë‚˜ --template ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.", err=True)
        click.echo("  - ìŠ¤í‚¤ë§ˆ ì¡°íšŒ: fs schema <feature-group-name>", err=True)
        click.echo("  - í…œí”Œë¦¿ ìƒì„±: fs schema --template", err=True)
        raise click.Abort()
    
    # ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ í˜¸ì¶œ
    if template:
        # í…œí”Œë¦¿ ìƒì„± ëª¨ë“œ
        add_features_cmd.generate_feature_template(template_output)
    else:
        # ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ëª¨ë“œ
        add_features_cmd.show_schema(feature_group_name, output_format)


@cli.command('bulk-update')
@click.argument('feature_group_name')
@click.option('--column', required=True, help='ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ëª…')
@click.option('--old-value', help='ë³€ê²½í•  ê¸°ì¡´ ê°’ (ë‹¨ì¼ ê°’ ë³€ê²½ìš©)')
@click.option('--new-value', help='ìƒˆë¡œìš´ ê°’ (ë‹¨ì¼ ê°’ ë³€ê²½ìš©)')
@click.option('--mapping-file', help='ë§¤í•‘ íŒŒì¼ ê²½ë¡œ (.json ë˜ëŠ” .csv)')
@click.option('--conditional-mapping', help='ì¡°ê±´ë¶€ ë§¤í•‘ JSON ë¬¸ìì—´')
@click.option('--transform-function', type=click.Choice(['regex_replace', 'prefix_suffix', 'uppercase', 'lowercase', 'copy_from_column', 'extract_time_prefix']),
              help='ë³€í™˜ í•¨ìˆ˜ íƒ€ì…')
@click.option('--regex-pattern', help='ì •ê·œì‹ íŒ¨í„´ (transform-function=regex_replace ì‹œ í•„ìš”)')
@click.option('--regex-replacement', default='', help='ì •ê·œì‹ ì¹˜í™˜ ë¬¸ìì—´ (ê¸°ë³¸ê°’: ë¹ˆ ë¬¸ìì—´)')
@click.option('--prefix', default='', help='ì ‘ë‘ì‚¬ (transform-function=prefix_suffix ì‹œ)')
@click.option('--suffix', default='', help='ì ‘ë¯¸ì‚¬ (transform-function=prefix_suffix ì‹œ)')
@click.option('--source-column', help='ë³µì‚¬í•  ì›ë³¸ ì»¬ëŸ¼ëª… (transform-function=copy_from_column ì‹œ í•„ìš”)')
@click.option('--prefix-pattern', default=r'(\d{4}-\d{2}-\d{2})', help='ì‹œê°„ ì¶”ì¶œìš© ì •ê·œì‹ íŒ¨í„´ (extract_time_prefix ì‹œ ì‚¬ìš©)')
@click.option('--time-format', default='auto', help='ì‹œê°„ í˜•ì‹ (extract_time_prefix ì‹œ ì‚¬ìš©, ê¸°ë³¸ê°’: auto)')
@click.option('--to-iso/--no-to-iso', default=True, help='ì¶”ì¶œëœ ì‹œê°„ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (extract_time_prefix ì‹œ ì‚¬ìš©)')
@click.option('--dry-run', is_flag=True, default=True, help='ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰ (ê¸°ë³¸ê°’: True)')
@click.option('--no-dry-run', is_flag=True, help='ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸')
@click.option('--skip-validation', is_flag=True, help='Athena ê²€ì¦ ê±´ë„ˆë›°ê¸°')
@click.option('--filter-column', help='ì¶”ê°€ í•„í„° ì»¬ëŸ¼ëª…')
@click.option('--filter-value', help='ì¶”ê°€ í•„í„° ê°’')
@click.option('--filter-null-only', is_flag=True, help='ì§€ì •ëœ ì»¬ëŸ¼ì˜ null ê°’ë§Œ ì—…ë°ì´íŠ¸ (ì„±ëŠ¥ ìµœì í™”)')
@click.option('--cleanup-backups', is_flag=True, help='ë°±ì—… íŒŒì¼ ìë™ ì •ë¦¬')
@click.option('--batch-size', default=1000, help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1000)')
@click.option('--deduplicate/--no-deduplicate', default=True, help='ì¤‘ë³µ record_id ì œê±° (EventTime ê¸°ì¤€ ìµœì‹ ë§Œ ìœ ì§€, ê¸°ë³¸ê°’: True)')
@click.pass_context
def bulk_update_feature_store(ctx, feature_group_name: str, column: str,
                              old_value: Optional[str], new_value: Optional[str],
                              mapping_file: Optional[str], conditional_mapping: Optional[str],
                              transform_function: Optional[str], regex_pattern: Optional[str], 
                              regex_replacement: str, prefix: str, suffix: str, 
                              source_column: Optional[str], prefix_pattern: str,
                              time_format: str, to_iso: bool,
                              dry_run: bool, no_dry_run: bool, skip_validation: bool,
                              filter_column: Optional[str], filter_value: Optional[str],
                              filter_null_only: bool,
                              cleanup_backups: bool, batch_size: int, deduplicate: bool):
    """í”¼ì²˜ ê·¸ë£¹ì˜ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„°ë¥¼ ëŒ€ëŸ‰ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    
    SageMaker Feature Storeì˜ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´(S3 Parquet íŒŒì¼)ì—ì„œ íŠ¹ì • ì»¬ëŸ¼ ê°’ì„ 
    íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸í•˜ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤. Athena ì¿¼ë¦¬ë¥¼ í†µí•œ ìµœì í™”ëœ ì²˜ë¦¬ì™€ 
    ë‹¤ì–‘í•œ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                           ğŸ“‹ ì—…ë°ì´íŠ¸ ë°©ì‹                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    1ï¸âƒ£ ë‹¨ì¼ ê°’ ë³€ê²½:
       fs bulk-update my-fg --column status --old-value "old" --new-value "new" --no-dry-run
    
    \b
    2ï¸âƒ£ ë§¤í•‘ íŒŒì¼ ì‚¬ìš© (JSON/CSV):
       fs bulk-update my-fg --column status --mapping-file mapping.json --no-dry-run
       
       ğŸ“ mapping.json ì˜ˆì‹œ:
       {"ABNORMAL": "NORMAL", "ERROR": "FIXED", "PENDING": "COMPLETED"}
    
    \b
    3ï¸âƒ£ ì¡°ê±´ë¶€ ë§¤í•‘ (ë³µì¡í•œ ì¡°ê±´):
       fs bulk-update my-fg --column result \\
         --conditional-mapping '{"category": {"A": {"old": "new"}}}' --no-dry-run
    
    \b
    4ï¸âƒ£ ë³€í™˜ í•¨ìˆ˜ ì‚¬ìš©:
       fs bulk-update my-fg --column data --transform-function uppercase --no-dry-run
       fs bulk-update my-fg --column text --transform-function regex_replace \\
         --regex-pattern "old_.*" --regex-replacement "new_value" --no-dry-run

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                        ğŸ› ï¸ ë³€í™˜ í•¨ìˆ˜ ìƒì„¸ ê°€ì´ë“œ                        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    ğŸ”¤ extract_time_prefix - íŒŒì¼ëª…/ì»¬ëŸ¼ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ í›„ ISO ë³€í™˜:
       fs bulk-update mlops-fg --column Origin_Time \\
         --transform-function extract_time_prefix \\
         --prefix-pattern '(\d{14})' --source-column Filename \\
         --filter-null-only --no-dry-run
       
       ğŸ’¡ ì„¤ëª…: Filenameì—ì„œ 14ìë¦¬ ìˆ«ì(YYYYMMDDHHMMSS)ë¥¼ ì¶”ì¶œí•˜ì—¬ 
               Origin_Time ì»¬ëŸ¼ì— ISO í˜•ì‹(2024-01-15T10:30:45Z)ìœ¼ë¡œ ì €ì¥
    
    \b
    ğŸ“‹ copy_from_column - ë‹¤ë¥¸ ì»¬ëŸ¼ì—ì„œ ê°’ ë³µì‚¬:
       fs bulk-update my-fg --column target_col \\
         --transform-function copy_from_column --source-column source_col \\
         --filter-null-only --no-dry-run
    
    \b
    ğŸ” regex_replace - ì •ê·œì‹ìœ¼ë¡œ íŒ¨í„´ ì¹˜í™˜:
       fs bulk-update my-fg --column text \\
         --transform-function regex_replace \\
         --regex-pattern "error_(\d+)" --regex-replacement "fixed_\1" --no-dry-run

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                         âš¡ ì„±ëŠ¥ ìµœì í™” ì˜µì…˜                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    ğŸ¯ --filter-null-only: null ê°’ë§Œ ì—…ë°ì´íŠ¸ (Athenaë¡œ ëŒ€ìƒ íŒŒì¼ë§Œ ì„ ë³„)
       fs bulk-update my-fg --column status --new-value "default" \\
         --filter-null-only --no-dry-run
    
    \b
    ğŸ” --filter-column/--filter-value: íŠ¹ì • ì¡°ê±´ ë ˆì½”ë“œë§Œ ëŒ€ìƒ
       fs bulk-update my-fg --column status --old-value "old" --new-value "new" \\
         --filter-column region --filter-value "us-east-1" --no-dry-run
    
    \b
    âš™ï¸ --batch-size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° ì¡°ì • (ê¸°ë³¸: 1000)
       fs bulk-update my-fg --column status --old-value "old" --new-value "new" \\
         --batch-size 500 --no-dry-run

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                      ğŸ“Š ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ë¬´)                         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    ğŸ“… ì˜ˆì‹œ 1: íŒŒì¼ëª…ì—ì„œ ì‹œê°„ ì¶”ì¶œí•˜ì—¬ Origin_Time ì»¬ëŸ¼ì— ì €ì¥
       fs bulk-update mlops-datascience-feature-store-acpoc-faccw-a-cm2d-421 \\
         --column Origin_Time \\
         --transform-function extract_time_prefix \\
         --prefix-pattern '(\d{14})' \\
         --source-column Filename \\
         --filter-null-only \\
         --no-dry-run
       
       ğŸ’¡ ì‹¤ì œ ì ìš©: Filenameì´ "data_20240115103045_v1.parquet"ì¸ ê²½ìš°
                   Origin_Timeì— "2024-01-15T10:30:45Z" ì €ì¥
    
    \b
    ğŸ”„ ì˜ˆì‹œ 2: ìƒíƒœ ê°’ ì¼ê´„ ë³€ê²½ (ë§¤í•‘ íŒŒì¼ ì‚¬ìš©)
       fs bulk-update my-feature-group \\
         --column RB_Result \\
         --mapping-file status_mapping.json \\
         --filter-null-only \\
         --cleanup-backups \\
         --no-dry-run
       
       ğŸ“ status_mapping.json:
       {
         "ABNORMAL": "NORMAL",
         "ERROR": "FIXED", 
         "PENDING": "COMPLETED",
         "null": "DEFAULT"
       }
    
    \b
    ğŸ¯ ì˜ˆì‹œ 3: íŠ¹ì • ì¡°ê±´ì˜ ë ˆì½”ë“œë§Œ ì—…ë°ì´íŠ¸
       fs bulk-update prod-feature-group \\
         --column status --old-value "processing" --new-value "completed" \\
         --filter-column environment --filter-value "production" \\
         --batch-size 2000 \\
         --no-dry-run

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                       ğŸ”§ ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    ğŸ§ª DRY-RUN ëª¨ë“œ (ê¸°ë³¸): ì‹¤ì œ ë³€ê²½ ì—†ì´ ë¶„ì„ ë° ì˜ˆìƒ ì‹œê°„ ë³´ê³ ì„œ
       fs bulk-update my-fg --column status --old-value "old" --new-value "new"
       
       ğŸ“Š ì¶œë ¥: ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜, ì˜ˆìƒ ì†Œìš” ì‹œê°„, ì„¸ë¶€ ë¶„ì„ ë¦¬í¬íŠ¸
    
    \b
    ğŸ” ì‹¤íŒ¨ ë¶„ì„: ì²˜ë¦¬ ì‹¤íŒ¨í•œ íŒŒì¼ì— ëŒ€í•œ ìƒì„¸ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
       failed_files_[feature-group]_[column]_[timestamp].json/txt

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                           âš ï¸ ì¤‘ìš” ì£¼ì˜ì‚¬í•­                            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    \b
    ğŸ”’ ì•ˆì „ì„±:
      â€¢ ê¸°ë³¸ì ìœ¼ë¡œ --dry-run ëª¨ë“œë¡œ ì‹¤í–‰ (ì‹¤ì œ ë³€ê²½ ì—†ìŒ)
      â€¢ ì‹¤ì œ ë³€ê²½ì€ --no-dry-run í”Œë˜ê·¸ í•„ìš”
      â€¢ ë³€ê²½ ì „ ìë™ ë°±ì—… ìƒì„± (ë¡œì»¬ backups/ í´ë”)
      â€¢ EventTime ìë™ ì—…ë°ì´íŠ¸ë¡œ Feature Store ë™ê¸°í™”
    
    \b
    â±ï¸ ì„±ëŠ¥:
      â€¢ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œ ì›Œì»¤)
      â€¢ ì¤‘ë³µ record_id ìë™ ì œê±° (EventTime ê¸°ì¤€ ìµœì‹ ë§Œ ìœ ì§€)
      â€¢ --filter-null-onlyë¡œ Athena ê¸°ë°˜ íŒŒì¼ ì„ ë³„ ìµœì í™”
      â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ê¶Œì¥ (nohup, screen)
    
    \b
    ğŸ’¾ ìŠ¤í† ë¦¬ì§€:
      â€¢ ë°±ì—… íŒŒì¼ì€ ë¡œì»¬ì—ë§Œ ì €ì¥ (S3 ë¹„ìš© ì ˆì•½)
      â€¢ --cleanup-backupsë¡œ S3ì˜ ê¸°ì¡´ _backup_ íŒŒì¼ ì •ë¦¬ ê°€ëŠ¥
      â€¢ ì¶©ë¶„í•œ ë¡œì»¬ ë””ìŠ¤í¬ ê³µê°„ í™•ë³´ í•„ìš”
    """
    # dry-run ë¡œì§ ì²˜ë¦¬
    if no_dry_run:
        dry_run = False
    
    # ì…ë ¥ ê²€ì¦
    input_methods = sum([
        bool(old_value and new_value),
        bool(mapping_file),
        bool(conditional_mapping),
        bool(transform_function)
    ])
    
    if input_methods == 0:
        click.echo("âŒ ì—…ë°ì´íŠ¸ ë°©ì‹ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤:", err=True)
        click.echo("  - ë‹¨ì¼ ê°’: --old-value 'old' --new-value 'new'", err=True)
        click.echo("  - ë§¤í•‘ íŒŒì¼: --mapping-file mapping.json", err=True)
        click.echo("  - ì¡°ê±´ë¶€ ë§¤í•‘: --conditional-mapping '{...}'", err=True)
        click.echo("  - ë³€í™˜ í•¨ìˆ˜: --transform-function regex_replace", err=True)
        raise click.Abort()
    
    if input_methods > 1:
        click.echo("âŒ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì—…ë°ì´íŠ¸ ë°©ì‹ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    if old_value and not new_value:
        click.echo("âŒ --old-valueë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” --new-valueë„ í•„ìš”í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    if new_value and not old_value:
        click.echo("âŒ --new-valueë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” --old-valueë„ í•„ìš”í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    if batch_size <= 0 or batch_size > 10000:
        click.echo("âŒ ë°°ì¹˜ í¬ê¸°ëŠ” 1-10000 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.", err=True)
        raise click.Abort()
    
    # ë³€í™˜ í•¨ìˆ˜ë³„ ì¶”ê°€ ê²€ì¦
    if transform_function:
        if transform_function == 'regex_replace' and not regex_pattern:
            click.echo("âŒ regex_replace ë³€í™˜ì—ëŠ” --regex-patternì´ í•„ìš”í•©ë‹ˆë‹¤.", err=True)
            raise click.Abort()
        
        if transform_function == 'copy_from_column' and not source_column:
            click.echo("âŒ copy_from_column ë³€í™˜ì—ëŠ” --source-columnì´ í•„ìš”í•©ë‹ˆë‹¤.", err=True)
            raise click.Abort()
    
    # ë³€í™˜ í•¨ìˆ˜ ì˜µì…˜ ì¤€ë¹„
    transform_options = {}
    if transform_function:
        if transform_function == 'regex_replace':
            transform_options = {
                'pattern': regex_pattern,
                'replacement': regex_replacement
            }
        elif transform_function == 'prefix_suffix':
            transform_options = {
                'prefix': prefix,
                'suffix': suffix
            }
        elif transform_function == 'copy_from_column':
            transform_options = {
                'source_column': source_column
            }
        elif transform_function == 'extract_time_prefix':
            transform_options = {
                'time_format': time_format,
                'prefix_pattern': prefix_pattern,
                'to_iso': to_iso,
                'source_column': source_column
            }
    
    config = ctx.obj['config']
    batch_update_cmd.batch_update(
        config=config,
        feature_group_name=feature_group_name,
        column_name=column,
        old_value=old_value,
        new_value=new_value,
        mapping_file=mapping_file,
        conditional_mapping=conditional_mapping,
        transform_type=transform_function,
        transform_options=transform_options,
        dry_run=dry_run,
        skip_validation=skip_validation,
        filter_column=filter_column,
        filter_value=filter_value,
        filter_null_only=filter_null_only,
        cleanup_backups=cleanup_backups,
        batch_size=batch_size,
        deduplicate=deduplicate
    )


if __name__ == '__main__':
    cli()