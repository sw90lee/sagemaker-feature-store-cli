"""
Feature Store Export ëª…ë ¹ì–´ êµ¬í˜„
"""
import json
import csv
import gzip
import time
import os
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from urllib.parse import urlparse

import boto3
import click
from tqdm import tqdm

from ..config import Config


@click.command()
@click.argument('feature_group_name')
@click.argument('output_file')
@click.option('--format', '-f', type=click.Choice(['csv', 'json', 'parquet']), default='csv', 
              help='ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: csv)')
@click.option('--limit', type=int, help='ë‚´ë³´ë‚¼ ë ˆì½”ë“œ ìˆ˜ ì œí•œ')
@click.option('--where', help='SQL WHERE ì¡°ê±´ì ˆ ì¶”ê°€')
@click.option('--columns', help='ë‚´ë³´ë‚¼ ì»¬ëŸ¼ ì„ íƒ (ì‰¼í‘œë¡œ êµ¬ë¶„)')
@click.option('--order-by', help='ì •ë ¬ ê¸°ì¤€ ì»¬ëŸ¼ ì§€ì •')
@click.option('--compress/--no-compress', default=False, help='ì••ì¶• ì—¬ë¶€ (ê¸°ë³¸ê°’: False)')
@click.option('--chunk-size', default=10000, help='ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 10000)')
@click.option('--s3-output-location', help='Athena ì¿¼ë¦¬ ê²°ê³¼ ì„ì‹œ ì €ì¥ S3 ìœ„ì¹˜')
@click.option('--database', default='sagemaker_featurestore', help='Athena ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ (ê¸°ë³¸ê°’: sagemaker_featurestore)')
@click.option('--online-compatible', is_flag=True, help='Online store bulk-put í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜')
@click.option('--column-mapping', help='ì»¬ëŸ¼ëª… ë§¤í•‘ ì§€ì • (ì˜ˆ: "event_time:EventTime,customer_id:record_id")')
@click.option('--add-event-time', is_flag=True, help='EventTime í•„ë“œ ìë™ ì¶”ê°€/ë³€í™˜')
@click.option('--record-identifier', help='ë ˆì½”ë“œ ì‹ë³„ì í•„ë“œëª… ì§€ì •')
@click.option('--dry-run', is_flag=True, help='ì‹¤ì œ ë‚´ë³´ë‚´ê¸° ì—†ì´ ì¿¼ë¦¬ ë° ì˜ˆìƒ ê²°ê³¼ë§Œ í‘œì‹œ')
@click.pass_context
def export(
    ctx,
    feature_group_name: str,
    output_file: str,
    format: str,
    limit: Optional[int],
    where: Optional[str],
    columns: Optional[str],
    order_by: Optional[str],
    compress: bool,
    chunk_size: int,
    s3_output_location: Optional[str],
    database: str,
    online_compatible: bool,
    column_mapping: Optional[str],
    add_event_time: bool,
    record_identifier: Optional[str],
    dry_run: bool
):
    """Offline Storeì—ì„œ Feature Group ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°í•©ë‹ˆë‹¤.
    
    FEATURE_GROUP_NAME: ë‚´ë³´ë‚¼ Feature Groupì˜ ì´ë¦„
    OUTPUT_FILE: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    
    \b
    ì˜ˆì‹œ:
      # ê¸°ë³¸ ë‚´ë³´ë‚´ê¸° (CSV í˜•ì‹)
      fs export my-feature-group data.csv
      
      # JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group data.json --format json
      
      # íŠ¹ì • ì»¬ëŸ¼ë§Œ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group data.csv \\
        --columns "customer_id,age,balance"
      
      # ì¡°ê±´ë¶€ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group recent_data.csv \\
        --where "event_time >= '2024-01-01'"
      
      # ìµœëŒ€ 1000ê±´ë§Œ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group sample_data.csv --limit 1000
      
      # ì••ì¶•ëœ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group data.csv.gz --compress
      
      # Online Store í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group online_data.json \\
        --online-compatible
      
      # ì»¬ëŸ¼ëª… ë§¤í•‘í•˜ì—¬ ë‚´ë³´ë‚´ê¸°
      fs export my-feature-group mapped_data.csv \\
        --column-mapping "event_time:EventTime,customer_id:record_id"
      
      # ë‚´ë³´ë‚´ê¸° ê³„íšë§Œ í™•ì¸
      fs export my-feature-group data.csv --dry-run
    """
    try:
        config = ctx.obj['config']
        
        if dry_run:
            click.echo("ğŸ” ë‚´ë³´ë‚´ê¸° ê³„íš í™•ì¸ (Dry Run)")
        else:
            click.echo("ğŸ“¥ Feature Store ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹œì‘...")
        
        # ì˜µì…˜ ê²€ì¦
        _validate_options(feature_group_name, output_file, format, chunk_size, 
                         column_mapping, config)
        
        # Feature Group ë° Offline Store í™•ì¸
        fg_details = _validate_feature_group(config, feature_group_name)
        
        # Athena í…Œì´ë¸” ì´ë¦„ ì°¾ê¸°
        table_name = _find_athena_table_name(config, database, feature_group_name)
        if not table_name:
            raise click.ClickException(f"Feature Group '{feature_group_name}'ì— ëŒ€ì‘í•˜ëŠ” Athena í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        click.echo(f"âœ“ Feature Group ê²€ì¦ ì™„ë£Œ: {feature_group_name}")
        click.echo(f"âœ“ Athena í…Œì´ë¸” í™•ì¸: {database}.{table_name}")
        
        # SQL ì¿¼ë¦¬ ìƒì„±
        query = _build_query(database, table_name, columns, where, order_by, limit)
        click.echo("âœ“ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")
        
        if dry_run:
            _display_dry_run_info(feature_group_name, fg_details, query, output_file, format)
            return
        
        # S3 ì¶œë ¥ ìœ„ì¹˜ ì„¤ì •
        if not s3_output_location:
            s3_output_location = _get_default_s3_output_location(config)
        
        # Athena ì¿¼ë¦¬ ì‹¤í–‰
        result_location = _execute_athena_query(config, query, s3_output_location)
        click.echo(f"âœ“ ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ")
        
        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜
        _process_query_results(
            config=config,
            result_location=result_location,
            output_file=output_file,
            format=format,
            compress=compress,
            online_compatible=online_compatible,
            column_mapping=column_mapping,
            add_event_time=add_event_time,
            record_identifier=record_identifier
        )
        
        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        _display_export_summary(output_file, format, compress)
        
        click.echo("âœ… ë°ì´í„° ë‚´ë³´ë‚´ê¸°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        raise click.ClickException(f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def _validate_options(
    feature_group_name: str,
    output_file: str,
    format: str,
    chunk_size: int,
    column_mapping: Optional[str],
    config: Config
):
    """ì˜µì…˜ ê²€ì¦"""
    
    # ì²­í¬ í¬ê¸° ê²€ì¦
    if chunk_size <= 0:
        raise click.ClickException("ì²­í¬ í¬ê¸°ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ì¶œë ¥ íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = os.path.dirname(os.path.abspath(output_file))
    if output_dir and not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
        except OSError as e:
            raise click.ClickException(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    # ì»¬ëŸ¼ ë§¤í•‘ í˜•ì‹ ê²€ì¦
    if column_mapping:
        try:
            _parse_column_mapping(column_mapping)
        except Exception as e:
            raise click.ClickException(f"ì»¬ëŸ¼ ë§¤í•‘ í˜•ì‹ ì˜¤ë¥˜: {str(e)}")


def _validate_feature_group(config: Config, feature_group_name: str) -> Dict[str, Any]:
    """Feature Group ë° Offline Store ê²€ì¦"""
    try:
        sagemaker_client = config.session.client('sagemaker')
        
        # ë¨¼ì € ëª¨ë“  Feature Group ëª©ë¡ì„ í™•ì¸
        try:
            list_response = sagemaker_client.list_feature_groups(MaxResults=100)
            available_fgs = [fg['FeatureGroupName'] for fg in list_response.get('FeatureGroupSummaries', [])]
            
            if not available_fgs:
                raise click.ClickException(f"ê³„ì •ì— Feature Groupì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Feature Groupì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
            
            # ìœ ì‚¬í•œ ì´ë¦„ ì°¾ê¸°
            similar_names = [fg for fg in available_fgs if feature_group_name.lower() in fg.lower() or fg.lower() in feature_group_name.lower()]
            
        except Exception as e:
            click.echo(f"âš ï¸ Feature Group ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            similar_names = []
        
        response = sagemaker_client.describe_feature_group(
            FeatureGroupName=feature_group_name
        )
        
        # Offline Store í™œì„±í™” í™•ì¸
        if not response.get('OfflineStoreConfig'):
            raise click.ClickException(f"Feature Group '{feature_group_name}'ì— Offline Storeê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        return response
        
    except sagemaker_client.exceptions.ResourceNotFound:
        error_msg = f"Feature Group '{feature_group_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if similar_names:
            error_msg += f"\n\nìœ ì‚¬í•œ ì´ë¦„ì˜ Feature Groupë“¤:"
            for fg_name in similar_names[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                error_msg += f"\n  - {fg_name}"
        elif available_fgs:
            error_msg += f"\n\nì‚¬ìš© ê°€ëŠ¥í•œ Feature Groupë“¤:"
            for fg_name in available_fgs[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                error_msg += f"\n  - {fg_name}"
        
        raise click.ClickException(error_msg)
        
    except Exception as e:
        if "ResourceNotFound" in str(e):
            raise click.ClickException(f"Feature Group '{feature_group_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            raise click.ClickException(f"Feature Group ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def _find_athena_table_name(config: Config, database: str, feature_group_name: str) -> Optional[str]:
    """Feature Groupì— ëŒ€ì‘í•˜ëŠ” Athena í…Œì´ë¸” ì´ë¦„ ì°¾ê¸°"""
    try:
        athena_client = config.session.client('athena')
        
        # ê°€ëŠ¥í•œ í…Œì´ë¸” ì´ë¦„ íŒ¨í„´ë“¤
        account_id = config.session.client('sts').get_caller_identity()['Account']
        table_name_patterns = [
            feature_group_name.replace('-', '_'),
            feature_group_name.lower().replace('-', '_'),
            f"{feature_group_name.replace('-', '_')}_{account_id}",
            f"{feature_group_name.lower().replace('-', '_')}_{account_id}"
        ]
        
        # ì‹¤ì œ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        response = athena_client.list_table_metadata(
            CatalogName='AwsDataCatalog',
            DatabaseName=database
        )
        
        existing_tables = [table['Name'] for table in response.get('TableMetadataList', [])]
        
        # ë””ë²„ê¹…: ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” ëª©ë¡ í‘œì‹œ
        click.echo(f"ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ Athena í…Œì´ë¸” ëª©ë¡:")
        for table_name in existing_tables:
            click.echo(f"  - {table_name}")
        
        click.echo(f"ğŸ” ì‹œë„í•  íŒ¨í„´ë“¤:")
        for pattern in table_name_patterns:
            click.echo(f"  - {pattern}")
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ í…Œì´ë¸” ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
        for pattern in table_name_patterns:
            for table_name in existing_tables:
                if pattern.lower() == table_name.lower():
                    click.echo(f"âœ“ ë§¤ì¹­ëœ í…Œì´ë¸”: {table_name}")
                    return table_name
        
        # ë¶€ë¶„ ë§¤ì¹­ë„ ì‹œë„í•´ë³´ê¸°
        click.echo(f"ğŸ” ë¶€ë¶„ ë§¤ì¹­ ì‹œë„...")
        feature_group_base = feature_group_name.replace('-', '_').lower()
        for table_name in existing_tables:
            if feature_group_base in table_name.lower():
                click.echo(f"âœ“ ë¶€ë¶„ ë§¤ì¹­ëœ í…Œì´ë¸”: {table_name}")
                return table_name
        
        return None
        
    except Exception as e:
        click.echo(f"âš ï¸ Athena í…Œì´ë¸” ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


def _build_query(
    database: str, 
    table_name: str, 
    columns: Optional[str], 
    where: Optional[str], 
    order_by: Optional[str], 
    limit: Optional[int]
) -> str:
    """SQL ì¿¼ë¦¬ ë™ì  ìƒì„±"""
    
    # SELECT ì ˆ
    if columns:
        column_list = [col.strip() for col in columns.split(',')]
        select_clause = ', '.join(column_list)
    else:
        select_clause = '*'
    
    # ê¸°ë³¸ ì¿¼ë¦¬
    query = f'SELECT {select_clause} FROM "{database}"."{table_name}"'
    
    # WHERE ì ˆ
    if where:
        # WHERE í‚¤ì›Œë“œê°€ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        where_clause = where.strip()
        if not where_clause.upper().startswith('WHERE'):
            where_clause = f"WHERE {where_clause}"
        query += f" {where_clause}"
    
    # ORDER BY ì ˆ
    if order_by:
        order_clause = order_by.strip()
        if not order_clause.upper().startswith('ORDER BY'):
            order_clause = f"ORDER BY {order_clause}"
        query += f" {order_clause}"
    
    # LIMIT ì ˆ
    if limit:
        query += f" LIMIT {limit}"
    
    return query


def _get_default_s3_output_location(config: Config) -> str:
    """ê¸°ë³¸ S3 ì¶œë ¥ ìœ„ì¹˜ ì„¤ì •"""
    try:
        # SageMaker ê¸°ë³¸ ë²„í‚· ì‚¬ìš©
        account_id = config.session.client('sts').get_caller_identity()['Account']
        region = config.session.region_name
        bucket_name = f"sagemaker-{region}-{account_id}"
        
        # ë²„í‚· ì¡´ì¬ í™•ì¸
        s3_client = config.session.client('s3')
        try:
            s3_client.head_bucket(Bucket=bucket_name)
            return f"s3://{bucket_name}/athena-results/"
        except:
            pass
        
        # ëŒ€ì²´ ë²„í‚· ì°¾ê¸°
        response = s3_client.list_buckets()
        for bucket in response['Buckets']:
            if 'sagemaker' in bucket['Name'].lower():
                return f"s3://{bucket['Name']}/athena-results/"
        
        # ë§ˆì§€ë§‰ ëŒ€ì•ˆ
        if response['Buckets']:
            return f"s3://{response['Buckets'][0]['Name']}/athena-results/"
        
        raise click.ClickException("ì‚¬ìš© ê°€ëŠ¥í•œ S3 ë²„í‚·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --s3-output-locationì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
        
    except Exception as e:
        raise click.ClickException(f"S3 ì¶œë ¥ ìœ„ì¹˜ ì„¤ì • ì‹¤íŒ¨: {str(e)}")


def _execute_athena_query(config: Config, query: str, s3_output_location: str) -> str:
    """Athena ì¿¼ë¦¬ ì‹¤í–‰"""
    try:
        athena_client = config.session.client('athena')
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        response = athena_client.start_query_execution(
            QueryString=query,
            ResultConfiguration={'OutputLocation': s3_output_location},
            WorkGroup='primary'
        )
        
        query_execution_id = response['QueryExecutionId']
        
        # ì¿¼ë¦¬ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        with tqdm(desc="Athena ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘", unit="ì´ˆ") as pbar:
            while True:
                result = athena_client.get_query_execution(
                    QueryExecutionId=query_execution_id
                )
                
                status = result['QueryExecution']['Status']['State']
                pbar.set_description(f"ìƒíƒœ: {status}")
                
                if status == 'SUCCEEDED':
                    break
                elif status in ['FAILED', 'CANCELLED']:
                    reason = result['QueryExecution']['Status'].get('StateChangeReason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    raise click.ClickException(f"Athena ì¿¼ë¦¬ ì‹¤íŒ¨: {reason}")
                
                time.sleep(2)
                pbar.update(2)
        
        # ê²°ê³¼ ìœ„ì¹˜ ë°˜í™˜
        result_location = result['QueryExecution']['ResultConfiguration']['OutputLocation']
        return result_location
        
    except Exception as e:
        if isinstance(e, click.ClickException):
            raise
        else:
            raise click.ClickException(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def _process_query_results(
    config: Config,
    result_location: str,
    output_file: str,
    format: str,
    compress: bool,
    online_compatible: bool,
    column_mapping: Optional[str],
    add_event_time: bool,
    record_identifier: Optional[str]
):
    """ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë³€í™˜"""
    
    # S3ì—ì„œ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    temp_file = _download_from_s3(config, result_location)
    
    try:
        # ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜
        _convert_and_save_data(
            temp_file=temp_file,
            output_file=output_file,
            format=format,
            compress=compress,
            online_compatible=online_compatible,
            column_mapping=column_mapping,
            add_event_time=add_event_time,
            record_identifier=record_identifier
        )
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_file):
            os.remove(temp_file)


def _download_from_s3(config: Config, s3_url: str) -> str:
    """S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        s3_client = config.session.client('s3')
        parsed_url = urlparse(s3_url)
        bucket_name = parsed_url.netloc
        key = parsed_url.path.lstrip('/')
        
        # ì„ì‹œ íŒŒì¼ëª…
        temp_file = f"/tmp/athena_result_{int(time.time())}.csv"
        
        # ë‹¤ìš´ë¡œë“œ
        with tqdm(desc="S3ì—ì„œ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ì¤‘", unit="B", unit_scale=True) as pbar:
            def progress_callback(bytes_transferred):
                pbar.update(bytes_transferred)
            
            s3_client.download_file(
                bucket_name, key, temp_file,
                Callback=progress_callback
            )
        
        return temp_file
        
    except Exception as e:
        raise click.ClickException(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


def _convert_and_save_data(
    temp_file: str,
    output_file: str,
    format: str,
    compress: bool,
    online_compatible: bool,
    column_mapping: Optional[str],
    add_event_time: bool,
    record_identifier: Optional[str]
):
    """ë°ì´í„° ë³€í™˜ ë° ì €ì¥"""
    
    # CSV ë°ì´í„° ì½ê¸°
    with open(temp_file, 'r', encoding='utf-8') as f:
        csv_reader = csv.DictReader(f)
        rows = list(csv_reader)
    
    if not rows:
        click.echo("âš ï¸ ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Online í˜¸í™˜ì„± ì²˜ë¦¬
    if online_compatible or column_mapping or add_event_time:
        rows = _apply_online_compatibility(rows, column_mapping, add_event_time, record_identifier)
    
    # í˜•ì‹ë³„ ì €ì¥
    if format == 'json':
        _save_as_json(rows, output_file, compress)
    elif format == 'csv':
        _save_as_csv(rows, output_file, compress)
    elif format == 'parquet':
        _save_as_parquet(rows, output_file, compress)


def _apply_online_compatibility(
    rows: List[Dict],
    column_mapping: Optional[str],
    add_event_time: bool,
    record_identifier: Optional[str]
) -> List[Dict]:
    """Online store í˜¸í™˜ì„± ë³€í™˜ ì ìš©"""
    
    # ì»¬ëŸ¼ ë§¤í•‘ íŒŒì‹±
    mapping = {}
    if column_mapping:
        mapping = _parse_column_mapping(column_mapping)
    
    # ê¸°ë³¸ ë§¤í•‘ ê·œì¹™ ì¶”ê°€
    default_mappings = {
        'event_time': 'EventTime',
        'eventtime': 'EventTime',
        'timestamp': 'EventTime',
        'created_at': 'EventTime'
    }
    
    for old_name, new_name in default_mappings.items():
        if old_name not in mapping:
            mapping[old_name] = new_name
    
    processed_rows = []
    current_timestamp = str(int(time.time()))
    
    for row in rows:
        new_row = {}
        
        # ì»¬ëŸ¼ ë§¤í•‘ ì ìš©
        for old_key, value in row.items():
            new_key = mapping.get(old_key, old_key)
            
            # ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (Online store ìš”êµ¬ì‚¬í•­)
            if value is None:
                new_row[new_key] = ""
            else:
                new_row[new_key] = str(value)
        
        # EventTime ìë™ ì¶”ê°€/ë³€í™˜
        if add_event_time and 'EventTime' not in new_row:
            new_row['EventTime'] = current_timestamp
        
        processed_rows.append(new_row)
    
    return processed_rows


def _parse_column_mapping(column_mapping: str) -> Dict[str, str]:
    """ì»¬ëŸ¼ ë§¤í•‘ ë¬¸ìì—´ íŒŒì‹±"""
    mapping = {}
    
    for pair in column_mapping.split(','):
        if ':' not in pair:
            raise ValueError(f"ì˜ëª»ëœ ë§¤í•‘ í˜•ì‹: '{pair}'. 'old_name:new_name' í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        old_name, new_name = pair.split(':', 1)
        mapping[old_name.strip()] = new_name.strip()
    
    return mapping


def _save_as_json(rows: List[Dict], output_file: str, compress: bool):
    """JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    
    def write_json(f):
        for row in tqdm(rows, desc="JSON ì €ì¥ ì¤‘"):
            json.dump(row, f, ensure_ascii=False)
            f.write('\n')
    
    if compress:
        with gzip.open(output_file, 'wt', encoding='utf-8') as f:
            write_json(f)
    else:
        with open(output_file, 'w', encoding='utf-8') as f:
            write_json(f)


def _save_as_csv(rows: List[Dict], output_file: str, compress: bool):
    """CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    if not rows:
        return
    
    fieldnames = rows[0].keys()
    
    def write_csv(f):
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in tqdm(rows, desc="CSV ì €ì¥ ì¤‘"):
            writer.writerow(row)
    
    if compress:
        with gzip.open(output_file, 'wt', encoding='utf-8', newline='') as f:
            write_csv(f)
    else:
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            write_csv(f)


def _save_as_parquet(rows: List[Dict], output_file: str, compress: bool):
    """Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥ (pandas í•„ìš”)"""
    try:
        import pandas as pd
        
        df = pd.DataFrame(rows)
        
        compression = 'gzip' if compress else None
        df.to_parquet(output_file, compression=compression, index=False)
        
    except ImportError:
        raise click.ClickException("Parquet í˜•ì‹ì„ ì‚¬ìš©í•˜ë ¤ë©´ pandasì™€ pyarrowë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤: pip install pandas pyarrow")


def _display_dry_run_info(
    feature_group_name: str,
    fg_details: Dict[str, Any],
    query: str,
    output_file: str,
    format: str
):
    """Dry-run ì •ë³´ í‘œì‹œ"""
    
    click.echo(f"\nFeature Group: {feature_group_name}")
    
    # Offline Store ì •ë³´
    offline_config = fg_details.get('OfflineStoreConfig', {})
    s3_uri = offline_config.get('S3StorageConfig', {}).get('S3Uri', 'N/A')
    table_format = offline_config.get('TableFormat', 'N/A')
    click.echo(f"  - Offline Store: {s3_uri}")
    click.echo(f"  - í…Œì´ë¸” í˜•ì‹: {table_format}")
    click.echo(f"  - ì¶œë ¥ íŒŒì¼: {output_file}")
    
    click.echo(f"\nì‹¤í–‰í•  ì¿¼ë¦¬:")
    click.echo(f"  {query}")
    
    click.echo(f"\nì˜ˆìƒ ê²°ê³¼:")
    click.echo(f"  - ì¶œë ¥ í˜•ì‹: {format}")
    click.echo(f"  - ì˜ˆìƒ íŒŒì¼ í¬ê¸°: ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¦„")
    click.echo(f"  - Athena ì¿¼ë¦¬ ë¹„ìš©: ìŠ¤ìº”ëœ ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¦„")
    
    click.echo(f"\nì‹¤ì œ ë‚´ë³´ë‚´ê¸°ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”.")


def _display_export_summary(output_file: str, format: str, compress: bool):
    """ë‚´ë³´ë‚´ê¸° ìš”ì•½ í‘œì‹œ"""
    try:
        file_size = os.path.getsize(output_file)
        file_size_mb = file_size / (1024 * 1024)
        
        click.echo(f"\nğŸ“‹ ë‚´ë³´ë‚´ê¸° ìš”ì•½:")
        click.echo(f"  - ì¶œë ¥ íŒŒì¼: {output_file}")
        click.echo(f"  - íŒŒì¼ í˜•ì‹: {format.upper()}")
        click.echo(f"  - ì••ì¶• ì—¬ë¶€: {'ì˜ˆ' if compress else 'ì•„ë‹ˆì˜¤'}")
        click.echo(f"  - íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
        
    except Exception as e:
        click.echo(f"âš ï¸ ìš”ì•½ ì •ë³´ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)}")