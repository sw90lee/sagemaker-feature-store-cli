"""Clear command implementation"""

import time
import click
from typing import Dict, Any, List, Optional, Tuple
from botocore.exceptions import ClientError
from urllib.parse import urlparse
from tqdm import tqdm

from ..config import Config


def _find_athena_table_name(config: Config, database: str, feature_group_name: str) -> Optional[str]:
    """Find the actual Athena table name for a feature group"""
    try:
        athena_client = config.session.client('athena')
        
        # List all tables in the database
        response = athena_client.list_table_metadata(
            CatalogName='AwsDataCatalog',
            DatabaseName=database
        )
        
        # Common transformation patterns SageMaker might use
        possible_names = [
            feature_group_name.replace('-', '_'),  # Original logic
            feature_group_name.replace('-', '_').lower(),  # Lowercase
            feature_group_name,  # No transformation
            feature_group_name.lower(),  # Just lowercase
        ]
        
        # Get actual table names
        table_names = [table['Name'] for table in response.get('TableMetadataList', [])]
        
        # Try exact matches first
        for possible_name in possible_names:
            if possible_name in table_names:
                return possible_name
        
        # Try partial matches (in case there are prefixes/suffixes)
        feature_group_base = feature_group_name.replace('-', '_').lower()
        for table_name in table_names:
            if feature_group_base in table_name.lower():
                return table_name
        
        # If no match found, show available tables for debugging
        click.echo(f"âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”: {table_names[:5]}...")  # Show first 5
        return None
        
    except Exception as e:
        click.echo(f"âš ï¸  í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def _get_athena_output_location(config: Config) -> str:
    """Get suitable S3 location for Athena query results"""
    try:
        # Try to use SageMaker default bucket
        session = config.session
        region = session.region_name or 'us-east-1'
        account_id = session.client('sts').get_caller_identity()['Account']
        
        # Try common SageMaker bucket patterns
        bucket_patterns = [
            f"sagemaker-{region}-{account_id}",
            f"aws-athena-query-results-{account_id}-{region}",
            f"sagemaker-studio-{account_id}-{region}"
        ]
        
        s3_client = session.client('s3')
        
        for bucket_name in bucket_patterns:
            try:
                s3_client.head_bucket(Bucket=bucket_name)
                return f"s3://{bucket_name}/athena-results/"
            except:
                continue
        
        # If no bucket found, try to list and use the first available bucket
        response = s3_client.list_buckets()
        if response['Buckets']:
            first_bucket = response['Buckets'][0]['Name']
            return f"s3://{first_bucket}/athena-results/"
        
        # Fallback - this will likely fail but gives a clear error
        return f"s3://sagemaker-{region}-{account_id}/athena-results/"
        
    except Exception as e:
        # Fallback to original
        return 's3://temp-query-results/'


def clear_feature_group(config: Config, feature_group_name: str, online_only: bool = False, 
                       offline_only: bool = False, force: bool = False, 
                       backup_s3: Optional[str] = None, dry_run: bool = False) -> None:
    """Clear all data from a feature group"""
    try:
        # Validate feature group exists and get details
        fg_details = _validate_feature_group(config, feature_group_name)
        
        # Determine what to clear
        clear_online = not offline_only
        clear_offline = not online_only
        
        # Validate options
        if not fg_details.get('OnlineStoreConfig') and clear_online:
            click.echo(f"ê²½ê³ : '{feature_group_name}'ì— ì˜¨ë¼ì¸ ìŠ¤í† ì–´ê°€ êµ¬ì„±ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.", err=True)
            clear_online = False
            
        if not fg_details.get('OfflineStoreConfig') and clear_offline:
            click.echo(f"ê²½ê³ : '{feature_group_name}'ì— ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ê°€ êµ¬ì„±ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.", err=True)
            clear_offline = False
            
        if not clear_online and not clear_offline:
            click.echo("ì‚­ì œí•  ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.", err=True)
            return
            
        # Show plan
        _show_clear_plan(feature_group_name, clear_online, clear_offline, backup_s3)
        
        if dry_run:
            click.echo("\nðŸ” Dry-run ëª¨ë“œ: ì‹¤ì œ ì‚­ì œëŠ” ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
            
        # Confirm deletion
        if not force and not _confirm_deletion(feature_group_name, clear_online, clear_offline):
            click.echo("ìž‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
            
        # Backup if requested
        if backup_s3 and clear_offline:
            click.echo(f"\nðŸ“¦ ì˜¤í”„ë¼ì¸ ë°ì´í„°ë¥¼ {backup_s3}ì— ë°±ì—… ì¤‘...")
            _backup_to_s3(config, feature_group_name, backup_s3, fg_details)
            
        # Execute coordinated clear: offline record IDs â†’ online deletion â†’ offline deletion
        _execute_coordinated_clear(config, feature_group_name, fg_details, clear_online, clear_offline)
            
        click.echo(f"\nâœ… '{feature_group_name}' ë°ì´í„° ì‚­ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except ClientError as e:
        click.echo(f"AWS API ì˜¤ë¥˜: {e}", err=True)
        raise click.Abort()
    except Exception as e:
        click.echo(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", err=True)
        raise click.Abort()


def _validate_feature_group(config: Config, feature_group_name: str) -> Dict[str, Any]:
    """Validate feature group exists and return details"""
    try:
        fg_details = config.sagemaker.describe_feature_group(
            FeatureGroupName=feature_group_name
        )
        return fg_details
    except ClientError as e:
        if e.response['Error']['Code'] == 'ResourceNotFound':
            click.echo(f"í”¼ì²˜ ê·¸ë£¹ '{feature_group_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", err=True)
        else:
            click.echo(f"í”¼ì²˜ ê·¸ë£¹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", err=True)
        raise click.Abort()


def _show_clear_plan(feature_group_name: str, clear_online: bool, clear_offline: bool, backup_s3: Optional[str]) -> None:
    """Show what will be cleared"""
    click.echo(f"\nðŸ“‹ ì‚­ì œ ê³„íš: '{feature_group_name}'")
    click.echo("=" * 50)
    
    if clear_online:
        click.echo("â€¢ ì˜¨ë¼ì¸ ìŠ¤í† ì–´: ëª¨ë“  ë ˆì½”ë“œ ì‚­ì œ")
    if clear_offline:
        click.echo("â€¢ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´: S3 ë°ì´í„° ì‚­ì œ")
    if backup_s3:
        click.echo(f"â€¢ ë°±ì—… ìœ„ì¹˜: {backup_s3}")


def _confirm_deletion(feature_group_name: str, clear_online: bool, clear_offline: bool) -> bool:
    """Ask user confirmation"""
    stores_to_clear = []
    if clear_online:
        stores_to_clear.append("ì˜¨ë¼ì¸ ìŠ¤í† ì–´")
    if clear_offline:
        stores_to_clear.append("ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´")
        
    stores_str = ", ".join(stores_to_clear)
    
    click.echo(f"\nâš ï¸  ê²½ê³ : '{feature_group_name}'ì˜ {stores_str} ë°ì´í„°ê°€ ì™„ì „ížˆ ì‚­ì œë©ë‹ˆë‹¤.")
    click.echo("ì´ ìž‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    return click.confirm("\nì •ë§ë¡œ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")


def _backup_to_s3(config: Config, feature_group_name: str, backup_path: str, fg_details: Dict[str, Any]) -> None:
    """Backup offline store data to S3"""
    if not fg_details.get('OfflineStoreConfig'):
        return
        
    try:
        s3_client = config.session.client('s3')
        
        # Get source S3 path
        source_s3_uri = fg_details['OfflineStoreConfig']['S3StorageConfig']['S3Uri']
        source_bucket, source_prefix = _parse_s3_uri(source_s3_uri)
        
        # Parse backup path
        backup_bucket, backup_prefix = _parse_s3_uri(backup_path)
        if backup_prefix and not backup_prefix.endswith('/'):
            backup_prefix += '/'
        backup_prefix += f"{feature_group_name}/"
        
        # Copy all objects
        paginator = s3_client.get_paginator('list_objects_v2')
        objects_copied = 0
        
        for page in paginator.paginate(Bucket=source_bucket, Prefix=source_prefix):
            if 'Contents' not in page:
                continue
                
            for obj in tqdm(page['Contents'], desc="ë°±ì—… ì¤‘"):
                source_key = obj['Key']
                # Remove source prefix and add backup prefix
                relative_key = source_key[len(source_prefix):].lstrip('/')
                backup_key = backup_prefix + relative_key
                
                copy_source = {'Bucket': source_bucket, 'Key': source_key}
                s3_client.copy_object(
                    CopySource=copy_source,
                    Bucket=backup_bucket,
                    Key=backup_key
                )
                objects_copied += 1
                
        click.echo(f"âœ… {objects_copied}ê°œ ê°ì²´ê°€ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        click.echo(f"ë°±ì—… ì‹¤íŒ¨: {e}", err=True)
        raise


def _execute_coordinated_clear(config: Config, feature_group_name: str, fg_details: Dict[str, Any],
                              clear_online: bool, clear_offline: bool) -> None:
    """Execute coordinated clear: offline record IDs â†’ online deletion â†’ offline deletion"""
    
    record_ids = []
    
    # Step 1: Get all record IDs from offline store (if exists and we need them)
    if fg_details.get('OfflineStoreConfig') and clear_online:
        click.echo("\nðŸ“Š ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì—ì„œ ë ˆì½”ë“œ ID ì¡°íšŒ ì¤‘...")
        record_ids = _get_record_ids_from_offline_athena(config, fg_details)
        click.echo(f"âœ… {len(record_ids):,}ê°œ record ID ì¡°íšŒ ì™„ë£Œ")
    
    # Step 2: Delete online records using the record IDs
    if clear_online:
        click.echo("\nðŸ—‘ï¸  ì˜¨ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        if record_ids:
            _delete_online_records_by_ids(config, feature_group_name, record_ids)
        elif not fg_details.get('OfflineStoreConfig'):
            click.echo("âš ï¸  ì˜¨ë¼ì¸ ì „ìš© í”¼ì²˜ê·¸ë£¹ì€ record ID ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            click.echo("ðŸ’¡ ëŒ€ì•ˆ: bulk-getìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì¡°íšŒí•œ í›„ record IDë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚­ì œí•˜ì„¸ìš”.")
        else:
            click.echo("âš ï¸  Record IDê°€ ì—†ì–´ ì˜¨ë¼ì¸ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # Step 3: Delete offline data
    if clear_offline:
        click.echo("\nðŸ—‘ï¸  ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        _delete_offline_s3_data(config, fg_details)


def _get_record_ids_from_offline_athena(config: Config, fg_details: Dict[str, Any]) -> List[str]:
    """Get all unique record IDs from offline store via Athena"""
    try:
        athena_client = config.session.client('athena')
        
        # Construct query to get unique record IDs
        database_name = "sagemaker_featurestore"
        feature_group_name = fg_details['FeatureGroupName']
        record_id_feature = fg_details['RecordIdentifierFeatureName']
        
        # Find actual table name
        table_name = _find_athena_table_name(config, database_name, feature_group_name)
        if not table_name:
            click.echo(f"âš ï¸  Athena í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {feature_group_name}")
            return []
        
        query = f"SELECT DISTINCT {record_id_feature} FROM {database_name}.{table_name}"
        
        # Execute query
        response = athena_client.start_query_execution(
            QueryString=query,
            ResultConfiguration={
                'OutputLocation': _get_athena_output_location(config)
            }
        )
        
        query_execution_id = response['QueryExecutionId']
        
        # Wait for query completion
        _wait_for_athena_query_completion(athena_client, query_execution_id)
        
        # Get results
        record_ids = []
        paginator = athena_client.get_paginator('get_query_results')
        
        for page in paginator.paginate(QueryExecutionId=query_execution_id):
            rows = page['ResultSet']['Rows'][1:]  # Skip header row
            for row in rows:
                if row['Data'] and row['Data'][0].get('VarCharValue'):
                    record_ids.append(row['Data'][0]['VarCharValue'])
        
        return record_ids
        
    except Exception as e:
        click.echo(f"Athenaë¥¼ í†µí•œ ë ˆì½”ë“œ ID ì¡°íšŒ ì‹¤íŒ¨: {e}", err=True)
        return []


def _delete_online_records_by_ids(config: Config, feature_group_name: str, record_ids: List[str]) -> None:
    """Delete online records by record IDs"""
    if not record_ids:
        click.echo("ì‚­ì œí•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    featurestore_runtime = config.session.client('sagemaker-featurestore-runtime')
    failed_deletions = []
    
    with tqdm(total=len(record_ids), desc="ì˜¨ë¼ì¸ ë ˆì½”ë“œ ì‚­ì œ ì¤‘") as pbar:
        for record_id in record_ids:
            try:
                _handle_throttling(
                    featurestore_runtime.delete_record,
                    FeatureGroupName=feature_group_name,
                    RecordIdentifierValueAsString=str(record_id)
                )
                pbar.update(1)
            except Exception as e:
                failed_deletions.append((record_id, str(e)))
                pbar.update(1)
                
    if failed_deletions:
        click.echo(f"âš ï¸  {len(failed_deletions)}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì‹¤íŒ¨:")
        for record_id, error in failed_deletions[:5]:  # Show first 5 failures
            click.echo(f"  - {record_id}: {error}")
        if len(failed_deletions) > 5:
            click.echo(f"  ... ê·¸ ì™¸ {len(failed_deletions) - 5}ê°œ")
    else:
        click.echo("âœ… ëª¨ë“  ì˜¨ë¼ì¸ ë ˆì½”ë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")


def _delete_offline_s3_data(config: Config, fg_details: Dict[str, Any]) -> None:
    """Delete all data from offline store (S3)"""
    if not fg_details.get('OfflineStoreConfig'):
        return
        
    try:
        s3_client = config.session.client('s3')
        
        # Get S3 path
        s3_uri = fg_details['OfflineStoreConfig']['S3StorageConfig']['S3Uri']
        bucket, prefix = _parse_s3_uri(s3_uri)
        
        # Delete all objects
        paginator = s3_client.get_paginator('list_objects_v2')
        objects_deleted = 0
        
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            if 'Contents' not in page:
                continue
                
            # Delete in batches of 1000 (S3 limit)
            objects = page['Contents']
            for i in tqdm(range(0, len(objects), 1000), desc="S3 ê°ì²´ ì‚­ì œ ì¤‘"):
                batch = objects[i:i+1000]
                delete_objects = [{'Key': obj['Key']} for obj in batch]
                
                s3_client.delete_objects(
                    Bucket=bucket,
                    Delete={'Objects': delete_objects}
                )
                objects_deleted += len(delete_objects)
                
        click.echo(f"âœ… {objects_deleted}ê°œ S3 ê°ì²´ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        click.echo(f"ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ì‚­ì œ ì‹¤íŒ¨: {e}", err=True)
        raise


def _wait_for_athena_query_completion(athena_client, query_execution_id: str) -> None:
    """Wait for Athena query to complete"""
    max_wait_time = 300  # 5 minutes
    wait_time = 0
    
    while wait_time < max_wait_time:
        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
        status = response['QueryExecution']['Status']['State']
        
        if status == 'SUCCEEDED':
            return
        elif status in ['FAILED', 'CANCELLED']:
            reason = response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
            raise Exception(f"Athena ì¿¼ë¦¬ ì‹¤íŒ¨: {reason}")
        
        time.sleep(2)
        wait_time += 2
    
    raise Exception(f"Athena ì¿¼ë¦¬ íƒ€ìž„ì•„ì›ƒ (>{max_wait_time}ì´ˆ)")




def _parse_s3_uri(s3_uri: str) -> Tuple[str, str]:
    """Parse S3 URI into bucket and prefix"""
    parsed = urlparse(s3_uri)
    bucket = parsed.netloc
    prefix = parsed.path.lstrip('/')
    return bucket, prefix


def _handle_throttling(func, *args, **kwargs):
    """Handle API throttling with exponential backoff"""
    max_retries = 5
    base_delay = 1
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except ClientError as e:
            if e.response['Error']['Code'] in ['ThrottlingException', 'TooManyRequestsException']:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    time.sleep(delay)
                    continue
            raise
    
    raise Exception(f"Max retries ({max_retries}) exceeded")