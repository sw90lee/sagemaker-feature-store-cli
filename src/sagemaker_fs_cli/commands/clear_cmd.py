"""Clear command implementation"""

import time
import click
from typing import Dict, Any, List, Optional, Tuple
from botocore.exceptions import ClientError
from urllib.parse import urlparse
from tqdm import tqdm

from ..config import Config


def _find_athena_table_name(config: Config, database: str, feature_group_name: str) -> Optional[str]:
    """Find the actual Athena table name for a feature group"""
    try:
        athena_client = config.session.client('athena')
        
        # List all tables in the database
        response = athena_client.list_table_metadata(
            CatalogName='AwsDataCatalog',
            DatabaseName=database
        )
        
        # Common transformation patterns SageMaker might use
        possible_names = [
            feature_group_name.replace('-', '_'),  # Original logic
            feature_group_name.replace('-', '_').lower(),  # Lowercase
            feature_group_name,  # No transformation
            feature_group_name.lower(),  # Just lowercase
        ]
        
        # Get actual table names
        table_names = [table['Name'] for table in response.get('TableMetadataList', [])]
        
        # Try exact matches first
        for possible_name in possible_names:
            if possible_name in table_names:
                return possible_name
        
        # Try partial matches (in case there are prefixes/suffixes)
        feature_group_base = feature_group_name.replace('-', '_').lower()
        for table_name in table_names:
            if feature_group_base in table_name.lower():
                return table_name
        
        # If no match found, show available tables for debugging
        click.echo(f"âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”: {table_names[:5]}...")  # Show first 5
        return None
        
    except Exception as e:
        click.echo(f"âš ï¸  í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def _get_athena_output_location(config: Config) -> str:
    """Get suitable S3 location for Athena query results"""
    try:
        # Try to use SageMaker default bucket
        session = config.session
        region = session.region_name or 'us-east-1'
        account_id = session.client('sts').get_caller_identity()['Account']
        
        # Try common SageMaker bucket patterns
        bucket_patterns = [
            f"sagemaker-{region}-{account_id}",
            f"aws-athena-query-results-{account_id}-{region}",
            f"sagemaker-studio-{account_id}-{region}"
        ]
        
        s3_client = session.client('s3')
        
        for bucket_name in bucket_patterns:
            try:
                s3_client.head_bucket(Bucket=bucket_name)
                return f"s3://{bucket_name}/athena-results/"
            except:
                continue
        
        # If no bucket found, try to list and use the first available bucket
        response = s3_client.list_buckets()
        if response['Buckets']:
            first_bucket = response['Buckets'][0]['Name']
            return f"s3://{first_bucket}/athena-results/"
        
        # Fallback - this will likely fail but gives a clear error
        return f"s3://sagemaker-{region}-{account_id}/athena-results/"
        
    except Exception as e:
        # Fallback to original
        return 's3://temp-query-results/'


def clear_feature_group(config: Config, feature_group_name: str, online_only: bool = False, 
                       offline_only: bool = False, force: bool = False, 
                       backup_s3: Optional[str] = None, dry_run: bool = False,
                       deduplicate_only: bool = False) -> None:
    """Clear all data from a feature group"""
    try:
        # Validate feature group exists and get details
        fg_details = _validate_feature_group(config, feature_group_name)
        
        # Handle deduplicate-only mode
        if deduplicate_only:
            _deduplicate_feature_group(config, feature_group_name, fg_details, dry_run, force)
            return
        
        # Determine what to clear
        clear_online = not offline_only
        clear_offline = not online_only
        
        # Validate options
        if not fg_details.get('OnlineStoreConfig') and clear_online:
            click.echo(f"ê²½ê³ : '{feature_group_name}'ì— ì˜¨ë¼ì¸ ìŠ¤í† ì–´ê°€ êµ¬ì„±ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.", err=True)
            clear_online = False
            
        if not fg_details.get('OfflineStoreConfig') and clear_offline:
            click.echo(f"ê²½ê³ : '{feature_group_name}'ì— ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ê°€ êµ¬ì„±ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤.", err=True)
            clear_offline = False
            
        if not clear_online and not clear_offline:
            click.echo("ì‚­ì œí•  ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.", err=True)
            return
            
        # Show plan
        _show_clear_plan(feature_group_name, clear_online, clear_offline, backup_s3)
        
        if dry_run:
            click.echo("\nðŸ” Dry-run ëª¨ë“œ: ì‹¤ì œ ì‚­ì œëŠ” ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
            
        # Confirm deletion
        if not force and not _confirm_deletion(feature_group_name, clear_online, clear_offline):
            click.echo("ìž‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
            
        # Backup if requested
        if backup_s3 and clear_offline:
            click.echo(f"\nðŸ“¦ ì˜¤í”„ë¼ì¸ ë°ì´í„°ë¥¼ {backup_s3}ì— ë°±ì—… ì¤‘...")
            _backup_to_s3(config, feature_group_name, backup_s3, fg_details)
            
        # Execute coordinated clear: offline record IDs â†’ online deletion â†’ offline deletion
        _execute_coordinated_clear(config, feature_group_name, fg_details, clear_online, clear_offline)
            
        click.echo(f"\nâœ… '{feature_group_name}' ë°ì´í„° ì‚­ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except ClientError as e:
        click.echo(f"AWS API ì˜¤ë¥˜: {e}", err=True)
        raise click.Abort()
    except Exception as e:
        click.echo(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", err=True)
        raise click.Abort()


def _validate_feature_group(config: Config, feature_group_name: str) -> Dict[str, Any]:
    """Validate feature group exists and return details"""
    try:
        fg_details = config.sagemaker.describe_feature_group(
            FeatureGroupName=feature_group_name
        )
        return fg_details
    except ClientError as e:
        if e.response['Error']['Code'] == 'ResourceNotFound':
            click.echo(f"í”¼ì²˜ ê·¸ë£¹ '{feature_group_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", err=True)
        else:
            click.echo(f"í”¼ì²˜ ê·¸ë£¹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}", err=True)
        raise click.Abort()


def _show_clear_plan(feature_group_name: str, clear_online: bool, clear_offline: bool, backup_s3: Optional[str]) -> None:
    """Show what will be cleared"""
    click.echo(f"\nðŸ“‹ ì‚­ì œ ê³„íš: '{feature_group_name}'")
    click.echo("=" * 50)
    
    if clear_online:
        click.echo("â€¢ ì˜¨ë¼ì¸ ìŠ¤í† ì–´: ëª¨ë“  ë ˆì½”ë“œ ì‚­ì œ")
    if clear_offline:
        click.echo("â€¢ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´: S3 ë°ì´í„° ì‚­ì œ")
    if backup_s3:
        click.echo(f"â€¢ ë°±ì—… ìœ„ì¹˜: {backup_s3}")


def _confirm_deletion(feature_group_name: str, clear_online: bool, clear_offline: bool) -> bool:
    """Ask user confirmation"""
    stores_to_clear = []
    if clear_online:
        stores_to_clear.append("ì˜¨ë¼ì¸ ìŠ¤í† ì–´")
    if clear_offline:
        stores_to_clear.append("ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´")
        
    stores_str = ", ".join(stores_to_clear)
    
    click.echo(f"\nâš ï¸  ê²½ê³ : '{feature_group_name}'ì˜ {stores_str} ë°ì´í„°ê°€ ì™„ì „ížˆ ì‚­ì œë©ë‹ˆë‹¤.")
    click.echo("ì´ ìž‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    
    return click.confirm("\nì •ë§ë¡œ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")


def _backup_to_s3(config: Config, feature_group_name: str, backup_path: str, fg_details: Dict[str, Any]) -> None:
    """Backup offline store data to S3"""
    if not fg_details.get('OfflineStoreConfig'):
        return
        
    try:
        s3_client = config.session.client('s3')
        
        # Get source S3 path
        source_s3_uri = fg_details['OfflineStoreConfig']['S3StorageConfig']['S3Uri']
        source_bucket, source_prefix = _parse_s3_uri(source_s3_uri)
        
        # Parse backup path
        backup_bucket, backup_prefix = _parse_s3_uri(backup_path)
        if backup_prefix and not backup_prefix.endswith('/'):
            backup_prefix += '/'
        backup_prefix += f"{feature_group_name}/"
        
        # Copy all objects
        paginator = s3_client.get_paginator('list_objects_v2')
        objects_copied = 0
        
        for page in paginator.paginate(Bucket=source_bucket, Prefix=source_prefix):
            if 'Contents' not in page:
                continue
                
            for obj in tqdm(page['Contents'], desc="ë°±ì—… ì¤‘"):
                source_key = obj['Key']
                # Remove source prefix and add backup prefix
                relative_key = source_key[len(source_prefix):].lstrip('/')
                backup_key = backup_prefix + relative_key
                
                copy_source = {'Bucket': source_bucket, 'Key': source_key}
                s3_client.copy_object(
                    CopySource=copy_source,
                    Bucket=backup_bucket,
                    Key=backup_key
                )
                objects_copied += 1
                
        click.echo(f"âœ… {objects_copied}ê°œ ê°ì²´ê°€ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        click.echo(f"ë°±ì—… ì‹¤íŒ¨: {e}", err=True)
        raise


def _execute_coordinated_clear(config: Config, feature_group_name: str, fg_details: Dict[str, Any],
                              clear_online: bool, clear_offline: bool) -> None:
    """Execute coordinated clear: offline record IDs â†’ online deletion â†’ offline deletion"""
    
    record_ids = []
    
    # Step 1: Get all record IDs from offline store (if exists and we need them)
    if fg_details.get('OfflineStoreConfig') and clear_online:
        click.echo("\nðŸ“Š ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì—ì„œ ë ˆì½”ë“œ ID ì¡°íšŒ ì¤‘...")
        record_ids = _get_record_ids_from_offline_athena(config, fg_details)
        click.echo(f"âœ… {len(record_ids):,}ê°œ record ID ì¡°íšŒ ì™„ë£Œ")
    
    # Step 2: Delete online records using the record IDs
    if clear_online:
        click.echo("\nðŸ—‘ï¸  ì˜¨ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        if record_ids:
            _delete_online_records_by_ids(config, feature_group_name, record_ids)
        elif not fg_details.get('OfflineStoreConfig'):
            click.echo("âš ï¸  ì˜¨ë¼ì¸ ì „ìš© í”¼ì²˜ê·¸ë£¹ì€ record ID ëª©ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            click.echo("ðŸ’¡ ëŒ€ì•ˆ: bulk-getìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì¡°íšŒí•œ í›„ record IDë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚­ì œí•˜ì„¸ìš”.")
        else:
            click.echo("âš ï¸  Record IDê°€ ì—†ì–´ ì˜¨ë¼ì¸ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # Step 3: Delete offline data
    if clear_offline:
        click.echo("\nðŸ—‘ï¸  ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„° ì‚­ì œ ì¤‘...")
        _delete_offline_s3_data(config, fg_details)


def _get_record_ids_from_offline_athena(config: Config, fg_details: Dict[str, Any]) -> List[str]:
    """Get all unique record IDs from offline store via Athena"""
    try:
        athena_client = config.session.client('athena')
        
        # Construct query to get unique record IDs
        database_name = "sagemaker_featurestore"
        feature_group_name = fg_details['FeatureGroupName']
        record_id_feature = fg_details['RecordIdentifierFeatureName']
        
        # Find actual table name
        table_name = _find_athena_table_name(config, database_name, feature_group_name)
        if not table_name:
            click.echo(f"âš ï¸  Athena í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {feature_group_name}")
            return []
        
        query = f"SELECT DISTINCT {record_id_feature} FROM {database_name}.{table_name}"
        
        # Execute query
        response = athena_client.start_query_execution(
            QueryString=query,
            ResultConfiguration={
                'OutputLocation': _get_athena_output_location(config)
            }
        )
        
        query_execution_id = response['QueryExecutionId']
        
        # Wait for query completion
        _wait_for_athena_query_completion(athena_client, query_execution_id)
        
        # Get results
        record_ids = []
        paginator = athena_client.get_paginator('get_query_results')
        
        for page in paginator.paginate(QueryExecutionId=query_execution_id):
            rows = page['ResultSet']['Rows'][1:]  # Skip header row
            for row in rows:
                if row['Data'] and row['Data'][0].get('VarCharValue'):
                    record_ids.append(row['Data'][0]['VarCharValue'])
        
        return record_ids
        
    except Exception as e:
        click.echo(f"Athenaë¥¼ í†µí•œ ë ˆì½”ë“œ ID ì¡°íšŒ ì‹¤íŒ¨: {e}", err=True)
        return []


def _delete_online_records_by_ids(config: Config, feature_group_name: str, record_ids: List[str]) -> None:
    """Delete online records by record IDs"""
    if not record_ids:
        click.echo("ì‚­ì œí•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    featurestore_runtime = config.session.client('sagemaker-featurestore-runtime')
    failed_deletions = []
    
    with tqdm(total=len(record_ids), desc="ì˜¨ë¼ì¸ ë ˆì½”ë“œ ì‚­ì œ ì¤‘") as pbar:
        for record_id in record_ids:
            try:
                _handle_throttling(
                    featurestore_runtime.delete_record,
                    FeatureGroupName=feature_group_name,
                    RecordIdentifierValueAsString=str(record_id)
                )
                pbar.update(1)
            except Exception as e:
                failed_deletions.append((record_id, str(e)))
                pbar.update(1)
                
    if failed_deletions:
        click.echo(f"âš ï¸  {len(failed_deletions)}ê°œ ë ˆì½”ë“œ ì‚­ì œ ì‹¤íŒ¨:")
        for record_id, error in failed_deletions[:5]:  # Show first 5 failures
            click.echo(f"  - {record_id}: {error}")
        if len(failed_deletions) > 5:
            click.echo(f"  ... ê·¸ ì™¸ {len(failed_deletions) - 5}ê°œ")
    else:
        click.echo("âœ… ëª¨ë“  ì˜¨ë¼ì¸ ë ˆì½”ë“œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")


def _delete_offline_s3_data(config: Config, fg_details: Dict[str, Any]) -> None:
    """Delete all data from offline store (S3)"""
    if not fg_details.get('OfflineStoreConfig'):
        return
        
    try:
        s3_client = config.session.client('s3')
        
        # Get S3 path
        s3_uri = fg_details['OfflineStoreConfig']['S3StorageConfig']['S3Uri']
        bucket, prefix = _parse_s3_uri(s3_uri)
        
        # Delete all objects
        paginator = s3_client.get_paginator('list_objects_v2')
        objects_deleted = 0
        
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            if 'Contents' not in page:
                continue
                
            # Delete in batches of 1000 (S3 limit)
            objects = page['Contents']
            for i in tqdm(range(0, len(objects), 1000), desc="S3 ê°ì²´ ì‚­ì œ ì¤‘"):
                batch = objects[i:i+1000]
                delete_objects = [{'Key': obj['Key']} for obj in batch]
                
                s3_client.delete_objects(
                    Bucket=bucket,
                    Delete={'Objects': delete_objects}
                )
                objects_deleted += len(delete_objects)
                
        click.echo(f"âœ… {objects_deleted}ê°œ S3 ê°ì²´ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        click.echo(f"ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ì‚­ì œ ì‹¤íŒ¨: {e}", err=True)
        raise


def _wait_for_athena_query_completion(athena_client, query_execution_id: str) -> None:
    """Wait for Athena query to complete"""
    max_wait_time = 300  # 5 minutes
    wait_time = 0
    
    while wait_time < max_wait_time:
        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
        status = response['QueryExecution']['Status']['State']
        
        if status == 'SUCCEEDED':
            return
        elif status in ['FAILED', 'CANCELLED']:
            reason = response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
            raise Exception(f"Athena ì¿¼ë¦¬ ì‹¤íŒ¨: {reason}")
        
        time.sleep(2)
        wait_time += 2
    
    raise Exception(f"Athena ì¿¼ë¦¬ íƒ€ìž„ì•„ì›ƒ (>{max_wait_time}ì´ˆ)")




def _parse_s3_uri(s3_uri: str) -> Tuple[str, str]:
    """Parse S3 URI into bucket and prefix"""
    parsed = urlparse(s3_uri)
    bucket = parsed.netloc
    prefix = parsed.path.lstrip('/')
    return bucket, prefix


def _handle_throttling(func, *args, **kwargs):
    """Handle API throttling with exponential backoff"""
    max_retries = 5
    base_delay = 1
    
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except ClientError as e:
            if e.response['Error']['Code'] in ['ThrottlingException', 'TooManyRequestsException']:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    time.sleep(delay)
                    continue
            raise
    
    raise Exception(f"Max retries ({max_retries}) exceeded")


def _deduplicate_feature_group(config: Config, feature_group_name: str, fg_details: Dict[str, Any], 
                              dry_run: bool = False, force: bool = False) -> None:
    """Remove duplicate record_ids keeping only latest EventTime records"""
    import pandas as pd
    from datetime import datetime, timedelta
    import os
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    if not fg_details.get('OfflineStoreConfig'):
        click.echo("âŒ ì¤‘ë³µ ì œê±°ëŠ” ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ê°€ êµ¬ì„±ëœ í”¼ì²˜ ê·¸ë£¹ì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.", err=True)
        return
    
    # Get S3 configuration
    offline_config = fg_details['OfflineStoreConfig']['S3StorageConfig']
    s3_uri = offline_config.get('S3Uri') or offline_config.get('ResolvedOutputS3Uri', '')
    bucket, prefix = _parse_s3_uri(s3_uri)
    
    # Get feature information
    record_id_feature = fg_details['RecordIdentifierFeatureName']
    event_time_feature = fg_details['EventTimeFeatureName']
    
    click.echo(f"\nðŸ”§ ì¤‘ë³µ ì œê±° ìž‘ì—… ì‹œìž‘: '{feature_group_name}'")
    click.echo("=" * 60)
    click.echo(f"Record ID ì»¬ëŸ¼: {record_id_feature}")
    click.echo(f"Event Time ì»¬ëŸ¼: {event_time_feature}")
    click.echo(f"S3 ê²½ë¡œ: {s3_uri}")
    
    if dry_run:
        click.echo("ðŸ” Dry-run ëª¨ë“œ: ì‹¤ì œ ë³€ê²½ì€ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    if not dry_run and not force:
        click.echo(f"\nâš ï¸  ê²½ê³ : '{feature_group_name}'ì—ì„œ ì¤‘ë³µëœ record_idë¥¼ ì œê±°í•©ë‹ˆë‹¤.")
        click.echo("EventTime ê¸°ì¤€ìœ¼ë¡œ ê°€ìž¥ ìµœì‹  ë ˆì½”ë“œë§Œ ìœ ì§€ë˜ë©°, ì˜¤ëž˜ëœ ë ˆì½”ë“œëŠ” ì˜êµ¬ížˆ ì‚­ì œë©ë‹ˆë‹¤!")
        click.echo("ì´ ìž‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        if not click.confirm("\nì •ë§ë¡œ ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            click.echo("ìž‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    try:
        s3_client = config.session.client('s3')
        
        # Get all parquet files
        click.echo("\nðŸ“ Parquet íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        parquet_files = _get_parquet_files(s3_client, bucket, prefix)
        click.echo(f"ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(parquet_files):,}")
        
        if not parquet_files:
            click.echo("ì²˜ë¦¬í•  Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Process files in parallel
        stats = _process_parquet_files_for_dedup(
            s3_client, bucket, parquet_files, record_id_feature, event_time_feature, dry_run
        )
        
        # Show results
        _show_deduplication_results(stats)
        
        if not dry_run:
            click.echo(f"\nâœ… '{feature_group_name}' ì¤‘ë³µ ì œê±°ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        click.echo(f"ì¤‘ë³µ ì œê±° ìž‘ì—… ì‹¤íŒ¨: {e}", err=True)
        raise


def _get_parquet_files(s3_client, bucket: str, prefix: str) -> List[str]:
    """Get all parquet files from S3 path"""
    parquet_files = []
    paginator = s3_client.get_paginator('list_objects_v2')
    
    try:
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    if obj['Key'].endswith('.parquet'):
                        parquet_files.append(obj['Key'])
        return parquet_files
    except Exception as e:
        click.echo(f"S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}", err=True)
        return []


def _process_parquet_files_for_dedup(s3_client, bucket: str, parquet_files: List[str], 
                                   record_id_feature: str, event_time_feature: str, 
                                   dry_run: bool = False) -> Dict:
    """Process parquet files to remove duplicates"""
    import pandas as pd
    from datetime import datetime
    import tempfile
    import time
    
    total_stats = {
        'files_processed': 0,
        'files_modified': 0,
        'total_records_before': 0,
        'total_records_after': 0,
        'duplicates_removed': 0,
        'failed_files': []
    }
    
    def process_single_file(file_key: str) -> Dict:
        temp_file = None
        try:
            # Download file to temporary location
            with tempfile.NamedTemporaryFile(delete=False, suffix='.parquet') as temp_file:
                s3_client.download_fileobj(bucket, file_key, temp_file)
                temp_file_path = temp_file.name
            
            # Read parquet file
            df = pd.read_parquet(temp_file_path, engine='fastparquet')
            original_count = len(df)
            
            # Check if required columns exist
            if record_id_feature not in df.columns or event_time_feature not in df.columns:
                return {
                    'file': file_key,
                    'status': 'skipped',
                    'reason': 'missing_columns',
                    'original_count': original_count,
                    'final_count': original_count,
                    'duplicates_removed': 0
                }
            
            # Remove duplicates by keeping latest EventTime
            df_temp = df.copy()
            df_temp[event_time_feature + '_parsed'] = pd.to_datetime(df_temp[event_time_feature], errors='coerce')
            
            # Keep latest record for each record_id
            latest_records = df_temp.loc[df_temp.groupby(record_id_feature)[event_time_feature + '_parsed'].idxmax()]
            latest_records = latest_records.drop(columns=[event_time_feature + '_parsed'])
            
            final_count = len(latest_records)
            duplicates_removed = original_count - final_count
            
            if duplicates_removed == 0:
                return {
                    'file': file_key,
                    'status': 'no_duplicates',
                    'original_count': original_count,
                    'final_count': final_count,
                    'duplicates_removed': duplicates_removed
                }
            
            if not dry_run:
                # Create backup
                backup_key = file_key.replace('.parquet', f'_backup_{int(time.time())}.parquet')
                copy_source = {'Bucket': bucket, 'Key': file_key}
                s3_client.copy_object(CopySource=copy_source, Bucket=bucket, Key=backup_key)
                
                # Save deduplicated file back to S3
                latest_records.to_parquet(temp_file_path, engine='fastparquet', index=False)
                s3_client.upload_file(temp_file_path, bucket, file_key)
                
                return {
                    'file': file_key,
                    'status': 'deduplicated',
                    'original_count': original_count,
                    'final_count': final_count,
                    'duplicates_removed': duplicates_removed,
                    'backup_key': backup_key
                }
            else:
                return {
                    'file': file_key,
                    'status': 'dry_run',
                    'original_count': original_count,
                    'final_count': final_count,
                    'duplicates_removed': duplicates_removed
                }
                
        except Exception as e:
            return {
                'file': file_key,
                'status': 'error',
                'error': str(e),
                'original_count': 0,
                'final_count': 0,
                'duplicates_removed': 0
            }
        finally:
            # Clean up temp file
            if temp_file and os.path.exists(temp_file.name):
                os.unlink(temp_file.name)
    
    # Process files in parallel
    max_workers = min(os.cpu_count(), 5)
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_file, file_key) for file_key in parquet_files]
        
        with tqdm(total=len(futures), desc='íŒŒì¼ ì²˜ë¦¬ ì¤‘') as pbar:
            for future in as_completed(futures):
                result = future.result()
                
                total_stats['files_processed'] += 1
                total_stats['total_records_before'] += result['original_count']
                total_stats['total_records_after'] += result['final_count']
                total_stats['duplicates_removed'] += result['duplicates_removed']
                
                if result['status'] in ['deduplicated', 'dry_run'] and result['duplicates_removed'] > 0:
                    total_stats['files_modified'] += 1
                elif result['status'] == 'error':
                    total_stats['failed_files'].append(result['file'])
                
                pbar.update(1)
    
    return total_stats


def _show_deduplication_results(stats: Dict) -> None:
    """Show deduplication results"""
    click.echo("\nðŸ“Š ì¤‘ë³µ ì œê±° ê²°ê³¼:")
    click.echo("=" * 40)
    click.echo(f"ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {stats['files_processed']:,}")
    click.echo(f"ë³€ê²½ëœ íŒŒì¼ ìˆ˜: {stats['files_modified']:,}")
    click.echo(f"ì²˜ë¦¬ ì „ ì´ ë ˆì½”ë“œ ìˆ˜: {stats['total_records_before']:,}")
    click.echo(f"ì²˜ë¦¬ í›„ ì´ ë ˆì½”ë“œ ìˆ˜: {stats['total_records_after']:,}")
    click.echo(f"ì œê±°ëœ ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜: {stats['duplicates_removed']:,}")
    
    if stats['duplicates_removed'] > 0:
        reduction_percent = (stats['duplicates_removed'] / stats['total_records_before']) * 100
        click.echo(f"ì¤‘ë³µ ì œê±°ìœ¨: {reduction_percent:.2f}%")
    
    if stats['failed_files']:
        click.echo(f"\nâŒ ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜: {len(stats['failed_files'])}")
        for failed_file in stats['failed_files'][:5]:  # Show first 5
            click.echo(f"  - {failed_file}")
        if len(stats['failed_files']) > 5:
            click.echo(f"  ... ê·¸ ì™¸ {len(stats['failed_files']) - 5}ê°œ")