#!/usr/bin/env python3
"""
SageMaker Feature Store Offline Data Batch Updater

ëŒ€ëŸ‰ì˜ SageMaker Feature Store offline ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- íŠ¹ì • í”¼ì²˜ recordì˜ ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë³€ê²½
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- Athena ì¿¼ë¦¬ ê²€ì¦ í¬í•¨

ì‚¬ìš© ì˜ˆì‹œ:
# 1. ë‹¨ì¼ ê°’ ë³€ê²½
python sagemaker_feature_store_batch_updater.py \
    --feature-group-name "your-feature-group" \
    --update-column "RB_Result" \
    --old-value "old_value" \
    --new-value "new_value" \
    --dry-run

# 2. ë§¤í•‘ íŒŒì¼ë¡œ ì—¬ëŸ¬ ê°’ ë³€ê²½
python sagemaker_feature_store_batch_updater.py \
    --feature-group-name "your-feature-group" \
    --update-column "RB_Result" \
    --mapping-file "value_mapping.json" \
    --dry-run

# 3. ì¡°ê±´ë¶€ ë³€ê²½ (ë‹¤ë¥¸ ì»¬ëŸ¼ ì¡°ê±´)
python sagemaker_feature_store_batch_updater.py \
    --feature-group-name "your-feature-group" \
    --update-column "RB_Result" \
    --conditional-mapping '{"Category": {"A": {"old1": "new1", "old2": "new2"}, "B": {"old3": "new3"}}}' \
    --dry-run
"""

import argparse
import boto3
import pandas as pd
import time
import os
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Union, Callable
import logging
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SageMakerFeatureStoreUpdater:
    """SageMaker Feature Store offline ë°ì´í„° ëŒ€ëŸ‰ ìˆ˜ì • í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 feature_group_name: str,
                 region_name: str = "ap-northeast-2",
                 batch_size: int = 1000):
        """
        ì´ˆê¸°í™”
        
        Args:
            feature_group_name: Feature Group ì´ë¦„
            region_name: AWS ë¦¬ì „
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        """
        self.feature_group_name = feature_group_name
        self.region_name = region_name
        self.batch_size = batch_size
        
        # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.session = boto3.Session(region_name=region_name)
        self.sagemaker_client = self.session.client('sagemaker')
        self.s3_client = self.session.client('s3')
        self.athena_client = self.session.client('athena')
        
        # Feature Group ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.feature_group_info = self._get_feature_group_info()
        self.offline_store_config = self.feature_group_info.get('OfflineStoreConfig', {})
        self.s3_config = self.offline_store_config.get('S3StorageConfig', {})
        self.data_catalog_config = self.offline_store_config.get('DataCatalogConfig', {})
        
        # S3 ê²½ë¡œ ì •ë³´
        self.s3_uri = self.s3_config.get('ResolvedOutputS3Uri', '')
        self.bucket_name, self.prefix = self._parse_s3_uri(self.s3_uri)
        
        # Glue í…Œì´ë¸” ì •ë³´
        self.database_name = self.data_catalog_config.get('Database', 'sagemaker_featurestore')
        self.table_name = self.data_catalog_config.get('TableName', '')
        
        logger.info(f"Feature Group: {feature_group_name}")
        logger.info(f"S3 ê²½ë¡œ: {self.s3_uri}")
        logger.info(f"Glue ë°ì´í„°ë² ì´ìŠ¤: {self.database_name}")
        logger.info(f"Glue í…Œì´ë¸”: {self.table_name}")
    
    def _get_feature_group_info(self) -> Dict:
        """Feature Group ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = self.sagemaker_client.describe_feature_group(
                FeatureGroupName=self.feature_group_name
            )
            return response
        except Exception as e:
            logger.error(f"Feature Group ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    def _parse_s3_uri(self, s3_uri: str) -> tuple:
        """S3 URIë¥¼ ë²„í‚·ê³¼ prefixë¡œ ë¶„ë¦¬"""
        if not s3_uri.startswith('s3://'):
            raise ValueError(f"ì˜ëª»ëœ S3 URI: {s3_uri}")
        
        path = s3_uri[5:]  # s3:// ì œê±°
        parts = path.split('/', 1)
        bucket = parts[0]
        prefix = parts[1] if len(parts) > 1 else ""
        
        return bucket, prefix
    
    def get_offline_data_paths(self) -> List[str]:
        """ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì˜ ëª¨ë“  Parquet íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        logger.info("ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ Parquet íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        
        parquet_files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')
        
        try:
            pages = paginator.paginate(Bucket=self.bucket_name, Prefix=self.prefix)
            
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        if obj['Key'].endswith('.parquet'):
                            s3_path = f"s3://{self.bucket_name}/{obj['Key']}"
                            parquet_files.append(s3_path)
            
            logger.info(f"ë°œê²¬ëœ Parquet íŒŒì¼ ìˆ˜: {len(parquet_files):,}")
            return parquet_files
            
        except Exception as e:
            logger.error(f"S3 íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    def read_data_sample(self, sample_size: int = 1000) -> pd.DataFrame:
        """ë°ì´í„° ìƒ˜í”Œ ì½ê¸° (êµ¬ì¡° í™•ì¸ìš©)"""
        logger.info(f"ë°ì´í„° ìƒ˜í”Œ {sample_size}ê°œ ì½ê¸° ì¤‘...")
        
        parquet_files = self.get_offline_data_paths()
        if not parquet_files:
            logger.warning("Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()
        
        # ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œ ìƒ˜í”Œ ì½ê¸°
        try:
            # pyarrow ì—”ì§„ì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì—”ì§„ ì‚¬ìš©
            try:
                sample_df = pd.read_parquet(parquet_files[0], engine='pyarrow')
            except ImportError:
                try:
                    sample_df = pd.read_parquet(parquet_files[0], engine='fastparquet')
                except ImportError:
                    sample_df = pd.read_parquet(parquet_files[0])
            if len(sample_df) > sample_size:
                sample_df = sample_df.head(sample_size)
            
            logger.info(f"ìƒ˜í”Œ ë°ì´í„° í¬ê¸°: {sample_df.shape}")
            logger.info(f"ì»¬ëŸ¼ ëª©ë¡: {list(sample_df.columns)}")
            return sample_df
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
    
    def load_mapping_from_file(self, mapping_file: str) -> Dict:
        """ë§¤í•‘ íŒŒì¼ì—ì„œ ê°’ ë³€í™˜ ê·œì¹™ ë¡œë“œ"""
        logger.info(f"ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì¤‘: {mapping_file}")
        
        try:
            if mapping_file.endswith('.json'):
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            elif mapping_file.endswith('.csv'):
                df = pd.read_csv(mapping_file)
                if 'old_value' not in df.columns or 'new_value' not in df.columns:
                    raise ValueError("CSV íŒŒì¼ì— 'old_value', 'new_value' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤")
                mapping = dict(zip(df['old_value'], df['new_value']))
            else:
                raise ValueError("ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: .json, .csv")
            
            logger.info(f"ë§¤í•‘ ê·œì¹™ {len(mapping)}ê°œ ë¡œë“œ ì™„ë£Œ")
            return mapping
            
        except Exception as e:
            logger.error(f"ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def create_conditional_mapping(self, conditional_mapping: Union[str, Dict]) -> Dict:
        """ì¡°ê±´ë¶€ ë§¤í•‘ ìƒì„±"""
        if isinstance(conditional_mapping, str):
            try:
                conditional_mapping = json.loads(conditional_mapping)
            except json.JSONDecodeError as e:
                logger.error(f"ì¡°ê±´ë¶€ ë§¤í•‘ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                raise
        
        logger.info(f"ì¡°ê±´ë¶€ ë§¤í•‘ ê·œì¹™: {conditional_mapping}")
        return conditional_mapping
    
    def create_transform_function(self, transform_type: str, **kwargs) -> Callable:
        """ë³€í™˜ í•¨ìˆ˜ ìƒì„±"""
        if transform_type == 'regex_replace':
            pattern = kwargs.get('pattern')
            replacement = kwargs.get('replacement', '')
            if not pattern:
                raise ValueError("ì •ê·œì‹ íŒ¨í„´ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            def regex_transform(value):
                if pd.isna(value):
                    return value
                return re.sub(pattern, replacement, str(value))
            
            return regex_transform
        
        elif transform_type == 'prefix_suffix':
            prefix = kwargs.get('prefix', '')
            suffix = kwargs.get('suffix', '')
            
            def prefix_suffix_transform(value):
                if pd.isna(value):
                    return value
                return f"{prefix}{value}{suffix}"
            
            return prefix_suffix_transform
        
        elif transform_type == 'uppercase':
            def upper_transform(value):
                if pd.isna(value):
                    return value
                return str(value).upper()
            return upper_transform
        
        elif transform_type == 'lowercase':
            def lower_transform(value):
                if pd.isna(value):
                    return value
                return str(value).lower()
            return lower_transform
        
        elif transform_type == 'copy_from_column':
            source_column = kwargs.get('source_column')
            if not source_column:
                raise ValueError("copy_from_columnì—ëŠ” source_column íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            # copy_from_columnì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì†ŒìŠ¤ ì»¬ëŸ¼ ì •ë³´ë¥¼ í•¨ìˆ˜ì— ì €ì¥
            def copy_from_column_transform(row):
                """ë‹¤ë¥¸ ì»¬ëŸ¼ì˜ ê°’ì„ ë³µì‚¬í•˜ëŠ” ë³€í™˜ í•¨ìˆ˜ (row ì „ì²´ë¥¼ ë°›ìŒ)"""
                if hasattr(row, 'index') and source_column in row.index:
                    return row[source_column]
                return None
            
            # ì†ŒìŠ¤ ì»¬ëŸ¼ ì •ë³´ë¥¼ í•¨ìˆ˜ ì†ì„±ìœ¼ë¡œ ì €ì¥
            copy_from_column_transform.source_column = source_column
            copy_from_column_transform.is_copy_function = True
            return copy_from_column_transform
        
        elif transform_type == 'extract_time_prefix':
            time_format = kwargs.get('time_format', 'auto')  # auto, timestamp, date ë“±
            prefix_pattern = kwargs.get('prefix_pattern', r'(\d{4}-\d{2}-\d{2})')  # ê¸°ë³¸ YYYY-MM-DD íŒ¨í„´
            to_iso = kwargs.get('to_iso', True)
            source_column = kwargs.get('source_column')
            
            def extract_time_prefix_transform(row):
                """prefixì—ì„œ ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì„œ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
                if source_column and hasattr(row, 'index') and source_column in row.index:
                    value = row[source_column]
                else:
                    # í˜„ì¬ ì»¬ëŸ¼ì˜ ê°’ì—ì„œ ì¶”ì¶œ
                    value = getattr(row, 'name', '') if hasattr(row, 'name') else str(row)
                
                if pd.isna(value):
                    return None
                
                value_str = str(value)
                
                # ì •ê·œì‹ìœ¼ë¡œ ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ
                match = re.search(prefix_pattern, value_str)
                if not match:
                    # íŒ¨í„´ì´ ë§¤ì¹˜ë˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ ì‹œê°„ì„ ISO í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
                    if to_iso:
                        return datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
                    return None
                
                time_str = match.group(1)
                
                if not to_iso:
                    return time_str
                
                try:
                    # ì‹œê°„ í˜•ì‹ ê°ì§€ ë° ë³€í™˜
                    if time_format == 'auto':
                        # ìë™ í˜•ì‹ ê°ì§€
                        if re.match(r'\d{4}-\d{2}-\d{2}', time_str):
                            dt = datetime.strptime(time_str, '%Y-%m-%d')
                        elif re.match(r'\d{8}', time_str):
                            dt = datetime.strptime(time_str, '%Y%m%d')
                        elif re.match(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', time_str):
                            dt = datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S')
                        elif re.match(r'\d{10}', time_str):  # Unix timestamp
                            dt = datetime.fromtimestamp(int(time_str))
                        else:
                            # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                            dt = datetime.now()
                    else:
                        # ì§€ì •ëœ í˜•ì‹ ì‚¬ìš©
                        dt = datetime.strptime(time_str, time_format)
                    
                    # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')
                    
                except ValueError as e:
                    logger.warning(f"ì‹œê°„ ë³€í™˜ ì‹¤íŒ¨ {time_str}: {e}")
                    # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
                    return datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
            
            # ì†ŒìŠ¤ ì»¬ëŸ¼ ì •ë³´ë¥¼ í•¨ìˆ˜ ì†ì„±ìœ¼ë¡œ ì €ì¥ (í•„ìš”í•œ ê²½ìš°)
            if source_column:
                extract_time_prefix_transform.source_column = source_column
                extract_time_prefix_transform.is_copy_function = True
            
            return extract_time_prefix_transform
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³€í™˜ íƒ€ì…: {transform_type}")
    
    def count_matching_records(self, 
                             column_name: str, 
                             old_value: str = None,
                             value_mapping: Dict = None,
                             conditional_mapping: Dict = None) -> Dict:
        """ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° (ìœ ë™ì  ë§¤í•‘ ì§€ì›)"""
        logger.info(f"ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° ì¤‘... (ì»¬ëŸ¼: {column_name})")
        
        parquet_files = self.get_offline_data_paths()
        result_counts = {'total_files': len(parquet_files), 'match_counts': {}}
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ ê²°ì •
        required_columns = [column_name]
        if conditional_mapping:
            for condition_col in conditional_mapping.keys():
                if condition_col not in required_columns:
                    required_columns.append(condition_col)
        
        def count_in_file(file_path: str) -> Dict:
            try:
                df = pd.read_parquet(file_path, columns=required_columns, engine='fastparquet')
                
                file_counts = {}
                
                if old_value is not None:
                    # ë‹¨ì¼ ê°’ ë§¤ì¹­
                    count = len(df[df[column_name] == old_value])
                    file_counts[old_value] = count
                    
                elif value_mapping:
                    # ë§¤í•‘ íŒŒì¼ ê¸°ë°˜
                    for old_val in value_mapping.keys():
                        count = len(df[df[column_name] == old_val])
                        if count > 0:
                            file_counts[old_val] = count
                            
                elif conditional_mapping:
                    # ì¡°ê±´ë¶€ ë§¤í•‘
                    total_conditional_count = 0
                    for condition_col, condition_mappings in conditional_mapping.items():
                        for condition_val, value_map in condition_mappings.items():
                            condition_mask = df[condition_col] == condition_val
                            for old_val in value_map.keys():
                                mask = condition_mask & (df[column_name] == old_val)
                                count = mask.sum()
                                if count > 0:
                                    key = f"{condition_col}={condition_val}, {column_name}={old_val}"
                                    file_counts[key] = count
                                    total_conditional_count += count
                    
                    if total_conditional_count > 0:
                        file_counts['_total_conditional'] = total_conditional_count
                
                return file_counts
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                return {}
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° íŒŒì¼ì˜ ë§¤ì¹­ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
        max_workers = min(os.cpu_count(), 10)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(count_in_file, file_path) for file_path in parquet_files]
            
            for i, future in enumerate(as_completed(futures)):
                file_counts = future.result()
                
                # ê²°ê³¼ ì§‘ê³„
                for key, count in file_counts.items():
                    if key not in result_counts['match_counts']:
                        result_counts['match_counts'][key] = 0
                    result_counts['match_counts'][key] += count
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % max(1, len(futures) // 10) == 0 or i == len(futures) - 1:
                    progress = (i + 1) / len(futures) * 100
                    logger.info(f"ì§„í–‰ë¥ : {progress:.1f}% ({i+1}/{len(futures)} íŒŒì¼)")
        
        # ê²°ê³¼ ì¶œë ¥
        total_records = sum(result_counts['match_counts'].values())
        logger.info(f"ì´ ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")
        
        if len(result_counts['match_counts']) > 1:
            logger.info("ì„¸ë¶€ ë§¤ì¹­ ê²°ê³¼:")
            for key, count in result_counts['match_counts'].items():
                if key != '_total_conditional' and count > 0:
                    logger.info(f"  - {key}: {count:,}ê°œ")
        
        return result_counts
    
    def update_records_batch(self,
                           column_name: str,
                           old_value: str = None,
                           new_value: str = None,
                           value_mapping: Dict = None,
                           conditional_mapping: Dict = None,
                           transform_function: Callable = None,
                           dry_run: bool = True,
                           filter_conditions: Optional[Dict] = None) -> Dict:
        """
        ë ˆì½”ë“œ ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸ (ìœ ë™ì  ë§¤í•‘ ì§€ì›)
        
        Args:
            column_name: ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ëª…
            old_value: ê¸°ì¡´ ê°’ (ë‹¨ì¼ ê°’ ë³€ê²½ìš©)
            new_value: ìƒˆë¡œìš´ ê°’ (ë‹¨ì¼ ê°’ ë³€ê²½ìš©)
            value_mapping: ê°’ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ {old_val: new_val}
            conditional_mapping: ì¡°ê±´ë¶€ ë§¤í•‘ {condition_col: {condition_val: {old_val: new_val}}}
            transform_function: ë³€í™˜ í•¨ìˆ˜ (ê°’ì„ ë™ì ìœ¼ë¡œ ë³€í™˜)
            dry_run: ì‹¤ì œ ì‹¤í–‰ ì—¬ë¶€ (True: í…ŒìŠ¤íŠ¸ë§Œ, False: ì‹¤ì œ ì‹¤í–‰)
            filter_conditions: ì¶”ê°€ í•„í„° ì¡°ê±´ {'column': 'value'}
        
        Returns:
            ì—…ë°ì´íŠ¸ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹œì‘:")
        logger.info(f"  ì»¬ëŸ¼: {column_name}")
        
        if old_value and new_value:
            logger.info(f"  ë‹¨ì¼ ê°’ ë³€ê²½: '{old_value}' -> '{new_value}'")
        elif value_mapping:
            logger.info(f"  ë§¤í•‘ ê¸°ë°˜ ë³€ê²½: {len(value_mapping)}ê°œ ê·œì¹™")
        elif conditional_mapping:
            logger.info(f"  ì¡°ê±´ë¶€ ë³€ê²½: {len(conditional_mapping)}ê°œ ì¡°ê±´")
        elif transform_function:
            logger.info(f"  í•¨ìˆ˜ ê¸°ë°˜ ë³€í™˜: ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜")
        
        logger.info(f"  DRY RUN: {dry_run}")
        
        if filter_conditions:
            logger.info(f"  ì¶”ê°€ í•„í„°: {filter_conditions}")
        
        parquet_files = self.get_offline_data_paths()
        results = {
            'total_files': len(parquet_files),
            'processed_files': 0,
            'updated_records': 0,
            'failed_files': [],
            'backup_files': []
        }
        
        def process_file(file_path: str) -> Dict:
            """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ìœ ë™ì  ë§¤í•‘ ì§€ì›)"""
            try:
                # íŒŒì¼ ì½ê¸°
                df = pd.read_parquet(file_path, engine='fastparquet')
                original_count = len(df)
                total_update_count = 0
                
                # ëŒ€ìƒ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ìƒˆë¡œ ì¶”ê°€
                if column_name not in df.columns:
                    df[column_name] = None  # ìƒˆ ì»¬ëŸ¼ì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
                
                # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ (ë³€ê²½ ì¶”ì ìš©)
                df_updated = df.copy() if not dry_run else df
                
                # 1. ë‹¨ì¼ ê°’ ë³€ê²½
                if old_value is not None and new_value is not None:
                    mask = df[column_name] == old_value
                    
                    # ì¶”ê°€ í•„í„° ì¡°ê±´ ì ìš©
                    if filter_conditions:
                        for filter_col, filter_val in filter_conditions.items():
                            if filter_col in df.columns:
                                mask = mask & (df[filter_col] == filter_val)
                    
                    update_count = mask.sum()
                    total_update_count += update_count
                    
                    if update_count > 0 and not dry_run:
                        df_updated.loc[mask, column_name] = new_value
                
                # 2. ë§¤í•‘ ê¸°ë°˜ ë³€ê²½
                elif value_mapping:
                    for old_val, new_val in value_mapping.items():
                        mask = df[column_name] == old_val
                        
                        # ì¶”ê°€ í•„í„° ì¡°ê±´ ì ìš©
                        if filter_conditions:
                            for filter_col, filter_val in filter_conditions.items():
                                if filter_col in df.columns:
                                    mask = mask & (df[filter_col] == filter_val)
                        
                        update_count = mask.sum()
                        total_update_count += update_count
                        
                        if update_count > 0 and not dry_run:
                            df_updated.loc[mask, column_name] = new_val
                
                # 3. ì¡°ê±´ë¶€ ë§¤í•‘
                elif conditional_mapping:
                    for condition_col, condition_mappings in conditional_mapping.items():
                        if condition_col not in df.columns:
                            continue
                            
                        for condition_val, value_map in condition_mappings.items():
                            condition_mask = df[condition_col] == condition_val
                            
                            for old_val, new_val in value_map.items():
                                mask = condition_mask & (df[column_name] == old_val)
                                
                                # ì¶”ê°€ í•„í„° ì¡°ê±´ ì ìš©
                                if filter_conditions:
                                    for filter_col, filter_val in filter_conditions.items():
                                        if filter_col in df.columns:
                                            mask = mask & (df[filter_col] == filter_val)
                                
                                update_count = mask.sum()
                                total_update_count += update_count
                                
                                if update_count > 0 and not dry_run:
                                    df_updated.loc[mask, column_name] = new_val
                
                # 4. ë³€í™˜ í•¨ìˆ˜ ê¸°ë°˜
                elif transform_function:
                    mask = pd.Series([True] * len(df), index=df.index)
                    
                    # ì¶”ê°€ í•„í„° ì¡°ê±´ ì ìš©
                    if filter_conditions:
                        for filter_col, filter_val in filter_conditions.items():
                            if filter_col in df.columns:
                                mask = mask & (df[filter_col] == filter_val)
                    
                    if mask.sum() > 0:
                        original_values = df.loc[mask, column_name]
                        
                        # copy_from_column í•¨ìˆ˜ì¸ì§€ í™•ì¸
                        if hasattr(transform_function, 'is_copy_function') and transform_function.is_copy_function:
                            # copy_from_columnì˜ ê²½ìš° í–‰ ì „ì²´ë¥¼ ì „ë‹¬
                            transformed_values = df.loc[mask].apply(transform_function, axis=1)
                        else:
                            # ì¼ë°˜ì ì¸ ë³€í™˜ í•¨ìˆ˜ì˜ ê²½ìš° ì»¬ëŸ¼ ê°’ë§Œ ì „ë‹¬
                            transformed_values = original_values.apply(transform_function)
                        
                        # ì‹¤ì œë¡œ ë³€ê²½ëœ ê°’ë§Œ ì¹´ìš´íŠ¸
                        try:
                            changed_mask = original_values != transformed_values
                            total_update_count = changed_mask.sum()
                        except:
                            # ìƒˆë¡œìš´ ì»¬ëŸ¼ì˜ ê²½ìš° ëª¨ë“  ê°’ì´ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                            total_update_count = len(transformed_values)
                        
                        if total_update_count > 0 and not dry_run:
                            df_updated.loc[mask, column_name] = transformed_values
                
                if total_update_count == 0:
                    return {
                        'file': file_path,
                        'status': 'no_updates',
                        'original_count': original_count,
                        'updated_count': 0
                    }
                
                if not dry_run:
                    # ë°±ì—… ìƒì„±
                    backup_path = file_path.replace('.parquet', f'_backup_{int(time.time())}.parquet')
                    
                    # S3ì—ì„œ ë°±ì—… ë³µì‚¬
                    copy_source = {
                        'Bucket': self.bucket_name,
                        'Key': file_path.replace(f's3://{self.bucket_name}/', '')
                    }
                    backup_key = backup_path.replace(f's3://{self.bucket_name}/', '')
                    
                    self.s3_client.copy_object(
                        CopySource=copy_source,
                        Bucket=self.bucket_name,
                        Key=backup_key
                    )
                    
                    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
                    df_updated.to_parquet(file_path, engine='fastparquet', index=False)
                    
                    return {
                        'file': file_path,
                        'status': 'updated',
                        'original_count': original_count,
                        'updated_count': total_update_count,
                        'backup_path': backup_path
                    }
                else:
                    return {
                        'file': file_path,
                        'status': 'dry_run',
                        'original_count': original_count,
                        'updated_count': total_update_count
                    }
                    
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                return {
                    'file': file_path,
                    'status': 'error',
                    'error': str(e),
                    'original_count': 0,
                    'updated_count': 0
                }
        
        # ë³‘ë ¬ ì²˜ë¦¬
        max_workers = min(os.cpu_count(), 5)  # ë™ì‹œ S3 ì‘ì—… ì œí•œ
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_file, file_path) for file_path in parquet_files]
            
            for i, future in enumerate(as_completed(futures)):
                file_result = future.result()
                results['processed_files'] += 1
                
                if file_result['status'] in ['updated', 'dry_run']:
                    results['updated_records'] += file_result['updated_count']
                    if file_result.get('backup_path'):
                        results['backup_files'].append(file_result['backup_path'])
                elif file_result['status'] == 'error':
                    results['failed_files'].append(file_result['file'])
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % max(1, len(futures) // 20) == 0 or i == len(futures) - 1:
                    progress = (i + 1) / len(futures) * 100
                    logger.info(f"ì²˜ë¦¬ ì§„í–‰ë¥ : {progress:.1f}% ({i+1}/{len(futures)} íŒŒì¼)")
        
        # ê²°ê³¼ ìš”ì•½
        logger.info("=== ì—…ë°ì´íŠ¸ ê²°ê³¼ ===")
        logger.info(f"ì´ íŒŒì¼ ìˆ˜: {results['total_files']:,}")
        logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {results['processed_files']:,}")
        logger.info(f"ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ: {results['updated_records']:,}")
        logger.info(f"ì‹¤íŒ¨í•œ íŒŒì¼: {len(results['failed_files'])}")
        logger.info(f"ë°±ì—… íŒŒì¼: {len(results['backup_files'])}")
        
        if results['failed_files']:
            logger.warning("ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡:")
            for failed_file in results['failed_files'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                logger.warning(f"  - {failed_file}")
        
        return results
    
    def validate_with_athena(self, 
                           column_name: str, 
                           value: str, 
                           expected_count: Optional[int] = None) -> Dict:
        """
        Athena ì¿¼ë¦¬ë¡œ ì—…ë°ì´íŠ¸ ê²°ê³¼ ê²€ì¦
        
        Args:
            column_name: ê²€ì¦í•  ì»¬ëŸ¼ëª…
            value: ê²€ì¦í•  ê°’
            expected_count: ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜ (ì„ íƒì )
        
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"Athena ì¿¼ë¦¬ ê²€ì¦ ì‹œì‘: {column_name} = '{value}'")
        
        if not self.table_name:
            logger.error("Glue í…Œì´ë¸” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'status': 'error', 'message': 'Glue í…Œì´ë¸” ì •ë³´ ì—†ìŒ'}
        
        # Athena ì¿¼ë¦¬ ì‘ì„±
        query = f"""
        SELECT COUNT(*) as record_count
        FROM "{self.database_name}"."{self.table_name}"
        WHERE {column_name} = '{value}'
        """
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ì„¤ì •
        query_execution_id = None
        
        try:
            # Athena ì¿¼ë¦¬ ì‹¤í–‰
            response = self.athena_client.start_query_execution(
                QueryString=query,
                QueryExecutionContext={'Database': self.database_name},
                ResultConfiguration={
                    'OutputLocation': f's3://{self.bucket_name}/athena-query-results/'
                }
            )
            
            query_execution_id = response['QueryExecutionId']
            logger.info(f"Athena ì¿¼ë¦¬ ì‹¤í–‰ ID: {query_execution_id}")
            
            # ì¿¼ë¦¬ ì™„ë£Œ ëŒ€ê¸°
            max_wait_time = 300  # 5ë¶„
            wait_interval = 5
            elapsed_time = 0
            
            while elapsed_time < max_wait_time:
                response = self.athena_client.get_query_execution(
                    QueryExecutionId=query_execution_id
                )
                
                status = response['QueryExecution']['Status']['State']
                
                if status == 'SUCCEEDED':
                    break
                elif status in ['FAILED', 'CANCELLED']:
                    error_msg = response['QueryExecution']['Status'].get('StateChangeReason', 'Unknown error')
                    logger.error(f"Athena ì¿¼ë¦¬ ì‹¤íŒ¨: {error_msg}")
                    return {'status': 'error', 'message': error_msg}
                
                time.sleep(wait_interval)
                elapsed_time += wait_interval
                logger.info(f"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... ({elapsed_time}ì´ˆ ê²½ê³¼)")
            
            if elapsed_time >= max_wait_time:
                logger.error("Athena ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ")
                return {'status': 'timeout', 'message': 'Query timeout'}
            
            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            result = self.athena_client.get_query_results(
                QueryExecutionId=query_execution_id
            )
            
            # ê²°ê³¼ íŒŒì‹±
            rows = result['ResultSet']['Rows']
            if len(rows) >= 2:  # í—¤ë” + ë°ì´í„°
                actual_count = int(rows[1]['Data'][0]['VarCharValue'])
                
                validation_result = {
                    'status': 'success',
                    'actual_count': actual_count,
                    'query_execution_id': query_execution_id
                }
                
                if expected_count is not None:
                    validation_result['expected_count'] = expected_count
                    validation_result['matches_expected'] = actual_count == expected_count
                    
                    if actual_count == expected_count:
                        logger.info(f"âœ… ê²€ì¦ ì„±ê³µ: {actual_count:,}ê°œ ë ˆì½”ë“œ")
                    else:
                        logger.warning(f"âš ï¸ ê²€ì¦ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_count:,}, ì‹¤ì œ {actual_count:,}")
                else:
                    logger.info(f"Athena ì¿¼ë¦¬ ê²°ê³¼: {actual_count:,}ê°œ ë ˆì½”ë“œ")
                
                return validation_result
            else:
                logger.error("Athena ì¿¼ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return {'status': 'error', 'message': 'Empty result'}
                
        except Exception as e:
            logger.error(f"Athena ì¿¼ë¦¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def cleanup_backup_files(self, backup_files: List[str], confirm: bool = False) -> Dict:
        """ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        if not confirm:
            logger.warning("ë°±ì—… íŒŒì¼ ì •ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” confirm=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”")
            return {'status': 'skipped', 'message': 'confirmation required'}
        
        logger.info(f"ë°±ì—… íŒŒì¼ {len(backup_files)}ê°œ ì‚­ì œ ì¤‘...")
        
        deleted_count = 0
        failed_count = 0
        
        for backup_file in backup_files:
            try:
                key = backup_file.replace(f's3://{self.bucket_name}/', '')
                self.s3_client.delete_object(Bucket=self.bucket_name, Key=key)
                deleted_count += 1
            except Exception as e:
                logger.error(f"ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {backup_file}: {e}")
                failed_count += 1
        
        logger.info(f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: ì‚­ì œ {deleted_count}, ì‹¤íŒ¨ {failed_count}")
        return {
            'status': 'completed',
            'deleted_count': deleted_count,
            'failed_count': failed_count
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='SageMaker Feature Store ëŒ€ëŸ‰ ë°ì´í„° ì—…ë°ì´íŠ¸')
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument('--feature-group-name', required=True, 
                       help='Feature Group ì´ë¦„')
    parser.add_argument('--update-column', required=True,
                       help='ì—…ë°ì´íŠ¸í•  ì»¬ëŸ¼ëª…')
    
    # ë‹¨ì¼ ê°’ ë³€ê²½ ì¸ì (ìƒí˜¸ ë°°íƒ€ì  ê·¸ë£¹)
    value_group = parser.add_mutually_exclusive_group(required=True)
    value_group.add_argument('--single-update', nargs=2, metavar=('OLD_VALUE', 'NEW_VALUE'),
                           help='ë‹¨ì¼ ê°’ ë³€ê²½: --single-update "old_value" "new_value"')
    value_group.add_argument('--mapping-file', 
                           help='ë§¤í•‘ íŒŒì¼ ê²½ë¡œ (.json ë˜ëŠ” .csv)')
    value_group.add_argument('--conditional-mapping', 
                           help='ì¡°ê±´ë¶€ ë§¤í•‘ JSON ë¬¸ìì—´')
    value_group.add_argument('--transform-function', choices=['regex_replace', 'prefix_suffix', 'uppercase', 'lowercase', 'copy_from_column', 'extract_time_prefix'],
                           help='ë³€í™˜ í•¨ìˆ˜ íƒ€ì…')
    
    # ì´ì „ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ì¸ìë“¤ (deprecated)
    parser.add_argument('--old-value', help='[Deprecated] ê¸°ì¡´ ê°’ - --single-update ì‚¬ìš© ê¶Œì¥')
    parser.add_argument('--new-value', help='[Deprecated] ìƒˆë¡œìš´ ê°’ - --single-update ì‚¬ìš© ê¶Œì¥')
    
    # ì„ íƒì  ì¸ì
    parser.add_argument('--region', default='ap-northeast-2',
                       help='AWS ë¦¬ì „ (ê¸°ë³¸ê°’: ap-northeast-2)')
    parser.add_argument('--batch-size', type=int, default=1000,
                       help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 1000)')
    parser.add_argument('--dry-run', action='store_true',
                       help='ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰')
    parser.add_argument('--skip-validation', action='store_true',
                       help='Athena ê²€ì¦ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--filter-column', 
                       help='ì¶”ê°€ í•„í„° ì»¬ëŸ¼ëª…')
    parser.add_argument('--filter-value',
                       help='ì¶”ê°€ í•„í„° ê°’')
    parser.add_argument('--cleanup-backups', action='store_true',
                       help='ë°±ì—… íŒŒì¼ ìë™ ì •ë¦¬')
    
    # ë³€í™˜ í•¨ìˆ˜ ì˜µì…˜ë“¤
    parser.add_argument('--regex-pattern', 
                       help='ì •ê·œì‹ íŒ¨í„´ (transform-function=regex_replace ì‹œ í•„ìš”)')
    parser.add_argument('--regex-replacement', default='',
                       help='ì •ê·œì‹ ì¹˜í™˜ ë¬¸ìì—´ (ê¸°ë³¸ê°’: ë¹ˆ ë¬¸ìì—´)')
    parser.add_argument('--prefix', default='',
                       help='ì ‘ë‘ì‚¬ (transform-function=prefix_suffix ì‹œ)')
    parser.add_argument('--suffix', default='',
                       help='ì ‘ë¯¸ì‚¬ (transform-function=prefix_suffix ì‹œ)')
    parser.add_argument('--source-column',
                       help='ë³µì‚¬í•  ì›ë³¸ ì»¬ëŸ¼ëª… (transform-function=copy_from_column ì‹œ í•„ìš”)')
    parser.add_argument('--prefix-pattern', default=r'(\d{4}-\d{2}-\d{2})',
                       help='ì‹œê°„ ì¶”ì¶œìš© ì •ê·œì‹ íŒ¨í„´ (extract_time_prefix ì‹œ ì‚¬ìš©, ê¸°ë³¸ê°’: YYYY-MM-DD)')
    parser.add_argument('--time-format', default='auto',
                       help='ì‹œê°„ í˜•ì‹ (extract_time_prefix ì‹œ ì‚¬ìš©, ê¸°ë³¸ê°’: auto)')
    parser.add_argument('--to-iso', action='store_true', default=True,
                       help='ì¶”ì¶œëœ ì‹œê°„ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (extract_time_prefix ì‹œ ì‚¬ìš©)')
    parser.add_argument('--auto-confirm', action='store_true',
                       help='ëˆ„ë½ëœ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì²˜ë¦¬ (ëŒ€í™”í˜• í™•ì¸ ê±´ë„ˆë›°ê¸°)')
    
    args = parser.parse_args()
    
    try:
        # ì´ì „ ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
        if args.old_value and args.new_value and not args.single_update:
            logger.warning("--old-value, --new-valueëŠ” deprecatedì…ë‹ˆë‹¤. --single-update ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            args.single_update = [args.old_value, args.new_value]
        
        # Feature Store ì—…ë°ì´í„° ì´ˆê¸°í™”
        updater = SageMakerFeatureStoreUpdater(
            feature_group_name=args.feature_group_name,
            region_name=args.region,
            batch_size=args.batch_size
        )
        
        # ë°ì´í„° êµ¬ì¡° í™•ì¸
        logger.info("=== ë°ì´í„° êµ¬ì¡° í™•ì¸ ===")
        sample_df = updater.read_data_sample(100)
        
        if sample_df.empty:
            logger.error("ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì»¬ëŸ¼ í™•ì¸
        missing_columns = []
        if args.update_column not in sample_df.columns:
            missing_columns.append(args.update_column)
        
        # ì›ë³¸ ì»¬ëŸ¼ í™•ì¸ (copy_from_columnì˜ ê²½ìš°)
        if args.transform_function == 'copy_from_column' and args.source_column:
            if args.source_column not in sample_df.columns:
                logger.error(f"ì›ë³¸ ì»¬ëŸ¼ '{args.source_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(sample_df.columns)}")
                return
        
        # ëˆ„ë½ëœ ì»¬ëŸ¼ ì²˜ë¦¬
        if missing_columns:
            logger.warning(f"âš ï¸  ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì»¬ëŸ¼: {missing_columns}")
            logger.info("ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            logger.info("1. Feature Group ìŠ¤í‚¤ë§ˆì—ëŠ” ìˆì§€ë§Œ ê¸°ì¡´ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ì»¬ëŸ¼")
            logger.info("2. ìµœê·¼ì— ì¶”ê°€ëœ ì»¬ëŸ¼ìœ¼ë¡œ ì•„ì§ ë°ì´í„°ê°€ ì…ë ¥ë˜ì§€ ì•Šì€ ê²½ìš°")
            
            print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ëª©ë¡ ({len(sample_df.columns)}ê°œ):")
            for i, col in enumerate(sorted(sample_df.columns), 1):
                print(f"{i:2d}. {col}")
            
            if args.auto_confirm:
                logger.info(f"ğŸ”„ --auto-confirm ì˜µì…˜ìœ¼ë¡œ '{args.update_column}'ì„ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ìë™ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                logger.info("ì£¼ì˜: ì´ ì»¬ëŸ¼ì´ Feature Group ìŠ¤í‚¤ë§ˆì— ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            else:
                print(f"\nâ“ '{args.update_column}' ì»¬ëŸ¼ì´ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ì— ì—†ìŠµë‹ˆë‹¤.")
                print("ë‹¤ìŒ ì¤‘ ì„ íƒí•˜ì„¸ìš”:")
                print("1. ë‹¤ë¥¸ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½")
                print("2. ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€ (Feature Group ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ ê²½ìš°)")
                print("3. ì‘ì—… ì·¨ì†Œ")
                
                choice = input("ì„ íƒ (1/2/3): ").strip()
                
                if choice == "1":
                    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ì—ì„œ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì»¬ëŸ¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”:")
                    new_column = input("ìƒˆ ì»¬ëŸ¼ëª…: ").strip()
                    if new_column:
                        args.update_column = new_column
                        logger.info(f"ì—…ë°ì´íŠ¸ ì»¬ëŸ¼ì„ '{new_column}'ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤")
                        if new_column not in sample_df.columns:
                            logger.info(f"'{new_column}'ì€ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
                    else:
                        logger.info("ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
                        return
                elif choice == "2":
                    logger.info(f"'{args.update_column}'ì„ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤")
                    logger.info("ì£¼ì˜: ì´ ì»¬ëŸ¼ì´ Feature Group ìŠ¤í‚¤ë§ˆì— ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
                else:
                    logger.info("ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
                    return
        
        # ì—…ë°ì´íŠ¸ ë°©ì‹ ê²°ì • ë° íŒŒë¼ë¯¸í„° ì¤€ë¹„
        old_value = None
        new_value = None
        value_mapping = None
        conditional_mapping = None
        transform_function = None
        
        if args.single_update:
            old_value, new_value = args.single_update
            logger.info(f"ë‹¨ì¼ ê°’ ë³€ê²½: '{old_value}' -> '{new_value}'")
            
        elif args.mapping_file:
            value_mapping = updater.load_mapping_from_file(args.mapping_file)
            logger.info(f"ë§¤í•‘ íŒŒì¼ ê¸°ë°˜ ë³€ê²½: {len(value_mapping)}ê°œ ê·œì¹™")
            
        elif args.conditional_mapping:
            conditional_mapping = updater.create_conditional_mapping(args.conditional_mapping)
            logger.info(f"ì¡°ê±´ë¶€ ë§¤í•‘: {len(conditional_mapping)}ê°œ ì¡°ê±´")
            
        elif args.transform_function:
            if args.transform_function == 'regex_replace':
                if not args.regex_pattern:
                    logger.error("regex_replace ë³€í™˜ì—ëŠ” --regex-patternì´ í•„ìš”í•©ë‹ˆë‹¤")
                    return
                transform_function = updater.create_transform_function(
                    'regex_replace', 
                    pattern=args.regex_pattern, 
                    replacement=args.regex_replacement
                )
            elif args.transform_function == 'prefix_suffix':
                transform_function = updater.create_transform_function(
                    'prefix_suffix',
                    prefix=args.prefix,
                    suffix=args.suffix
                )
            elif args.transform_function == 'copy_from_column':
                if not args.source_column:
                    logger.error("copy_from_column ë³€í™˜ì—ëŠ” --source-columnì´ í•„ìš”í•©ë‹ˆë‹¤")
                    return
                transform_function = updater.create_transform_function(
                    'copy_from_column',
                    source_column=args.source_column
                )
            elif args.transform_function == 'extract_time_prefix':
                transform_function = updater.create_transform_function(
                    'extract_time_prefix',
                    time_format=args.time_format,
                    prefix_pattern=args.prefix_pattern,
                    to_iso=args.to_iso,
                    source_column=args.source_column
                )
            else:
                transform_function = updater.create_transform_function(args.transform_function)
                
            logger.info(f"ë³€í™˜ í•¨ìˆ˜ ì ìš©: {args.transform_function}")
        
        # copy_from_columnì´ë‚˜ extract_time_prefixì´ê³  ëŒ€ìƒ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° ê±´ë„ˆë›°ê¸°
        if (args.transform_function in ['copy_from_column', 'extract_time_prefix'] and 
            args.update_column not in sample_df.columns):
            logger.info("=== ìƒˆ ì»¬ëŸ¼ ìƒì„± ëª¨ë“œ: ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° ê±´ë„ˆë›°ê¸° ===")
            if args.transform_function == 'copy_from_column':
                logger.info(f"ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ {args.source_column} -> {args.update_column} ë³µì‚¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤")
            elif args.transform_function == 'extract_time_prefix':
                logger.info(f"ëª¨ë“  ë ˆì½”ë“œì— ëŒ€í•´ ì‹œê°„ prefix ì¶”ì¶œ -> {args.update_column} ë³€í™˜ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤")
            total_target_count = 999999  # ì„ì‹œê°’ (ì‹¤ì œ ê°œìˆ˜ëŠ” ë‚˜ì¤‘ì— í™•ì¸)
        else:
            # ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            logger.info("=== ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚° ===")
            count_result = updater.count_matching_records(
                column_name=args.update_column,
                old_value=old_value,
                value_mapping=value_mapping,
                conditional_mapping=conditional_mapping
            )
            
            total_target_count = sum(count_result['match_counts'].values())
            if total_target_count == 0:
                logger.warning("ë³€ê²½ ëŒ€ìƒ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                return
        
        # ì‚¬ìš©ì í™•ì¸
        if not args.dry_run:
            print(f"\nâš ï¸  ì£¼ì˜: {total_target_count:,}ê°œì˜ ë ˆì½”ë“œê°€ ë³€ê²½ë©ë‹ˆë‹¤!")
            print(f"ì»¬ëŸ¼: {args.update_column}")
            
            if args.single_update:
                print(f"ë‹¨ì¼ ë³€ê²½: '{old_value}' -> '{new_value}'")
            elif args.mapping_file:
                print(f"ë§¤í•‘ íŒŒì¼: {args.mapping_file} ({len(value_mapping)}ê°œ ê·œì¹™)")
            elif args.conditional_mapping:
                print(f"ì¡°ê±´ë¶€ ë§¤í•‘: {len(conditional_mapping)}ê°œ ì¡°ê±´")
            elif args.transform_function:
                print(f"ë³€í™˜ í•¨ìˆ˜: {args.transform_function}")
            
            if input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() != 'y':
                logger.info("ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
                return
        
        # ì¶”ê°€ í•„í„° ì¡°ê±´ ì„¤ì •
        filter_conditions = None
        if args.filter_column and args.filter_value:
            filter_conditions = {args.filter_column: args.filter_value}
        
        # ì—…ë°ì´íŠ¸ ì‹¤í–‰
        logger.info("=== ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ ì‹¤í–‰ ===")
        update_results = updater.update_records_batch(
            column_name=args.update_column,
            old_value=old_value,
            new_value=new_value,
            value_mapping=value_mapping,
            conditional_mapping=conditional_mapping,
            transform_function=transform_function,
            dry_run=args.dry_run,
            filter_conditions=filter_conditions
        )
        
        # Athena ê²€ì¦
        if not args.skip_validation and not args.dry_run:
            logger.info("=== Athena ì¿¼ë¦¬ ê²€ì¦ ===")
            
            # ë‹¨ì¼ ê°’ ë³€ê²½ì˜ ê²½ìš°ë§Œ Athena ê²€ì¦
            if args.single_update:
                # ê¸°ì¡´ ê°’ í™•ì¸
                old_validation = updater.validate_with_athena(args.update_column, old_value)
                
                # ìƒˆ ê°’ í™•ì¸
                new_validation = updater.validate_with_athena(
                    args.update_column, 
                    new_value, 
                    expected_count=update_results['updated_records']
                )
                
                logger.info(f"ê¸°ì¡´ ê°’ '{old_value}' ë ˆì½”ë“œ ìˆ˜: {old_validation.get('actual_count', 'N/A')}")
                logger.info(f"ìƒˆ ê°’ '{new_value}' ë ˆì½”ë“œ ìˆ˜: {new_validation.get('actual_count', 'N/A')}")
            else:
                logger.info("ë³µí•© ë§¤í•‘ì˜ ê²½ìš° Athena ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # ë°±ì—… íŒŒì¼ ì •ë¦¬
        if args.cleanup_backups and update_results['backup_files'] and not args.dry_run:
            logger.info("=== ë°±ì—… íŒŒì¼ ì •ë¦¬ ===")
            cleanup_result = updater.cleanup_backup_files(
                update_results['backup_files'], 
                confirm=True
            )
            logger.info(f"ë°±ì—… íŒŒì¼ ì •ë¦¬: {cleanup_result}")
        
        logger.info("=== ì‘ì—… ì™„ë£Œ ===\n")
        logger.info("ğŸ’¡ ì¶”ê°€ ì‚¬ìš©ë²•:")
        logger.info("  ğŸ“ ë§¤í•‘ íŒŒì¼ ì˜ˆì‹œ:")
        logger.info("     JSON: {\"old1\": \"new1\", \"old2\": \"new2\"}")
        logger.info("     CSV: old_value,new_value í˜•ì‹")
        logger.info("  ğŸ¯ ì¡°ê±´ë¶€ ë§¤í•‘: --conditional-mapping '{\"Category\": {\"A\": {\"old1\": \"new1\"}}}'")
        logger.info("  ğŸ”§ ë³€í™˜ í•¨ìˆ˜: --transform-function regex_replace --regex-pattern 'pattern' --regex-replacement 'replacement'")
        
    except KeyboardInterrupt:
        logger.info("ì‘ì—…ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def create_example_files():
    """ì˜ˆì‹œ ë§¤í•‘ íŒŒì¼ ìƒì„±"""
    # JSON ë§¤í•‘ íŒŒì¼ ì˜ˆì‹œ
    json_example = {
        "OLD_VALUE_1": "NEW_VALUE_1",
        "OLD_VALUE_2": "NEW_VALUE_2",
        "ABNORMAL": "NORMAL",
        "ERROR": "SUCCESS"
    }
    
    with open('value_mapping_example.json', 'w', encoding='utf-8') as f:
        json.dump(json_example, f, indent=2, ensure_ascii=False)
    
    # CSV ë§¤í•‘ íŒŒì¼ ì˜ˆì‹œ
    csv_example = pd.DataFrame({
        'old_value': ['OLD_VALUE_1', 'OLD_VALUE_2', 'ABNORMAL', 'ERROR'],
        'new_value': ['NEW_VALUE_1', 'NEW_VALUE_2', 'NORMAL', 'SUCCESS']
    })
    csv_example.to_csv('value_mapping_example.csv', index=False)
    
    print("ì˜ˆì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print("  - value_mapping_example.json")
    print("  - value_mapping_example.csv")


if __name__ == '__main__':
    if len(os.sys.argv) > 1 and os.sys.argv[1] == '--create-examples':
        create_example_files()
    else:
        main()