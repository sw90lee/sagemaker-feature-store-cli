# 3단계: Migration 기능 구현 상세 계획

## 목적
Feature Store 간 데이터 마이그레이션 기능 구현
- Offline → Online+Offline
- Online+Offline → Online+Offline

## 요구사항
1. **스키마 호환성 검증**: 소스와 타겟의 피처 정의 호환성 확인
2. **데이터 마이그레이션**: 배치 처리를 통한 대용량 데이터 이동
3. **타겟 정리**: `--clear-target` 옵션으로 타겟 데이터 사전 삭제
4. **진행상황 모니터링**: 실시간 진행률 및 상태 표시
5. **에러 처리**: 실패한 레코드 추적 및 재시도
6. **Dry-run**: 실제 마이그레이션 전 계획 확인

## CLI 인터페이스 설계

### 기본 명령어
```bash
sagemaker-fs migrate <source-feature-group> <target-feature-group> [OPTIONS]
```

### 옵션들
- `--clear-target`: 타겟 피처그룹의 기존 데이터 삭제
- `--batch-size <N>`: 배치 처리 사이즈 (기본: 100)
- `--max-workers <N>`: 동시 처리 워커 수 (기본: 4)
- `--dry-run`: 실제 마이그레이션 없이 계획만 확인
- `--resume-from <record-id>`: 특정 레코드부터 재개
- `--filter-query <sql>`: 마이그레이션할 데이터 필터링

### 예시
```bash
# 기본 마이그레이션
sagemaker-fs migrate source-fg target-fg

# 타겟 데이터 삭제 후 마이그레이션
sagemaker-fs migrate source-fg target-fg --clear-target

# 배치 사이즈 조정
sagemaker-fs migrate source-fg target-fg --batch-size 50 --max-workers 8

# 특정 조건의 데이터만 마이그레이션
sagemaker-fs migrate source-fg target-fg --filter-query "WHERE created_date >= '2024-01-01'"
```

## 아키텍처 설계

### 1. 파일 구조
```
src/sagemaker_fs_cli/commands/
└── migrate_cmd.py              # 새로 생성
```

### 2. migrate_cmd.py 구조
```python
def migrate_feature_group(config, source_name, target_name, **options):
    """메인 마이그레이션 진입점"""
    
def _validate_migration_compatibility(config, source_fg, target_fg):
    """소스-타겟 호환성 검증"""
    
def _plan_migration(config, source_fg, target_fg, options):
    """마이그레이션 계획 수립"""
    
def _execute_migration(config, source_fg, target_fg, migration_plan):
    """실제 마이그레이션 실행"""
    
def _extract_from_offline_store(config, source_fg, filter_query, batch_size):
    """오프라인 스토어에서 데이터 추출"""
    
def _load_to_online_store(config, target_fg, records_batch):
    """온라인 스토어로 데이터 로드"""
    
def _extract_from_online_store(config, source_fg, batch_size):
    """온라인 스토어에서 데이터 추출"""
```

## 핵심 구현 로직

### 1. 스키마 호환성 검증
```python
def _validate_migration_compatibility(config, source_fg, target_fg):
    """스키마 호환성 검증"""
    
    # 피처 정의 비교
    source_features = {f['FeatureName']: f for f in source_fg['FeatureDefinitions']}
    target_features = {f['FeatureName']: f for f in target_fg['FeatureDefinitions']}
    
    # 필수 피처 존재 확인
    missing_features = set(target_features.keys()) - set(source_features.keys())
    if missing_features:
        raise ValueError(f"타겟에 있지만 소스에 없는 피처: {missing_features}")
    
    # 데이터 타입 호환성 확인
    incompatible_features = []
    for fname, target_feat in target_features.items():
        if fname in source_features:
            source_feat = source_features[fname]
            if not _is_type_compatible(source_feat['FeatureType'], target_feat['FeatureType']):
                incompatible_features.append((fname, source_feat['FeatureType'], target_feat['FeatureType']))
    
    if incompatible_features:
        raise ValueError(f"호환되지 않는 피처 타입: {incompatible_features}")
    
    # RecordIdentifier와 EventTime 피처 확인
    if source_fg['RecordIdentifierFeatureName'] != target_fg['RecordIdentifierFeatureName']:
        raise ValueError("RecordIdentifierFeatureName이 다릅니다")
    
    if source_fg['EventTimeFeatureName'] != target_fg['EventTimeFeatureName']:
        raise ValueError("EventTimeFeatureName이 다릅니다")
```

### 2. 마이그레이션 실행 엔진
```python
def _execute_migration(config, source_fg, target_fg, migration_plan):
    """마이그레이션 실행"""
    
    source_type = migration_plan['source_type']  # 'offline', 'online+offline'
    target_type = migration_plan['target_type']
    total_records = migration_plan['estimated_records']
    
    # 진행률 추적
    with tqdm(total=total_records, desc="마이그레이션 진행 중") as pbar:
        failed_records = []
        
        # 데이터 소스에 따른 처리
        if source_type == 'offline':
            # Athena를 통한 오프라인 스토어 읽기
            for batch in _extract_from_offline_store(config, source_fg, migration_plan):
                success_count, failures = _load_to_online_store(config, target_fg, batch)
                failed_records.extend(failures)
                pbar.update(len(batch))
                
        elif 'online' in source_type:
            # 온라인 스토어에서 읽기 + 오프라인 스토어 병합
            for batch in _extract_from_hybrid_store(config, source_fg, migration_plan):
                success_count, failures = _load_to_online_store(config, target_fg, batch)
                failed_records.extend(failures)
                pbar.update(len(batch))
    
    return {
        'total_processed': pbar.n,
        'failed_records': failed_records,
        'success_rate': (pbar.n - len(failed_records)) / pbar.n if pbar.n > 0 else 0
    }
```

### 3. 오프라인 스토어 데이터 추출
```python
def _extract_from_offline_store(config, source_fg, filter_query=None, batch_size=100):
    """Athena를 통한 오프라인 스토어 데이터 추출"""
    
    athena_client = config.session.client('athena')
    database = 'sagemaker_featurestore'
    table = source_fg['FeatureGroupName'].replace('-', '_')
    
    # 기본 쿼리 구성
    base_query = f"SELECT * FROM {database}.{table}"
    if filter_query:
        query = f"{base_query} {filter_query}"
    else:
        query = base_query
    
    # Athena 쿼리 실행
    response = athena_client.start_query_execution(
        QueryString=query,
        ResultConfiguration={
            'OutputLocation': 's3://temp-query-results/'
        }
    )
    
    query_execution_id = response['QueryExecutionId']
    
    # 쿼리 완료 대기
    _wait_for_query_completion(athena_client, query_execution_id)
    
    # 결과를 배치로 읽기
    paginator = athena_client.get_paginator('get_query_results')
    current_batch = []
    
    for page in paginator.paginate(QueryExecutionId=query_execution_id):
        rows = page['ResultSet']['Rows'][1:]  # 헤더 스킵
        
        for row in rows:
            record = _convert_athena_row_to_record(row, source_fg['FeatureDefinitions'])
            current_batch.append(record)
            
            if len(current_batch) >= batch_size:
                yield current_batch
                current_batch = []
    
    if current_batch:
        yield current_batch
```

### 4. 온라인 스토어 데이터 로드
```python
def _load_to_online_store(config, target_fg, records_batch):
    """온라인 스토어로 배치 데이터 로드"""
    
    featurestore_runtime = config.session.client('sagemaker-featurestore-runtime')
    target_name = target_fg['FeatureGroupName']
    
    success_count = 0
    failures = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        # 각 레코드를 병렬로 처리
        future_to_record = {
            executor.submit(_put_single_record, featurestore_runtime, target_name, record): record
            for record in records_batch
        }
        
        for future in as_completed(future_to_record):
            record = future_to_record[future]
            try:
                future.result()
                success_count += 1
            except Exception as e:
                failures.append({
                    'record': record,
                    'error': str(e)
                })
    
    return success_count, failures

def _put_single_record(client, feature_group_name, record):
    """단일 레코드 저장 (재시도 포함)"""
    max_retries = 3
    base_delay = 1
    
    for attempt in range(max_retries):
        try:
            client.put_record(
                FeatureGroupName=feature_group_name,
                Record=[
                    {'FeatureName': k, 'ValueAsString': str(v)}
                    for k, v in record.items()
                ]
            )
            return
        except ClientError as e:
            if e.response['Error']['Code'] in ['ThrottlingException', 'TooManyRequestsException']:
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    time.sleep(delay)
                    continue
            raise
```

### 5. 마이그레이션 계획 수립
```python
def _plan_migration(config, source_fg, target_fg, options):
    """마이그레이션 계획 수립"""
    
    source_online = bool(source_fg.get('OnlineStoreConfig'))
    source_offline = bool(source_fg.get('OfflineStoreConfig'))
    target_online = bool(target_fg.get('OnlineStoreConfig'))
    target_offline = bool(target_fg.get('OfflineStoreConfig'))
    
    # 소스 타입 결정
    if source_online and source_offline:
        source_type = 'online+offline'
    elif source_online:
        source_type = 'online'
    else:
        source_type = 'offline'
    
    # 타겟 타입 결정
    if target_online and target_offline:
        target_type = 'online+offline'
    elif target_online:
        target_type = 'online'
    else:
        target_type = 'offline'
    
    # 레코드 수 추정
    estimated_records = _estimate_record_count(config, source_fg, source_type)
    
    # 마이그레이션 전략 결정
    strategy = _determine_migration_strategy(source_type, target_type)
    
    return {
        'source_type': source_type,
        'target_type': target_type,
        'estimated_records': estimated_records,
        'strategy': strategy,
        'batch_size': options.get('batch_size', 100),
        'max_workers': options.get('max_workers', 4),
        'filter_query': options.get('filter_query')
    }
```

## CLI 통합

### cli.py에 추가
```python
@cli.command('migrate')
@click.argument('source_feature_group')
@click.argument('target_feature_group')
@click.option('--clear-target', is_flag=True, help='타겟 피처그룹의 기존 데이터 삭제')
@click.option('--batch-size', default=100, help='배치 처리 사이즈')
@click.option('--max-workers', default=4, help='동시 처리 워커 수')
@click.option('--dry-run', is_flag=True, help='실제 마이그레이션 없이 계획만 확인')
@click.option('--filter-query', help='마이그레이션할 데이터 필터링 (SQL WHERE 절)')
@click.pass_context
def migrate_feature_group(ctx, source_feature_group, target_feature_group, 
                         clear_target, batch_size, max_workers, dry_run, filter_query):
    """피처 그룹 간 데이터 마이그레이션"""
    config = ctx.obj['config']
    migrate_cmd.migrate_feature_group(
        config, source_feature_group, target_feature_group,
        clear_target=clear_target, batch_size=batch_size, 
        max_workers=max_workers, dry_run=dry_run, 
        filter_query=filter_query
    )
```

## 에러 처리 및 복구

### 1. 실패 레코드 추적
```python
def _save_failed_records(failed_records, output_file):
    """실패한 레코드를 파일로 저장"""
    with open(output_file, 'w') as f:
        json.dump(failed_records, f, indent=2, default=str)

def _retry_failed_records(config, target_fg, failed_records_file):
    """실패한 레코드 재시도"""
    with open(failed_records_file, 'r') as f:
        failed_records = json.load(f)
    
    # 재시도 로직 구현
```

### 2. 체크포인트 기능
```python
def _save_checkpoint(migration_state, checkpoint_file):
    """마이그레이션 상태 저장"""
    with open(checkpoint_file, 'w') as f:
        json.dump(migration_state, f, indent=2, default=str)

def _load_checkpoint(checkpoint_file):
    """마이그레이션 상태 복원"""
    try:
        with open(checkpoint_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return None
```

## 구현 단계

### Step 1: migrate_cmd.py 기본 구조 생성
- 파일 생성 및 기본 함수 틀 구현
- 스키마 호환성 검증 로직

### Step 2: 오프라인 스토어 추출 구현
- Athena 쿼리를 통한 데이터 읽기
- 배치 처리 로직

### Step 3: 온라인 스토어 로드 구현
- 배치 데이터 병렬 처리
- 에러 처리 및 재시도

### Step 4: 마이그레이션 계획 및 실행 엔진
- 전체 마이그레이션 플로우 통합

### Step 5: CLI 통합
- cli.py에 migrate 명령어 추가

### Step 6: 고급 기능
- 체크포인트, 재시도, 필터링

## 위험도 및 고려사항

### 위험도: 높음
- **데이터 일관성**: 마이그레이션 중 데이터 손실 가능
- **성능 영향**: 대용량 처리 시 시스템 부하
- **API 제한**: AWS API 호출 제한 도달 가능

### 완화 방안
1. **배치 처리**: 메모리 효율적인 처리
2. **재시도 로직**: 일시적 오류 자동 복구
3. **체크포인트**: 중단 시 재시작 가능
4. **Dry-run**: 사전 검증으로 위험 최소화

이 계획으로 Migration 기능을 구현할까요?