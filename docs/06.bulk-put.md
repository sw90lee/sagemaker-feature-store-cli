# 6. bulk-put - ëŒ€ëŸ‰ ë ˆì½”ë“œ ì €ì¥

## ê°œìš”
Feature Groupì— ì—¬ëŸ¬ ë ˆì½”ë“œë¥¼ ì¼ê´„ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. JSON ë˜ëŠ” CSV íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
fs bulk-put FEATURE_GROUP_NAME INPUT_FILE [OPTIONS]
```

## í•„ìˆ˜ ì¸ì
- `FEATURE_GROUP_NAME`: ì €ì¥í•  Feature Group ì´ë¦„
- `INPUT_FILE`: ì €ì¥í•  ë ˆì½”ë“œë“¤ì´ í¬í•¨ëœ íŒŒì¼ (JSON ë˜ëŠ” CSV)

## ì˜µì…˜
- `--output-file, -o PATH`: ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¡œê·¸ íŒŒì¼
- `--batch-size INTEGER`: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 100, ìµœëŒ€ 1000)
- `--max-workers INTEGER`: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 20)
- `--dry-run`: ì‹¤ì œ ì €ì¥ ì—†ì´ ê²€ì¦ë§Œ ìˆ˜í–‰
- `--continue-on-error`: ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê³„ì† ì§„í–‰
- `--progress/--no-progress`: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

## ì…ë ¥ íŒŒì¼ í˜•ì‹

### JSON í˜•ì‹ ì…ë ¥
```json
[
  {
    "customer_id": "cust_001",
    "event_time": "2024-01-15T10:00:00Z",
    "age": "25",
    "balance": "1500.00",
    "category": "premium"
  },
  {
    "customer_id": "cust_002", 
    "event_time": "2024-01-15T10:01:00Z",
    "age": "30",
    "balance": "2500.00",
    "category": "vip"
  }
]
```

### CSV í˜•ì‹ ì…ë ¥
```csv
Customer_id,Event_time,Age,Balance,Category
cust_001,2024-01-15T10:00:00Z,25,1500.00,premium
cust_002,2024-01-15T10:01:00Z,30,2500.00,vip
cust_003,2024-01-15T10:02:00Z,28,1800.00,standard
```

## ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ëŒ€ëŸ‰ ì €ì¥ (JSON)
```bash
fs bulk-put customer-profile customer_data.json
```

### 2. CSV íŒŒì¼ì„ ì‚¬ìš©í•œ ì €ì¥
```bash
fs bulk-put customer-profile customer_data.csv
```

### 3. ë°°ì¹˜ í¬ê¸° ìµœì í™”
```bash
fs bulk-put customer-profile large_dataset.json \
  --batch-size 500 \
  --max-workers 15
```

### 4. ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
```bash
fs bulk-put customer-profile customer_data.json \
  --output-file processing_log.txt \
  --batch-size 200
```

### 5. ê²€ì¦ ëª¨ë“œ ì‹¤í–‰
```bash
fs bulk-put customer-profile customer_data.json \
  --dry-run \
  --output-file validation_report.txt
```

### 6. ì˜¤ë¥˜ í—ˆìš© ëª¨ë“œ
```bash
fs bulk-put customer-profile partial_data.json \
  --continue-on-error \
  --output-file error_log.txt
```

## ê³ ê¸‰ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì—…ë¡œë“œ íŒŒì´í”„ë¼ì¸
```bash
#!/bin/bash
upload_large_dataset() {
  local feature_group=$1
  local input_file=$2
  local log_dir="/logs/bulk_uploads"
  
  # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
  mkdir -p "$log_dir"
  
  timestamp=$(date +"%Y%m%d_%H%M%S")
  log_file="$log_dir/upload_${feature_group}_${timestamp}.log"
  
  # íŒŒì¼ í¬ê¸° í™•ì¸
  file_size=$(stat -c%s "$input_file")
  record_count=$(jq length "$input_file" 2>/dev/null || wc -l < "$input_file")
  
  echo "ğŸ“Š Upload Statistics:" | tee "$log_file"
  echo "  File: $input_file" | tee -a "$log_file"
  echo "  Size: $(numfmt --to=iec $file_size)" | tee -a "$log_file"
  echo "  Records: $record_count" | tee -a "$log_file"
  
  # íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê²°ì •
  if [ $record_count -gt 100000 ]; then
    batch_size=1000
    workers=25
  elif [ $record_count -gt 10000 ]; then
    batch_size=500
    workers=20
  else
    batch_size=200
    workers=15
  fi
  
  echo "  Batch size: $batch_size" | tee -a "$log_file"
  echo "  Workers: $workers" | tee -a "$log_file"
  echo "" | tee -a "$log_file"
  
  # ì—…ë¡œë“œ ì‹œì‘
  echo "ğŸš€ Starting upload at $(date)" | tee -a "$log_file"
  start_time=$(date +%s)
  
  fs bulk-put "$feature_group" "$input_file" \
    --batch-size $batch_size \
    --max-workers $workers \
    --output-file "$log_file.detail" \
    --continue-on-error
  
  upload_result=$?
  end_time=$(date +%s)
  duration=$((end_time - start_time))
  
  # ê²°ê³¼ ìš”ì•½
  echo "" | tee -a "$log_file"
  echo "â± Upload completed in ${duration} seconds" | tee -a "$log_file"
  
  if [ $upload_result -eq 0 ]; then
    throughput=$((record_count / duration))
    echo "âœ… Success: $record_count records (${throughput} records/sec)" | tee -a "$log_file"
  else
    echo "âš  Completed with errors (see $log_file.detail for details)" | tee -a "$log_file"
  fi
  
  return $upload_result
}

upload_large_dataset "customer-profile" "massive_customer_data.json"
```

### 2. ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì—…ë¡œë“œ
```bash
#!/bin/bash
setup_streaming_upload() {
  local feature_group=$1
  local stream_dir="/data/stream"
  local batch_dir="/data/batches" 
  local batch_size=100
  
  mkdir -p "$stream_dir" "$batch_dir"
  
  # ìŠ¤íŠ¸ë¦¼ íŒŒì¼ ìƒì„±ê¸° (ì‹œë®¬ë ˆì´ì…˜)
  generate_streaming_data() {
    while true; do
      timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
      filename="${stream_dir}/stream_$(date +%s_%N).json"
      
      # ëœë¤ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ ë°›ì•„ì˜´)
      cat << EOF > "$filename"
{
  "sensor_id": "sensor_$(shuf -i 1-1000 -n 1)",
  "event_time": "$timestamp",
  "temperature": "$(shuf -i 10-40 -n 1)",
  "humidity": "$(shuf -i 30-90 -n 1)",
  "status": "active"
}
EOF
      
      sleep 1  # 1ì´ˆë§ˆë‹¤ ìƒˆ ë°ì´í„°
    done
  }
  
  # ë°°ì¹˜ ìˆ˜ì§‘ê¸°
  batch_collector() {
    local batch_counter=0
    local current_batch_file=""
    local records_in_batch=0
    
    while true; do
      # ìƒˆ ë°°ì¹˜ íŒŒì¼ ì‹œì‘
      if [ -z "$current_batch_file" ] || [ $records_in_batch -ge $batch_size ]; then
        if [ -n "$current_batch_file" ] && [ $records_in_batch -gt 0 ]; then
          echo "]" >> "$current_batch_file"
          
          # ë°°ì¹˜ ì—…ë¡œë“œ
          echo "ğŸ“¤ Uploading batch: $current_batch_file ($records_in_batch records)"
          fs bulk-put "$feature_group" "$current_batch_file" \
            --batch-size $records_in_batch \
            --no-progress
          
          # ì—…ë¡œë“œ ì™„ë£Œëœ íŒŒì¼ ì •ë¦¬
          rm "$current_batch_file"
        fi
        
        batch_counter=$((batch_counter + 1))
        current_batch_file="$batch_dir/batch_$(date +%Y%m%d_%H%M%S)_${batch_counter}.json"
        records_in_batch=0
        echo "[" > "$current_batch_file"
      fi
      
      # ìŠ¤íŠ¸ë¦¼ íŒŒì¼ë“¤ ì²˜ë¦¬
      for stream_file in "$stream_dir"/*.json; do
        if [ ! -f "$stream_file" ]; then
          continue
        fi
        
        # ì‰¼í‘œ ì¶”ê°€ (ì²« ë²ˆì§¸ ë ˆì½”ë“œê°€ ì•„ë‹Œ ê²½ìš°)
        if [ $records_in_batch -gt 0 ]; then
          echo "," >> "$current_batch_file"
        fi
        
        # ë ˆì½”ë“œ ë‚´ìš© ì¶”ê°€
        cat "$stream_file" >> "$current_batch_file"
        records_in_batch=$((records_in_batch + 1))
        
        # ì²˜ë¦¬ëœ ìŠ¤íŠ¸ë¦¼ íŒŒì¼ ì‚­ì œ
        rm "$stream_file"
      done
      
      sleep 5  # 5ì´ˆë§ˆë‹¤ ë°°ì¹˜ ìˆ˜ì§‘ í™•ì¸
    done
  }
  
  echo "ğŸŒŠ Starting streaming upload for $feature_group"
  echo "ğŸ“ Stream directory: $stream_dir"
  echo "ğŸ“¦ Batch directory: $batch_dir"
  echo "ğŸ“Š Batch size: $batch_size"
  
  # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ìƒì„± ë° ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘
  generate_streaming_data &
  batch_collector &
  
  echo "âœ… Streaming upload setup completed"
  echo "ğŸ›‘ Press Ctrl+C to stop"
  
  # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ë¡œ ì •ë¦¬
  trap 'echo "ğŸ›‘ Stopping streaming upload..."; kill $(jobs -p); exit 0' INT
  wait
}

setup_streaming_upload "sensor-data"
```

### 3. ë°ì´í„° ê²€ì¦ ë° ë³€í™˜ íŒŒì´í”„ë¼ì¸
```bash
#!/bin/bash
validated_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local output_log=$3
  
  # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
  temp_dir=$(mktemp -d)
  validated_file="$temp_dir/validated_data.json"
  
  echo "ğŸ” Starting data validation and transformation..."
  
  # Pythonì„ ì‚¬ìš©í•œ ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
  python << EOF
import json
import re
from datetime import datetime, timezone
import sys

def validate_and_transform(input_file, output_file):
    """ë°ì´í„° ê²€ì¦ ë° ë³€í™˜"""
    
    try:
        with open(input_file, 'r') as f:
            if input_file.endswith('.json'):
                data = json.load(f)
            elif input_file.endswith('.csv'):
                import csv
                data = []
                reader = csv.DictReader(f)
                for row in reader:
                    data.append(row)
    except Exception as e:
        print(f"âŒ Failed to load input file: {e}")
        return False
    
    validated_records = []
    validation_errors = []
    transformation_count = 0
    
    for i, record in enumerate(data):
        record_errors = []
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['customer_id', 'event_time']
        for field in required_fields:
            if field not in record or not record[field]:
                record_errors.append(f"Missing required field: {field}")
        
        if record_errors:
            validation_errors.append(f"Record {i}: {', '.join(record_errors)}")
            continue
        
        # ë°ì´í„° ë³€í™˜
        transformed_record = record.copy()
        
        # ì‹œê°„ í˜•ì‹ ì •ê·œí™”
        try:
            # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ì„ ISO 8601ë¡œ ë³€í™˜
            time_str = str(record['event_time'])
            if not time_str.endswith('Z') and '+' not in time_str:
                # Unix timestamp ì²˜ë¦¬
                if time_str.isdigit():
                    dt = datetime.fromtimestamp(float(time_str), tz=timezone.utc)
                    transformed_record['event_time'] = dt.isoformat().replace('+00:00', 'Z')
                    transformation_count += 1
                # ë‹¤ë¥¸ í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: YYYY-MM-DD HH:MM:SS)
                elif re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', time_str):
                    dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
                    transformed_record['event_time'] = dt.isoformat().replace('+00:00', 'Z')
                    transformation_count += 1
        except Exception as e:
            validation_errors.append(f"Record {i}: Invalid time format '{record['event_time']}': {e}")
            continue
        
        # ìˆ«ì í•„ë“œ ë¬¸ìì—´ ë³€í™˜
        numeric_fields = ['age', 'balance', 'price', 'quantity']
        for field in numeric_fields:
            if field in transformed_record and transformed_record[field] is not None:
                try:
                    # ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (FeatureStore ìš”êµ¬ì‚¬í•­)
                    transformed_record[field] = str(float(transformed_record[field]))
                except (ValueError, TypeError):
                    # ì´ë¯¸ ë¬¸ìì—´ì´ê±°ë‚˜ ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€
                    pass
        
        # ë¬¸ìì—´ í•„ë“œ ì •ë¦¬
        string_fields = ['category', 'status', 'location']
        for field in string_fields:
            if field in transformed_record and transformed_record[field] is not None:
                transformed_record[field] = str(transformed_record[field]).strip()
        
        validated_records.append(transformed_record)
    
    # ê²°ê³¼ ì €ì¥
    with open(output_file, 'w') as f:
        json.dump(validated_records, f, indent=2)
    
    # ê²€ì¦ ê²°ê³¼ ì¶œë ¥
    total_records = len(data)
    valid_records = len(validated_records)
    success_rate = (valid_records / total_records) * 100 if total_records > 0 else 0
    
    print(f"ğŸ“Š Validation Results:")
    print(f"  Total records: {total_records}")
    print(f"  Valid records: {valid_records}")
    print(f"  Invalid records: {len(validation_errors)}")
    print(f"  Success rate: {success_rate:.2f}%")
    print(f"  Transformations applied: {transformation_count}")
    
    if validation_errors:
        print(f"âŒ Validation errors:")
        for error in validation_errors[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            print(f"    {error}")
        if len(validation_errors) > 10:
            print(f"    ... and {len(validation_errors) - 10} more errors")
    
    return success_rate >= 80.0  # 80% ì´ìƒ ì„±ê³µë¥  ìš”êµ¬

if validate_and_transform('$input_file', '$validated_file'):
    print("âœ… Validation passed")
    sys.exit(0)
else:
    print("âŒ Validation failed")
    sys.exit(1)
EOF
  
  if [ $? -eq 0 ]; then
    echo "ğŸš€ Uploading validated data..."
    fs bulk-put "$feature_group" "$validated_file" \
      --output-file "$output_log" \
      --batch-size 300 \
      --continue-on-error
    
    upload_result=$?
    
    # ì—…ë¡œë“œ ê²°ê³¼ ê²€ì¦
    if [ $upload_result -eq 0 ]; then
      echo "âœ… Upload completed successfully"
    else
      echo "âš  Upload completed with errors (see $output_log)"
    fi
  else
    echo "âŒ Data validation failed, upload cancelled"
    upload_result=1
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -rf "$temp_dir"
  
  return $upload_result
}

validated_bulk_put "customer-profile" "raw_customer_data.csv" "upload_log.txt"
```

### 4. ì¦ë¶„ ë°ì´í„° ì—…ë°ì´íŠ¸
```bash
#!/bin/bash
incremental_update() {
  local feature_group=$1
  local new_data_file=$2
  local last_update_marker="/tmp/.last_update_${feature_group}"
  
  # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í™•ì¸
  if [ -f "$last_update_marker" ]; then
    last_update=$(cat "$last_update_marker")
    echo "ğŸ“… Last update: $last_update"
  else
    last_update="1970-01-01T00:00:00Z"
    echo "ğŸ“… No previous update found, processing all data"
  fi
  
  current_time=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  temp_incremental="/tmp/incremental_${feature_group}_$(date +%s).json"
  
  # ì¦ë¶„ ë°ì´í„° í•„í„°ë§
  echo "ğŸ” Filtering incremental data since $last_update..."
  
  python << EOF
import json
from datetime import datetime

def filter_incremental_data(input_file, output_file, since_time):
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    since_dt = datetime.fromisoformat(since_time.replace('Z', '+00:00'))
    incremental_records = []
    
    for record in data:
        try:
            event_time = record.get('event_time', '')
            if event_time:
                # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                if event_time.endswith('Z'):
                    record_dt = datetime.fromisoformat(event_time.replace('Z', '+00:00'))
                else:
                    record_dt = datetime.fromisoformat(event_time)
                
                if record_dt > since_dt:
                    incremental_records.append(record)
        except Exception as e:
            print(f"Warning: Could not parse time for record {record.get('customer_id', 'unknown')}: {e}")
            # ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ í¬í•¨
            incremental_records.append(record)
    
    with open(output_file, 'w') as f:
        json.dump(incremental_records, f, indent=2)
    
    print(f"Filtered {len(incremental_records)} records out of {len(data)} total")
    return len(incremental_records)

record_count = filter_incremental_data('$new_data_file', '$temp_incremental', '$last_update')
if record_count == 0:
    print("No new records to update")
    exit(0)
EOF
  
  if [ $? -ne 0 ] || [ ! -s "$temp_incremental" ]; then
    echo "â„¹ No incremental data to process"
    rm -f "$temp_incremental"
    return 0
  fi
  
  incremental_count=$(jq length "$temp_incremental")
  echo "ğŸ“Š Found $incremental_count new/updated records"
  
  # ì¦ë¶„ ë°ì´í„° ì—…ë¡œë“œ
  echo "ğŸš€ Uploading incremental data..."
  log_file="/tmp/incremental_upload_${feature_group}_$(date +%Y%m%d_%H%M%S).log"
  
  fs bulk-put "$feature_group" "$temp_incremental" \
    --output-file "$log_file" \
    --batch-size 200 \
    --continue-on-error
  
  if [ $? -eq 0 ]; then
    # ì„±ê³µ ì‹œ ë§ˆì»¤ ì—…ë°ì´íŠ¸
    echo "$current_time" > "$last_update_marker"
    echo "âœ… Incremental update completed: $incremental_count records"
    echo "ğŸ“ Update marker saved: $current_time"
  else
    echo "âŒ Incremental update failed (see $log_file)"
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -f "$temp_incremental"
}

# ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰
incremental_update "customer-profile" "daily_customer_updates.json"
```

### 5. ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° í†µí•© ì—…ë¡œë“œ
```bash
#!/bin/bash
multi_source_upload() {
  local feature_group=$1
  local config_file=$2  # ì†ŒìŠ¤ ì„¤ì • JSON íŒŒì¼
  
  # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
  temp_dir=$(mktemp -d)
  consolidated_file="$temp_dir/consolidated_data.json"
  
  echo "ğŸ”„ Multi-source data consolidation starting..."
  
  # ì„¤ì • íŒŒì¼ ì˜ˆì‹œ:
  # {
  #   "sources": [
  #     {"name": "crm", "file": "crm_data.json", "id_field": "customer_id"},
  #     {"name": "web", "file": "web_data.csv", "id_field": "user_id", "id_mapping": {"user_id": "customer_id"}},
  #     {"name": "mobile", "file": "mobile_data.json", "id_field": "mobile_user_id", "id_mapping": {"mobile_user_id": "customer_id"}}
  #   ]
  # }
  
  python << EOF
import json
import csv
from collections import defaultdict

# ì„¤ì • íŒŒì¼ ë¡œë“œ
with open('$config_file', 'r') as f:
    config = json.load(f)

consolidated_data = {}
source_stats = {}

for source_config in config['sources']:
    source_name = source_config['name']
    source_file = source_config['file']
    id_field = source_config['id_field']
    id_mapping = source_config.get('id_mapping', {})
    
    print(f"ğŸ“¥ Processing source: {source_name} ({source_file})")
    
    # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë°ì´í„° ë¡œë“œ
    try:
        if source_file.endswith('.json'):
            with open(source_file, 'r') as f:
                source_data = json.load(f)
        elif source_file.endswith('.csv'):
            source_data = []
            with open(source_file, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    source_data.append(row)
        else:
            print(f"âŒ Unsupported file format for {source_file}")
            continue
            
        records_processed = 0
        
        for record in source_data:
            # ID ë§¤í•‘ ì ìš©
            record_id = record.get(id_field)
            if not record_id:
                continue
            
            # ID í•„ë“œëª… ë³€í™˜
            if id_mapping:
                for old_field, new_field in id_mapping.items():
                    if old_field in record:
                        record[new_field] = record.pop(old_field)
                        record_id = record[new_field]
            
            # ê¸°ì¡´ ë ˆì½”ë“œì™€ ë³‘í•©
            if record_id in consolidated_data:
                # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ìƒˆ ë°ì´í„°ê°€ ìš°ì„ )
                consolidated_data[record_id].update(record)
            else:
                consolidated_data[record_id] = record.copy()
            
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
            if f'{source_name}_source' not in consolidated_data[record_id]:
                consolidated_data[record_id][f'{source_name}_source'] = True
                
            records_processed += 1
        
        source_stats[source_name] = records_processed
        print(f"âœ… {source_name}: {records_processed} records processed")
        
    except Exception as e:
        print(f"âŒ Error processing {source_name}: {e}")
        source_stats[source_name] = 0

# í†µí•© ë°ì´í„° ì €ì¥
consolidated_list = list(consolidated_data.values())
with open('$consolidated_file', 'w') as f:
    json.dump(consolidated_list, f, indent=2)

print(f"\nğŸ“Š Consolidation Summary:")
print(f"  Total unique records: {len(consolidated_list)}")
for source, count in source_stats.items():
    print(f"  {source}: {count} records")

EOF
  
  if [ ! -s "$consolidated_file" ]; then
    echo "âŒ No consolidated data generated"
    rm -rf "$temp_dir"
    return 1
  fi
  
  record_count=$(jq length "$consolidated_file")
  echo "ğŸš€ Uploading $record_count consolidated records..."
  
  # í†µí•©ëœ ë°ì´í„° ì—…ë¡œë“œ
  log_file="/tmp/multi_source_upload_$(date +%Y%m%d_%H%M%S).log"
  
  fs bulk-put "$feature_group" "$consolidated_file" \
    --output-file "$log_file" \
    --batch-size 400 \
    --max-workers 18 \
    --continue-on-error
  
  upload_result=$?
  
  if [ $upload_result -eq 0 ]; then
    echo "âœ… Multi-source upload completed successfully"
  else
    echo "âš  Multi-source upload completed with errors (see $log_file)"
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -rf "$temp_dir"
  
  return $upload_result
}

# ì„¤ì • íŒŒì¼ ìƒì„± ì˜ˆì‹œ
cat << 'EOF' > multi_source_config.json
{
  "sources": [
    {
      "name": "crm",
      "file": "crm_customers.json",
      "id_field": "customer_id"
    },
    {
      "name": "web_analytics", 
      "file": "web_activity.csv",
      "id_field": "user_id",
      "id_mapping": {"user_id": "customer_id"}
    },
    {
      "name": "mobile_app",
      "file": "mobile_usage.json", 
      "id_field": "app_user_id",
      "id_mapping": {"app_user_id": "customer_id"}
    }
  ]
}
EOF

multi_source_upload "unified-customer-features" "multi_source_config.json"
```

### 6. ì˜¤ë¥˜ ë³µêµ¬ ë° ì¬ì‹œë„ ì‹œìŠ¤í…œ
```bash
#!/bin/bash
resilient_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local max_retries=3
  local base_batch_size=200
  
  # ì‹¤íŒ¨ ì¶”ì  ë””ë ‰í† ë¦¬
  failure_dir="/tmp/bulk_put_failures_$(date +%s)"
  mkdir -p "$failure_dir"
  
  attempt=1
  current_batch_size=$base_batch_size
  
  while [ $attempt -le $max_retries ]; do
    echo "ğŸ”„ Attempt $attempt/$max_retries (batch size: $current_batch_size)"
    
    attempt_log="$failure_dir/attempt_${attempt}.log"
    
    if fs bulk-put "$feature_group" "$input_file" \
       --batch-size $current_batch_size \
       --output-file "$attempt_log" \
       --continue-on-error; then
      
      echo "âœ… Upload succeeded on attempt $attempt"
      rm -rf "$failure_dir"
      return 0
    fi
    
    echo "âŒ Attempt $attempt failed"
    
    # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì¤€ë¹„
    if [ $attempt -lt $max_retries ]; then
      # ë°°ì¹˜ í¬ê¸° ê°ì†Œ (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ëŒ€ì‘)
      current_batch_size=$((current_batch_size / 2))
      if [ $current_batch_size -lt 50 ]; then
        current_batch_size=50
      fi
      
      # ì§€ìˆ˜ì  ë°±ì˜¤í”„
      sleep_time=$((2 ** attempt))
      echo "â³ Waiting ${sleep_time}s before retry with smaller batch size..."
      sleep $sleep_time
      
      # ì‹¤íŒ¨í•œ ë ˆì½”ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ì¬ì‹œë„ (ì„ íƒì )
      if command -v jq >/dev/null && [ -s "$attempt_log" ]; then
        echo "ğŸ” Analyzing failed records..."
        # ì—¬ê¸°ì— ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
      fi
    fi
    
    attempt=$((attempt + 1))
  done
  
  echo "ğŸ’¥ All $max_retries attempts failed"
  echo "ğŸ“ Failure logs saved in: $failure_dir"
  
  return 1
}

resilient_bulk_put "customer-profile" "problematic_data.json"
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
```bash
#!/bin/bash
adaptive_batch_size() {
  local input_file=$1
  local record_count
  local file_size
  local avg_record_size
  local optimal_batch_size
  
  # íŒŒì¼ ë¶„ì„
  if [[ "$input_file" == *.json ]]; then
    record_count=$(jq length "$input_file")
  else
    record_count=$(($(wc -l < "$input_file") - 1))  # CSV í—¤ë” ì œì™¸
  fi
  
  file_size=$(stat -c%s "$input_file")
  avg_record_size=$((file_size / record_count))
  
  # í‰ê·  ë ˆì½”ë“œ í¬ê¸°ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ê²°ì •
  if [ $avg_record_size -lt 1000 ]; then      # < 1KB
    optimal_batch_size=1000
  elif [ $avg_record_size -lt 5000 ]; then   # 1-5KB  
    optimal_batch_size=500
  elif [ $avg_record_size -lt 10000 ]; then  # 5-10KB
    optimal_batch_size=200
  elif [ $avg_record_size -lt 50000 ]; then  # 10-50KB
    optimal_batch_size=100
  else                                        # > 50KB
    optimal_batch_size=50
  fi
  
  # API ì œí•œ ì ìš©
  if [ $optimal_batch_size -gt 1000 ]; then
    optimal_batch_size=1000
  fi
  
  echo $optimal_batch_size
}

# ì‚¬ìš© ì˜ˆì‹œ
BATCH_SIZE=$(adaptive_batch_size "input_data.json")
echo "ğŸ“Š Optimal batch size: $BATCH_SIZE"

fs bulk-put feature-group "input_data.json" --batch-size $BATCH_SIZE
```

### 2. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ì›Œì»¤ ì¡°ì •
```bash
#!/bin/bash
optimal_workers() {
  local cpu_cores=$(nproc)
  local memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  local network_quality="good"  # good/medium/poor
  
  # CPU ê¸°ë°˜ ê¸°ë³¸ ì›Œì»¤ ìˆ˜
  base_workers=$((cpu_cores * 2))
  
  # ë©”ëª¨ë¦¬ ì œí•œ ì ìš©
  if [ $memory_gb -lt 4 ]; then
    max_memory_workers=5
  elif [ $memory_gb -lt 8 ]; then
    max_memory_workers=10  
  elif [ $memory_gb -lt 16 ]; then
    max_memory_workers=15
  else
    max_memory_workers=25
  fi
  
  # ë„¤íŠ¸ì›Œí¬ í’ˆì§ˆ ê³ ë ¤
  case $network_quality in
    "poor")
      network_factor=0.5
      ;;
    "medium")  
      network_factor=0.8
      ;;
    "good")
      network_factor=1.0
      ;;
  esac
  
  optimal_workers=$(echo "$base_workers * $network_factor" | bc | cut -d. -f1)
  
  # ë©”ëª¨ë¦¬ ì œí•œ ì ìš©
  if [ $optimal_workers -gt $max_memory_workers ]; then
    optimal_workers=$max_memory_workers
  fi
  
  # ìµœì†Œ/ìµœëŒ€ ì œí•œ
  if [ $optimal_workers -lt 5 ]; then
    optimal_workers=5
  elif [ $optimal_workers -gt 25 ]; then
    optimal_workers=25
  fi
  
  echo $optimal_workers
}

WORKERS=$(optimal_workers)
echo "âš™ Optimal workers: $WORKERS"

fs bulk-put feature-group "data.json" --max-workers $WORKERS
```

### 3. ì§„í–‰ë¥  ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
#!/bin/bash
monitored_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local batch_size=${3:-200}
  
  # ì§„í–‰ë¥  íŒŒì¼
  progress_file="/tmp/bulk_put_progress_$$.json"
  
  # ì´ˆê¸° ì§„í–‰ë¥  íŒŒì¼ ìƒì„±
  total_records=$(jq length "$input_file" 2>/dev/null || echo "unknown")
  cat << EOF > "$progress_file"
{
  "total_records": $total_records,
  "processed_records": 0,
  "start_time": "$(date -Iseconds)",
  "current_batch": 0,
  "errors": 0,
  "throughput": 0
}
EOF
  
  # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤
  (
    while [ -f "$progress_file" ]; do
      if [ -f "$progress_file" ]; then
        start_time=$(jq -r '.start_time' "$progress_file")
        processed=$(jq -r '.processed_records' "$progress_file" 2>/dev/null || echo 0)
        total=$(jq -r '.total_records' "$progress_file")
        
        if [ "$processed" != "0" ] && [ "$total" != "unknown" ]; then
          percentage=$(( processed * 100 / total ))
          
          # ê²½ê³¼ ì‹œê°„ ê³„ì‚°
          current_time=$(date +%s)
          start_unix=$(date -d "$start_time" +%s)
          elapsed=$((current_time - start_unix))
          
          if [ $elapsed -gt 0 ]; then
            throughput=$((processed / elapsed))
            eta=$(( (total - processed) / throughput ))
            eta_human=$(date -d "@$((current_time + eta))" '+%H:%M:%S')
            
            printf "\râ³ Progress: %d/%d (%d%%) | Speed: %d rec/s | ETA: %s" \
              $processed $total $percentage $throughput $eta_human
          fi
        fi
      fi
      sleep 2
    done
  ) &
  monitor_pid=$!
  
  # ì‹¤ì œ bulk-put ì‹¤í–‰
  start_time=$(date +%s)
  
  fs bulk-put "$feature_group" "$input_file" \
    --batch-size $batch_size \
    --output-file "/tmp/bulk_put_details_$$.log"
    
  result=$?
  end_time=$(date +%s)
  
  # ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
  kill $monitor_pid 2>/dev/null
  rm -f "$progress_file"
  
  # ìµœì¢… ê²°ê³¼ ì¶œë ¥
  duration=$((end_time - start_time))
  echo -e "\n"
  
  if [ $result -eq 0 ]; then
    if [ "$total_records" != "unknown" ]; then
      throughput=$((total_records / duration))
      echo "âœ… Upload completed: $total_records records in ${duration}s (${throughput} rec/s)"
    else
      echo "âœ… Upload completed in ${duration}s"
    fi
  else
    echo "âŒ Upload failed after ${duration}s"
  fi
  
  return $result
}

monitored_bulk_put "customer-profile" "customer_data.json" 300
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤
```bash
#!/bin/bash
handle_common_errors() {
  local feature_group=$1
  local input_file=$2
  
  # ì‚¬ì „ ê²€ì¦
  echo "ğŸ” Pre-upload validation..."
  
  # íŒŒì¼ ì¡´ì¬ í™•ì¸
  if [ ! -f "$input_file" ]; then
    echo "âŒ Input file not found: $input_file"
    return 1
  fi
  
  # íŒŒì¼ í˜•ì‹ ê²€ì¦
  if [[ "$input_file" == *.json ]]; then
    if ! jq empty "$input_file" 2>/dev/null; then
      echo "âŒ Invalid JSON format in $input_file"
      return 1
    fi
  elif [[ "$input_file" == *.csv ]]; then
    if ! head -1 "$input_file" | grep -q ','; then
      echo "âŒ Invalid CSV format in $input_file"
      return 1
    fi
  else
    echo "âŒ Unsupported file format: $input_file"
    return 1
  fi
  
  # Feature Group ì¡´ì¬ í™•ì¸
  if ! fs list -o json | jq -r '.[].FeatureGroupName' | grep -q "^$feature_group$"; then
    echo "âŒ Feature group not found: $feature_group"
    echo "Available feature groups:"
    fs list -o json | jq -r '.[].FeatureGroupName' | sed 's/^/  - /'
    return 1
  fi
  
  echo "âœ… Pre-validation passed"
  
  # ì‹¤ì œ ì—…ë¡œë“œ (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)
  fs bulk-put "$feature_group" "$input_file" --continue-on-error
}

handle_common_errors "customer-profile" "customer_data.json"
```

### 2. ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ì²˜ë¦¬
```bash
#!/bin/bash
schema_validation_upload() {
  local feature_group=$1
  local input_file=$2
  
  echo "ğŸ“‹ Validating against feature group schema..."
  
  # í˜„ì¬ ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
  schema_file="/tmp/${feature_group}_schema.json"
  fs schema "$feature_group" -o json > "$schema_file"
  
  python << EOF
import json

# ìŠ¤í‚¤ë§ˆ ë¡œë“œ
with open('$schema_file', 'r') as f:
    schema = json.load(f)

schema_fields = {item['FeatureName'] for item in schema}

# ì…ë ¥ ë°ì´í„° ë¡œë“œ
with open('$input_file', 'r') as f:
    if '$input_file'.endswith('.json'):
        data = json.load(f)
    else:
        import csv
        data = []
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)

if not data:
    print("âŒ No data found in input file")
    exit(1)

# í•„ë“œ ì¼ì¹˜ ê²€ì‚¬
sample_record = data[0]
data_fields = set(sample_record.keys())

missing_in_schema = data_fields - schema_fields
extra_in_data = data_fields - schema_fields

if missing_in_schema:
    print(f"âš  Fields in data but not in schema: {list(missing_in_schema)}")

missing_required = schema_fields - data_fields
if missing_required:
    print(f"âŒ Required schema fields missing in data: {list(missing_required)}")
    exit(1)

print("âœ… Schema validation passed")
EOF
  
  if [ $? -eq 0 ]; then
    echo "ğŸš€ Proceeding with upload..."
    fs bulk-put "$feature_group" "$input_file"
  else
    echo "âŒ Schema validation failed, upload cancelled"
    return 1
  fi
  
  rm -f "$schema_file"
}

schema_validation_upload "customer-profile" "customer_data.json"
```

## ëª¨ë²” ì‚¬ë¡€

1. **ì‚¬ì „ ê²€ì¦**: íŒŒì¼ í˜•ì‹, ìŠ¤í‚¤ë§ˆ ì¼ì¹˜, í•„ìˆ˜ í•„ë“œ í™•ì¸
2. **ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°**: ë ˆì½”ë“œ í¬ê¸°ì™€ ë„¤íŠ¸ì›Œí¬ ìƒí™© ê³ ë ¤
3. **ì˜¤ë¥˜ ì²˜ë¦¬**: ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš© ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„  
4. **ëª¨ë‹ˆí„°ë§**: ì§„í–‰ë¥  ì¶”ì  ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
5. **ë¦¬ì†ŒìŠ¤ ìµœì í™”**: CPU/ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë§ëŠ” ì„¤ì •
6. **ë¡œê¹…**: ìƒì„¸í•œ ì²˜ë¦¬ ë¡œê·¸ ë° ì˜¤ë¥˜ ì¶”ì 

## ê´€ë ¨ ëª…ë ¹ì–´
- `fs bulk-get`: ëŒ€ëŸ‰ ë ˆì½”ë“œ ì¡°íšŒ
- `fs put`: ë‹¨ì¼ ë ˆì½”ë“œ ì €ì¥
- `fs schema`: ìŠ¤í‚¤ë§ˆ ì •ë³´ í™•ì¸
- `fs list`: Feature Group ëª©ë¡ ì¡°íšŒ