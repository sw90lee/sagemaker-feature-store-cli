# 6. bulk-put - 대량 레코드 저장

## 개요
Feature Group에 여러 레코드를 일괄적으로 저장합니다. JSON 또는 CSV 파일을 입력으로 받아 대량의 데이터를 효율적으로 업로드할 수 있습니다.

## 기본 사용법
```bash
fs bulk-put FEATURE_GROUP_NAME INPUT_FILE [OPTIONS]
```

## 필수 인자
- `FEATURE_GROUP_NAME`: 저장할 Feature Group 이름
- `INPUT_FILE`: 저장할 레코드들이 포함된 파일 (JSON 또는 CSV)

## 옵션
- `--output-file, -o PATH`: 처리 결과를 저장할 로그 파일
- `--batch-size INTEGER`: 배치 처리 크기 (기본값: 100, 최대 1000)
- `--max-workers INTEGER`: 최대 워커 수 (기본값: 20)
- `--dry-run`: 실제 저장 없이 검증만 수행
- `--continue-on-error`: 오류 발생 시에도 계속 진행
- `--progress/--no-progress`: 진행률 표시 여부 (기본값: True)

## 입력 파일 형식

### JSON 형식 입력
```json
[
  {
    "customer_id": "cust_001",
    "event_time": "2024-01-15T10:00:00Z",
    "age": "25",
    "balance": "1500.00",
    "category": "premium"
  },
  {
    "customer_id": "cust_002", 
    "event_time": "2024-01-15T10:01:00Z",
    "age": "30",
    "balance": "2500.00",
    "category": "vip"
  }
]
```

### CSV 형식 입력
```csv
Customer_id,Event_time,Age,Balance,Category
cust_001,2024-01-15T10:00:00Z,25,1500.00,premium
cust_002,2024-01-15T10:01:00Z,30,2500.00,vip
cust_003,2024-01-15T10:02:00Z,28,1800.00,standard
```

## 상세 사용 예시

### 1. 기본 대량 저장 (JSON)
```bash
fs bulk-put customer-profile customer_data.json
```

### 2. CSV 파일을 사용한 저장
```bash
fs bulk-put customer-profile customer_data.csv
```

### 3. 배치 크기 최적화
```bash
fs bulk-put customer-profile large_dataset.json \
  --batch-size 500 \
  --max-workers 15
```

### 4. 처리 로그 저장
```bash
fs bulk-put customer-profile customer_data.json \
  --output-file processing_log.txt \
  --batch-size 200
```

### 5. 검증 모드 실행
```bash
fs bulk-put customer-profile customer_data.json \
  --dry-run \
  --output-file validation_report.txt
```

### 6. 오류 허용 모드
```bash
fs bulk-put customer-profile partial_data.json \
  --continue-on-error \
  --output-file error_log.txt
```

## 고급 사용 시나리오

### 1. 대용량 데이터 업로드 파이프라인
```bash
#!/bin/bash
upload_large_dataset() {
  local feature_group=$1
  local input_file=$2
  local log_dir="/logs/bulk_uploads"
  
  # 로그 디렉토리 생성
  mkdir -p "$log_dir"
  
  timestamp=$(date +"%Y%m%d_%H%M%S")
  log_file="$log_dir/upload_${feature_group}_${timestamp}.log"
  
  # 파일 크기 확인
  file_size=$(stat -c%s "$input_file")
  record_count=$(jq length "$input_file" 2>/dev/null || wc -l < "$input_file")
  
  echo "📊 Upload Statistics:" | tee "$log_file"
  echo "  File: $input_file" | tee -a "$log_file"
  echo "  Size: $(numfmt --to=iec $file_size)" | tee -a "$log_file"
  echo "  Records: $record_count" | tee -a "$log_file"
  
  # 파일 크기에 따른 최적 배치 크기 결정
  if [ $record_count -gt 100000 ]; then
    batch_size=1000
    workers=25
  elif [ $record_count -gt 10000 ]; then
    batch_size=500
    workers=20
  else
    batch_size=200
    workers=15
  fi
  
  echo "  Batch size: $batch_size" | tee -a "$log_file"
  echo "  Workers: $workers" | tee -a "$log_file"
  echo "" | tee -a "$log_file"
  
  # 업로드 시작
  echo "🚀 Starting upload at $(date)" | tee -a "$log_file"
  start_time=$(date +%s)
  
  fs bulk-put "$feature_group" "$input_file" \
    --batch-size $batch_size \
    --max-workers $workers \
    --output-file "$log_file.detail" \
    --continue-on-error
  
  upload_result=$?
  end_time=$(date +%s)
  duration=$((end_time - start_time))
  
  # 결과 요약
  echo "" | tee -a "$log_file"
  echo "⏱ Upload completed in ${duration} seconds" | tee -a "$log_file"
  
  if [ $upload_result -eq 0 ]; then
    throughput=$((record_count / duration))
    echo "✅ Success: $record_count records (${throughput} records/sec)" | tee -a "$log_file"
  else
    echo "⚠ Completed with errors (see $log_file.detail for details)" | tee -a "$log_file"
  fi
  
  return $upload_result
}

upload_large_dataset "customer-profile" "massive_customer_data.json"
```

### 2. 실시간 데이터 스트리밍 업로드
```bash
#!/bin/bash
setup_streaming_upload() {
  local feature_group=$1
  local stream_dir="/data/stream"
  local batch_dir="/data/batches" 
  local batch_size=100
  
  mkdir -p "$stream_dir" "$batch_dir"
  
  # 스트림 파일 생성기 (시뮬레이션)
  generate_streaming_data() {
    while true; do
      timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
      filename="${stream_dir}/stream_$(date +%s_%N).json"
      
      # 랜덤 데이터 생성 (실제로는 외부 시스템에서 받아옴)
      cat << EOF > "$filename"
{
  "sensor_id": "sensor_$(shuf -i 1-1000 -n 1)",
  "event_time": "$timestamp",
  "temperature": "$(shuf -i 10-40 -n 1)",
  "humidity": "$(shuf -i 30-90 -n 1)",
  "status": "active"
}
EOF
      
      sleep 1  # 1초마다 새 데이터
    done
  }
  
  # 배치 수집기
  batch_collector() {
    local batch_counter=0
    local current_batch_file=""
    local records_in_batch=0
    
    while true; do
      # 새 배치 파일 시작
      if [ -z "$current_batch_file" ] || [ $records_in_batch -ge $batch_size ]; then
        if [ -n "$current_batch_file" ] && [ $records_in_batch -gt 0 ]; then
          echo "]" >> "$current_batch_file"
          
          # 배치 업로드
          echo "📤 Uploading batch: $current_batch_file ($records_in_batch records)"
          fs bulk-put "$feature_group" "$current_batch_file" \
            --batch-size $records_in_batch \
            --no-progress
          
          # 업로드 완료된 파일 정리
          rm "$current_batch_file"
        fi
        
        batch_counter=$((batch_counter + 1))
        current_batch_file="$batch_dir/batch_$(date +%Y%m%d_%H%M%S)_${batch_counter}.json"
        records_in_batch=0
        echo "[" > "$current_batch_file"
      fi
      
      # 스트림 파일들 처리
      for stream_file in "$stream_dir"/*.json; do
        if [ ! -f "$stream_file" ]; then
          continue
        fi
        
        # 쉼표 추가 (첫 번째 레코드가 아닌 경우)
        if [ $records_in_batch -gt 0 ]; then
          echo "," >> "$current_batch_file"
        fi
        
        # 레코드 내용 추가
        cat "$stream_file" >> "$current_batch_file"
        records_in_batch=$((records_in_batch + 1))
        
        # 처리된 스트림 파일 삭제
        rm "$stream_file"
      done
      
      sleep 5  # 5초마다 배치 수집 확인
    done
  }
  
  echo "🌊 Starting streaming upload for $feature_group"
  echo "📁 Stream directory: $stream_dir"
  echo "📦 Batch directory: $batch_dir"
  echo "📊 Batch size: $batch_size"
  
  # 백그라운드에서 데이터 생성 및 배치 수집 시작
  generate_streaming_data &
  batch_collector &
  
  echo "✅ Streaming upload setup completed"
  echo "🛑 Press Ctrl+C to stop"
  
  # 시그널 핸들러로 정리
  trap 'echo "🛑 Stopping streaming upload..."; kill $(jobs -p); exit 0' INT
  wait
}

setup_streaming_upload "sensor-data"
```

### 3. 데이터 검증 및 변환 파이프라인
```bash
#!/bin/bash
validated_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local output_log=$3
  
  # 임시 디렉토리 생성
  temp_dir=$(mktemp -d)
  validated_file="$temp_dir/validated_data.json"
  
  echo "🔍 Starting data validation and transformation..."
  
  # Python을 사용한 데이터 검증 및 변환
  python << EOF
import json
import re
from datetime import datetime, timezone
import sys

def validate_and_transform(input_file, output_file):
    """데이터 검증 및 변환"""
    
    try:
        with open(input_file, 'r') as f:
            if input_file.endswith('.json'):
                data = json.load(f)
            elif input_file.endswith('.csv'):
                import csv
                data = []
                reader = csv.DictReader(f)
                for row in reader:
                    data.append(row)
    except Exception as e:
        print(f"❌ Failed to load input file: {e}")
        return False
    
    validated_records = []
    validation_errors = []
    transformation_count = 0
    
    for i, record in enumerate(data):
        record_errors = []
        
        # 필수 필드 검증
        required_fields = ['customer_id', 'event_time']
        for field in required_fields:
            if field not in record or not record[field]:
                record_errors.append(f"Missing required field: {field}")
        
        if record_errors:
            validation_errors.append(f"Record {i}: {', '.join(record_errors)}")
            continue
        
        # 데이터 변환
        transformed_record = record.copy()
        
        # 시간 형식 정규화
        try:
            # 다양한 시간 형식을 ISO 8601로 변환
            time_str = str(record['event_time'])
            if not time_str.endswith('Z') and '+' not in time_str:
                # Unix timestamp 처리
                if time_str.isdigit():
                    dt = datetime.fromtimestamp(float(time_str), tz=timezone.utc)
                    transformed_record['event_time'] = dt.isoformat().replace('+00:00', 'Z')
                    transformation_count += 1
                # 다른 형식 처리 (예: YYYY-MM-DD HH:MM:SS)
                elif re.match(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}', time_str):
                    dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S').replace(tzinfo=timezone.utc)
                    transformed_record['event_time'] = dt.isoformat().replace('+00:00', 'Z')
                    transformation_count += 1
        except Exception as e:
            validation_errors.append(f"Record {i}: Invalid time format '{record['event_time']}': {e}")
            continue
        
        # 숫자 필드 문자열 변환
        numeric_fields = ['age', 'balance', 'price', 'quantity']
        for field in numeric_fields:
            if field in transformed_record and transformed_record[field] is not None:
                try:
                    # 숫자를 문자열로 변환 (FeatureStore 요구사항)
                    transformed_record[field] = str(float(transformed_record[field]))
                except (ValueError, TypeError):
                    # 이미 문자열이거나 변환 불가능한 경우 그대로 유지
                    pass
        
        # 문자열 필드 정리
        string_fields = ['category', 'status', 'location']
        for field in string_fields:
            if field in transformed_record and transformed_record[field] is not None:
                transformed_record[field] = str(transformed_record[field]).strip()
        
        validated_records.append(transformed_record)
    
    # 결과 저장
    with open(output_file, 'w') as f:
        json.dump(validated_records, f, indent=2)
    
    # 검증 결과 출력
    total_records = len(data)
    valid_records = len(validated_records)
    success_rate = (valid_records / total_records) * 100 if total_records > 0 else 0
    
    print(f"📊 Validation Results:")
    print(f"  Total records: {total_records}")
    print(f"  Valid records: {valid_records}")
    print(f"  Invalid records: {len(validation_errors)}")
    print(f"  Success rate: {success_rate:.2f}%")
    print(f"  Transformations applied: {transformation_count}")
    
    if validation_errors:
        print(f"❌ Validation errors:")
        for error in validation_errors[:10]:  # 처음 10개만 표시
            print(f"    {error}")
        if len(validation_errors) > 10:
            print(f"    ... and {len(validation_errors) - 10} more errors")
    
    return success_rate >= 80.0  # 80% 이상 성공률 요구

if validate_and_transform('$input_file', '$validated_file'):
    print("✅ Validation passed")
    sys.exit(0)
else:
    print("❌ Validation failed")
    sys.exit(1)
EOF
  
  if [ $? -eq 0 ]; then
    echo "🚀 Uploading validated data..."
    fs bulk-put "$feature_group" "$validated_file" \
      --output-file "$output_log" \
      --batch-size 300 \
      --continue-on-error
    
    upload_result=$?
    
    # 업로드 결과 검증
    if [ $upload_result -eq 0 ]; then
      echo "✅ Upload completed successfully"
    else
      echo "⚠ Upload completed with errors (see $output_log)"
    fi
  else
    echo "❌ Data validation failed, upload cancelled"
    upload_result=1
  fi
  
  # 임시 파일 정리
  rm -rf "$temp_dir"
  
  return $upload_result
}

validated_bulk_put "customer-profile" "raw_customer_data.csv" "upload_log.txt"
```

### 4. 증분 데이터 업데이트
```bash
#!/bin/bash
incremental_update() {
  local feature_group=$1
  local new_data_file=$2
  local last_update_marker="/tmp/.last_update_${feature_group}"
  
  # 마지막 업데이트 시간 확인
  if [ -f "$last_update_marker" ]; then
    last_update=$(cat "$last_update_marker")
    echo "📅 Last update: $last_update"
  else
    last_update="1970-01-01T00:00:00Z"
    echo "📅 No previous update found, processing all data"
  fi
  
  current_time=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
  temp_incremental="/tmp/incremental_${feature_group}_$(date +%s).json"
  
  # 증분 데이터 필터링
  echo "🔍 Filtering incremental data since $last_update..."
  
  python << EOF
import json
from datetime import datetime

def filter_incremental_data(input_file, output_file, since_time):
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    since_dt = datetime.fromisoformat(since_time.replace('Z', '+00:00'))
    incremental_records = []
    
    for record in data:
        try:
            event_time = record.get('event_time', '')
            if event_time:
                # 다양한 시간 형식 처리
                if event_time.endswith('Z'):
                    record_dt = datetime.fromisoformat(event_time.replace('Z', '+00:00'))
                else:
                    record_dt = datetime.fromisoformat(event_time)
                
                if record_dt > since_dt:
                    incremental_records.append(record)
        except Exception as e:
            print(f"Warning: Could not parse time for record {record.get('customer_id', 'unknown')}: {e}")
            # 시간 파싱 실패 시 안전하게 포함
            incremental_records.append(record)
    
    with open(output_file, 'w') as f:
        json.dump(incremental_records, f, indent=2)
    
    print(f"Filtered {len(incremental_records)} records out of {len(data)} total")
    return len(incremental_records)

record_count = filter_incremental_data('$new_data_file', '$temp_incremental', '$last_update')
if record_count == 0:
    print("No new records to update")
    exit(0)
EOF
  
  if [ $? -ne 0 ] || [ ! -s "$temp_incremental" ]; then
    echo "ℹ No incremental data to process"
    rm -f "$temp_incremental"
    return 0
  fi
  
  incremental_count=$(jq length "$temp_incremental")
  echo "📊 Found $incremental_count new/updated records"
  
  # 증분 데이터 업로드
  echo "🚀 Uploading incremental data..."
  log_file="/tmp/incremental_upload_${feature_group}_$(date +%Y%m%d_%H%M%S).log"
  
  fs bulk-put "$feature_group" "$temp_incremental" \
    --output-file "$log_file" \
    --batch-size 200 \
    --continue-on-error
  
  if [ $? -eq 0 ]; then
    # 성공 시 마커 업데이트
    echo "$current_time" > "$last_update_marker"
    echo "✅ Incremental update completed: $incremental_count records"
    echo "📝 Update marker saved: $current_time"
  else
    echo "❌ Incremental update failed (see $log_file)"
  fi
  
  # 임시 파일 정리
  rm -f "$temp_incremental"
}

# 증분 업데이트 실행
incremental_update "customer-profile" "daily_customer_updates.json"
```

### 5. 다중 소스 데이터 통합 업로드
```bash
#!/bin/bash
multi_source_upload() {
  local feature_group=$1
  local config_file=$2  # 소스 설정 JSON 파일
  
  # 임시 디렉토리 생성
  temp_dir=$(mktemp -d)
  consolidated_file="$temp_dir/consolidated_data.json"
  
  echo "🔄 Multi-source data consolidation starting..."
  
  # 설정 파일 예시:
  # {
  #   "sources": [
  #     {"name": "crm", "file": "crm_data.json", "id_field": "customer_id"},
  #     {"name": "web", "file": "web_data.csv", "id_field": "user_id", "id_mapping": {"user_id": "customer_id"}},
  #     {"name": "mobile", "file": "mobile_data.json", "id_field": "mobile_user_id", "id_mapping": {"mobile_user_id": "customer_id"}}
  #   ]
  # }
  
  python << EOF
import json
import csv
from collections import defaultdict

# 설정 파일 로드
with open('$config_file', 'r') as f:
    config = json.load(f)

consolidated_data = {}
source_stats = {}

for source_config in config['sources']:
    source_name = source_config['name']
    source_file = source_config['file']
    id_field = source_config['id_field']
    id_mapping = source_config.get('id_mapping', {})
    
    print(f"📥 Processing source: {source_name} ({source_file})")
    
    # 파일 형식에 따른 데이터 로드
    try:
        if source_file.endswith('.json'):
            with open(source_file, 'r') as f:
                source_data = json.load(f)
        elif source_file.endswith('.csv'):
            source_data = []
            with open(source_file, 'r') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    source_data.append(row)
        else:
            print(f"❌ Unsupported file format for {source_file}")
            continue
            
        records_processed = 0
        
        for record in source_data:
            # ID 매핑 적용
            record_id = record.get(id_field)
            if not record_id:
                continue
            
            # ID 필드명 변환
            if id_mapping:
                for old_field, new_field in id_mapping.items():
                    if old_field in record:
                        record[new_field] = record.pop(old_field)
                        record_id = record[new_field]
            
            # 기존 레코드와 병합
            if record_id in consolidated_data:
                # 기존 데이터와 병합 (새 데이터가 우선)
                consolidated_data[record_id].update(record)
            else:
                consolidated_data[record_id] = record.copy()
            
            # 소스 정보 추가
            if f'{source_name}_source' not in consolidated_data[record_id]:
                consolidated_data[record_id][f'{source_name}_source'] = True
                
            records_processed += 1
        
        source_stats[source_name] = records_processed
        print(f"✅ {source_name}: {records_processed} records processed")
        
    except Exception as e:
        print(f"❌ Error processing {source_name}: {e}")
        source_stats[source_name] = 0

# 통합 데이터 저장
consolidated_list = list(consolidated_data.values())
with open('$consolidated_file', 'w') as f:
    json.dump(consolidated_list, f, indent=2)

print(f"\n📊 Consolidation Summary:")
print(f"  Total unique records: {len(consolidated_list)}")
for source, count in source_stats.items():
    print(f"  {source}: {count} records")

EOF
  
  if [ ! -s "$consolidated_file" ]; then
    echo "❌ No consolidated data generated"
    rm -rf "$temp_dir"
    return 1
  fi
  
  record_count=$(jq length "$consolidated_file")
  echo "🚀 Uploading $record_count consolidated records..."
  
  # 통합된 데이터 업로드
  log_file="/tmp/multi_source_upload_$(date +%Y%m%d_%H%M%S).log"
  
  fs bulk-put "$feature_group" "$consolidated_file" \
    --output-file "$log_file" \
    --batch-size 400 \
    --max-workers 18 \
    --continue-on-error
  
  upload_result=$?
  
  if [ $upload_result -eq 0 ]; then
    echo "✅ Multi-source upload completed successfully"
  else
    echo "⚠ Multi-source upload completed with errors (see $log_file)"
  fi
  
  # 임시 파일 정리
  rm -rf "$temp_dir"
  
  return $upload_result
}

# 설정 파일 생성 예시
cat << 'EOF' > multi_source_config.json
{
  "sources": [
    {
      "name": "crm",
      "file": "crm_customers.json",
      "id_field": "customer_id"
    },
    {
      "name": "web_analytics", 
      "file": "web_activity.csv",
      "id_field": "user_id",
      "id_mapping": {"user_id": "customer_id"}
    },
    {
      "name": "mobile_app",
      "file": "mobile_usage.json", 
      "id_field": "app_user_id",
      "id_mapping": {"app_user_id": "customer_id"}
    }
  ]
}
EOF

multi_source_upload "unified-customer-features" "multi_source_config.json"
```

### 6. 오류 복구 및 재시도 시스템
```bash
#!/bin/bash
resilient_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local max_retries=3
  local base_batch_size=200
  
  # 실패 추적 디렉토리
  failure_dir="/tmp/bulk_put_failures_$(date +%s)"
  mkdir -p "$failure_dir"
  
  attempt=1
  current_batch_size=$base_batch_size
  
  while [ $attempt -le $max_retries ]; do
    echo "🔄 Attempt $attempt/$max_retries (batch size: $current_batch_size)"
    
    attempt_log="$failure_dir/attempt_${attempt}.log"
    
    if fs bulk-put "$feature_group" "$input_file" \
       --batch-size $current_batch_size \
       --output-file "$attempt_log" \
       --continue-on-error; then
      
      echo "✅ Upload succeeded on attempt $attempt"
      rm -rf "$failure_dir"
      return 0
    fi
    
    echo "❌ Attempt $attempt failed"
    
    # 마지막 시도가 아니면 재시도 준비
    if [ $attempt -lt $max_retries ]; then
      # 배치 크기 감소 (네트워크 문제 대응)
      current_batch_size=$((current_batch_size / 2))
      if [ $current_batch_size -lt 50 ]; then
        current_batch_size=50
      fi
      
      # 지수적 백오프
      sleep_time=$((2 ** attempt))
      echo "⏳ Waiting ${sleep_time}s before retry with smaller batch size..."
      sleep $sleep_time
      
      # 실패한 레코드만 추출하여 재시도 (선택적)
      if command -v jq >/dev/null && [ -s "$attempt_log" ]; then
        echo "🔍 Analyzing failed records..."
        # 여기에 실패한 레코드 추출 로직 추가 가능
      fi
    fi
    
    attempt=$((attempt + 1))
  done
  
  echo "💥 All $max_retries attempts failed"
  echo "📁 Failure logs saved in: $failure_dir"
  
  return 1
}

resilient_bulk_put "customer-profile" "problematic_data.json"
```

## 성능 최적화

### 1. 배치 크기 자동 조정
```bash
#!/bin/bash
adaptive_batch_size() {
  local input_file=$1
  local record_count
  local file_size
  local avg_record_size
  local optimal_batch_size
  
  # 파일 분석
  if [[ "$input_file" == *.json ]]; then
    record_count=$(jq length "$input_file")
  else
    record_count=$(($(wc -l < "$input_file") - 1))  # CSV 헤더 제외
  fi
  
  file_size=$(stat -c%s "$input_file")
  avg_record_size=$((file_size / record_count))
  
  # 평균 레코드 크기에 따른 배치 크기 결정
  if [ $avg_record_size -lt 1000 ]; then      # < 1KB
    optimal_batch_size=1000
  elif [ $avg_record_size -lt 5000 ]; then   # 1-5KB  
    optimal_batch_size=500
  elif [ $avg_record_size -lt 10000 ]; then  # 5-10KB
    optimal_batch_size=200
  elif [ $avg_record_size -lt 50000 ]; then  # 10-50KB
    optimal_batch_size=100
  else                                        # > 50KB
    optimal_batch_size=50
  fi
  
  # API 제한 적용
  if [ $optimal_batch_size -gt 1000 ]; then
    optimal_batch_size=1000
  fi
  
  echo $optimal_batch_size
}

# 사용 예시
BATCH_SIZE=$(adaptive_batch_size "input_data.json")
echo "📊 Optimal batch size: $BATCH_SIZE"

fs bulk-put feature-group "input_data.json" --batch-size $BATCH_SIZE
```

### 2. 시스템 리소스 기반 워커 조정
```bash
#!/bin/bash
optimal_workers() {
  local cpu_cores=$(nproc)
  local memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  local network_quality="good"  # good/medium/poor
  
  # CPU 기반 기본 워커 수
  base_workers=$((cpu_cores * 2))
  
  # 메모리 제한 적용
  if [ $memory_gb -lt 4 ]; then
    max_memory_workers=5
  elif [ $memory_gb -lt 8 ]; then
    max_memory_workers=10  
  elif [ $memory_gb -lt 16 ]; then
    max_memory_workers=15
  else
    max_memory_workers=25
  fi
  
  # 네트워크 품질 고려
  case $network_quality in
    "poor")
      network_factor=0.5
      ;;
    "medium")  
      network_factor=0.8
      ;;
    "good")
      network_factor=1.0
      ;;
  esac
  
  optimal_workers=$(echo "$base_workers * $network_factor" | bc | cut -d. -f1)
  
  # 메모리 제한 적용
  if [ $optimal_workers -gt $max_memory_workers ]; then
    optimal_workers=$max_memory_workers
  fi
  
  # 최소/최대 제한
  if [ $optimal_workers -lt 5 ]; then
    optimal_workers=5
  elif [ $optimal_workers -gt 25 ]; then
    optimal_workers=25
  fi
  
  echo $optimal_workers
}

WORKERS=$(optimal_workers)
echo "⚙ Optimal workers: $WORKERS"

fs bulk-put feature-group "data.json" --max-workers $WORKERS
```

### 3. 진행률 및 성능 모니터링
```bash
#!/bin/bash
monitored_bulk_put() {
  local feature_group=$1
  local input_file=$2
  local batch_size=${3:-200}
  
  # 진행률 파일
  progress_file="/tmp/bulk_put_progress_$$.json"
  
  # 초기 진행률 파일 생성
  total_records=$(jq length "$input_file" 2>/dev/null || echo "unknown")
  cat << EOF > "$progress_file"
{
  "total_records": $total_records,
  "processed_records": 0,
  "start_time": "$(date -Iseconds)",
  "current_batch": 0,
  "errors": 0,
  "throughput": 0
}
EOF
  
  # 백그라운드 모니터링 프로세스
  (
    while [ -f "$progress_file" ]; do
      if [ -f "$progress_file" ]; then
        start_time=$(jq -r '.start_time' "$progress_file")
        processed=$(jq -r '.processed_records' "$progress_file" 2>/dev/null || echo 0)
        total=$(jq -r '.total_records' "$progress_file")
        
        if [ "$processed" != "0" ] && [ "$total" != "unknown" ]; then
          percentage=$(( processed * 100 / total ))
          
          # 경과 시간 계산
          current_time=$(date +%s)
          start_unix=$(date -d "$start_time" +%s)
          elapsed=$((current_time - start_unix))
          
          if [ $elapsed -gt 0 ]; then
            throughput=$((processed / elapsed))
            eta=$(( (total - processed) / throughput ))
            eta_human=$(date -d "@$((current_time + eta))" '+%H:%M:%S')
            
            printf "\r⏳ Progress: %d/%d (%d%%) | Speed: %d rec/s | ETA: %s" \
              $processed $total $percentage $throughput $eta_human
          fi
        fi
      fi
      sleep 2
    done
  ) &
  monitor_pid=$!
  
  # 실제 bulk-put 실행
  start_time=$(date +%s)
  
  fs bulk-put "$feature_group" "$input_file" \
    --batch-size $batch_size \
    --output-file "/tmp/bulk_put_details_$$.log"
    
  result=$?
  end_time=$(date +%s)
  
  # 모니터링 프로세스 종료
  kill $monitor_pid 2>/dev/null
  rm -f "$progress_file"
  
  # 최종 결과 출력
  duration=$((end_time - start_time))
  echo -e "\n"
  
  if [ $result -eq 0 ]; then
    if [ "$total_records" != "unknown" ]; then
      throughput=$((total_records / duration))
      echo "✅ Upload completed: $total_records records in ${duration}s (${throughput} rec/s)"
    else
      echo "✅ Upload completed in ${duration}s"
    fi
  else
    echo "❌ Upload failed after ${duration}s"
  fi
  
  return $result
}

monitored_bulk_put "customer-profile" "customer_data.json" 300
```

## 오류 처리 및 문제 해결

### 1. 일반적인 오류 시나리오
```bash
#!/bin/bash
handle_common_errors() {
  local feature_group=$1
  local input_file=$2
  
  # 사전 검증
  echo "🔍 Pre-upload validation..."
  
  # 파일 존재 확인
  if [ ! -f "$input_file" ]; then
    echo "❌ Input file not found: $input_file"
    return 1
  fi
  
  # 파일 형식 검증
  if [[ "$input_file" == *.json ]]; then
    if ! jq empty "$input_file" 2>/dev/null; then
      echo "❌ Invalid JSON format in $input_file"
      return 1
    fi
  elif [[ "$input_file" == *.csv ]]; then
    if ! head -1 "$input_file" | grep -q ','; then
      echo "❌ Invalid CSV format in $input_file"
      return 1
    fi
  else
    echo "❌ Unsupported file format: $input_file"
    return 1
  fi
  
  # Feature Group 존재 확인
  if ! fs list -o json | jq -r '.[].FeatureGroupName' | grep -q "^$feature_group$"; then
    echo "❌ Feature group not found: $feature_group"
    echo "Available feature groups:"
    fs list -o json | jq -r '.[].FeatureGroupName' | sed 's/^/  - /'
    return 1
  fi
  
  echo "✅ Pre-validation passed"
  
  # 실제 업로드 (오류 처리 포함)
  fs bulk-put "$feature_group" "$input_file" --continue-on-error
}

handle_common_errors "customer-profile" "customer_data.json"
```

### 2. 스키마 불일치 처리
```bash
#!/bin/bash
schema_validation_upload() {
  local feature_group=$1
  local input_file=$2
  
  echo "📋 Validating against feature group schema..."
  
  # 현재 스키마 가져오기
  schema_file="/tmp/${feature_group}_schema.json"
  fs schema "$feature_group" -o json > "$schema_file"
  
  python << EOF
import json

# 스키마 로드
with open('$schema_file', 'r') as f:
    schema = json.load(f)

schema_fields = {item['FeatureName'] for item in schema}

# 입력 데이터 로드
with open('$input_file', 'r') as f:
    if '$input_file'.endswith('.json'):
        data = json.load(f)
    else:
        import csv
        data = []
        reader = csv.DictReader(f)
        for row in reader:
            data.append(row)

if not data:
    print("❌ No data found in input file")
    exit(1)

# 필드 일치 검사
sample_record = data[0]
data_fields = set(sample_record.keys())

missing_in_schema = data_fields - schema_fields
extra_in_data = data_fields - schema_fields

if missing_in_schema:
    print(f"⚠ Fields in data but not in schema: {list(missing_in_schema)}")

missing_required = schema_fields - data_fields
if missing_required:
    print(f"❌ Required schema fields missing in data: {list(missing_required)}")
    exit(1)

print("✅ Schema validation passed")
EOF
  
  if [ $? -eq 0 ]; then
    echo "🚀 Proceeding with upload..."
    fs bulk-put "$feature_group" "$input_file"
  else
    echo "❌ Schema validation failed, upload cancelled"
    return 1
  fi
  
  rm -f "$schema_file"
}

schema_validation_upload "customer-profile" "customer_data.json"
```

## 모범 사례

1. **사전 검증**: 파일 형식, 스키마 일치, 필수 필드 확인
2. **적절한 배치 크기**: 레코드 크기와 네트워크 상황 고려
3. **오류 처리**: 부분 실패 허용 및 재시도 로직 구현  
4. **모니터링**: 진행률 추적 및 성능 메트릭 수집
5. **리소스 최적화**: CPU/메모리/네트워크 상황에 맞는 설정
6. **로깅**: 상세한 처리 로그 및 오류 추적

## 관련 명령어
- `fs bulk-get`: 대량 레코드 조회
- `fs put`: 단일 레코드 저장
- `fs schema`: 스키마 정보 확인
- `fs list`: Feature Group 목록 조회