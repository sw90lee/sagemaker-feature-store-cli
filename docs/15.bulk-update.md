# bulk-update 명령어

SageMaker Feature Store의 오프라인 스토어(S3 Parquet 파일)에서 특정 컬럼 값을 효율적으로 대량 업데이트하는 명령어입니다. Athena 쿼리를 통한 최적화된 처리와 다양한 변환 함수를 지원합니다.

## 기본 사용법

```bash
fs bulk-update <feature_group_name> --column <column_name> [업데이트 방식] [옵션]
```

## 매개변수

### 필수 매개변수
- `feature_group_name`: 업데이트할 피처 그룹 이름
- `--column`: 업데이트할 컬럼명

### 업데이트 방식 (필수 - 하나만 선택)

#### 1. 단일 값 변경
- `--old-value`: 변경할 기존 값
- `--new-value`: 새로운 값

#### 2. 매핑 파일 사용
- `--mapping-file`: 매핑 파일 경로 (.json 또는 .csv)

#### 3. 조건부 매핑
- `--conditional-mapping`: 조건부 매핑 JSON 문자열

#### 4. 변환 함수
- `--transform-function`: 변환 함수 타입
  - `regex_replace`: 정규식 패턴 치환
  - `prefix_suffix`: 접두사/접미사 추가
  - `uppercase`: 대문자 변환
  - `lowercase`: 소문자 변환
  - `copy_from_column`: 다른 컬럼에서 값 복사
  - `extract_time_prefix`: 파일명/컬럼에서 시간 정보 추출

### 변환 함수별 추가 옵션

#### regex_replace
- `--regex-pattern`: 정규식 패턴 (필수)
- `--regex-replacement`: 정규식 치환 문자열 (기본값: 빈 문자열)

#### prefix_suffix
- `--prefix`: 접두사 (기본값: 빈 문자열)
- `--suffix`: 접미사 (기본값: 빈 문자열)

#### copy_from_column
- `--source-column`: 복사할 원본 컬럼명 (필수)

#### extract_time_prefix
- `--source-column`: 시간을 추출할 원본 컬럼명
- `--prefix-pattern`: 시간 추출용 정규식 패턴 (기본값: `(\d{4}-\d{2}-\d{2})`)
- `--time-format`: 시간 형식 (기본값: auto)
- `--to-iso/--no-to-iso`: 추출된 시간을 ISO 형식으로 변환 (기본값: True)

### 실행 제어 옵션
- `--dry-run`: 실제 실행하지 않고 테스트만 수행 (기본값: True)
- `--no-dry-run`: 실제로 데이터를 업데이트
- `--skip-validation`: Athena 검증 건너뛰기

### 성능 최적화 옵션
- `--filter-null-only`: 지정된 컬럼의 null 값만 업데이트 (성능 최적화)
- `--filter-column`: 추가 필터 컬럼명
- `--filter-value`: 추가 필터 값
- `--batch-size`: 배치 크기 (기본값: 1000, 최대: 10000)
- `--deduplicate/--no-deduplicate`: 중복 record_id 제거 (기본값: True)

### 관리 옵션
- `--cleanup-backups`: 백업 파일 자동 정리

## 사용 예시

### 1. 단일 값 변경

```bash
# 기본 사용법 (dry-run)
fs bulk-update my-feature-group --column status --old-value "old" --new-value "new"

# 실제 업데이트 실행
fs bulk-update my-feature-group --column status --old-value "old" --new-value "new" --no-dry-run
```

### 2. 매핑 파일 사용

#### JSON 매핑 파일 (mapping.json)
```json
{
  "ABNORMAL": "NORMAL",
  "ERROR": "FIXED",
  "PENDING": "COMPLETED",
  "null": "DEFAULT"
}
```

```bash
# 매핑 파일로 업데이트
fs bulk-update my-feature-group --column status --mapping-file mapping.json --no-dry-run
```

#### CSV 매핑 파일 (mapping.csv)
```csv
old_value,new_value
ABNORMAL,NORMAL
ERROR,FIXED
PENDING,COMPLETED
```

```bash
fs bulk-update my-feature-group --column status --mapping-file mapping.csv --no-dry-run
```

### 3. 조건부 매핑

```bash
# 복잡한 조건부 매핑
fs bulk-update my-feature-group --column result \
  --conditional-mapping '{"category": {"A": {"old": "new"}}}' --no-dry-run
```

### 4. 변환 함수 사용

#### 대소문자 변환
```bash
# 대문자로 변환
fs bulk-update my-feature-group --column data --transform-function uppercase --no-dry-run

# 소문자로 변환
fs bulk-update my-feature-group --column data --transform-function lowercase --no-dry-run
```

#### 정규식 패턴 치환
```bash
# 정규식으로 패턴 치환
fs bulk-update my-feature-group --column text \
  --transform-function regex_replace \
  --regex-pattern "error_(\d+)" \
  --regex-replacement "fixed_\1" --no-dry-run

# 패턴 제거 (빈 문자열로 치환)
fs bulk-update my-feature-group --column filename \
  --transform-function regex_replace \
  --regex-pattern "old_.*" \
  --regex-replacement "" --no-dry-run
```

#### 접두사/접미사 추가
```bash
# 접두사 추가
fs bulk-update my-feature-group --column identifier \
  --transform-function prefix_suffix \
  --prefix "NEW_" --no-dry-run

# 접미사 추가
fs bulk-update my-feature-group --column filename \
  --transform-function prefix_suffix \
  --suffix "_processed" --no-dry-run

# 접두사와 접미사 모두 추가
fs bulk-update my-feature-group --column name \
  --transform-function prefix_suffix \
  --prefix "prod_" --suffix "_v2" --no-dry-run
```

#### 다른 컬럼에서 값 복사
```bash
# 다른 컬럼의 값을 복사 (null 값만 대상)
fs bulk-update my-feature-group --column target_col \
  --transform-function copy_from_column \
  --source-column source_col \
  --filter-null-only --no-dry-run
```

#### 시간 정보 추출 및 변환
```bash
# 파일명에서 시간 추출 후 ISO 형식으로 변환
fs bulk-update mlops-feature-group --column Origin_Time \
  --transform-function extract_time_prefix \
  --prefix-pattern '(\d{14})' \
  --source-column Filename \
  --filter-null-only --no-dry-run

# 8자리 날짜 형식 추출
fs bulk-update my-feature-group --column date_field \
  --transform-function extract_time_prefix \
  --prefix-pattern '(\d{8})' \
  --source-column data_path \
  --time-format '%Y%m%d' \
  --filter-null-only --no-dry-run
```

### 5. 성능 최적화 예시

```bash
# null 값만 업데이트 (성능 최적화)
fs bulk-update my-feature-group --column status \
  --old-value "null" --new-value "default" \
  --filter-null-only --no-dry-run

# 특정 조건의 레코드만 대상
fs bulk-update my-feature-group --column status \
  --old-value "processing" --new-value "completed" \
  --filter-column environment --filter-value "production" \
  --batch-size 2000 --no-dry-run

# 백업 파일 정리와 함께
fs bulk-update my-feature-group --column RB_Result \
  --mapping-file status_mapping.json \
  --filter-null-only \
  --cleanup-backups \
  --no-dry-run
```

## 실무 사용 시나리오

### 1. 파일명에서 시간 추출
MLOps 환경에서 파일명에 포함된 타임스탬프를 Origin_Time 컬럼에 저장:

```bash
fs bulk-update {s3_bucket}\
  --column Origin_Time \
  --transform-function extract_time_prefix \
  --prefix-pattern '(\d{14})' \
  --source-column Filename \
  --filter-null-only \
  --no-dry-run
```

**적용 예시:**
- Filename: `"data_20240115103045_v1.parquet"`
- Origin_Time: `"2024-01-15T10:30:45Z"`

### 2. 상태 값 일괄 변경
비정상 상태값들을 정상 상태로 일괄 변경:

```bash
# status_mapping.json
{
  "ABNORMAL": "NORMAL",
  "ERROR": "FIXED",
  "PENDING": "COMPLETED",
  "UNKNOWN": "DEFAULT"
}

fs bulk-update production-feature-group \
  --column RB_Result \
  --mapping-file status_mapping.json \
  --filter-null-only \
  --cleanup-backups \
  --no-dry-run
```

### 3. 환경별 조건부 업데이트
프로덕션 환경의 특정 상태만 업데이트:

```bash
fs bulk-update prod-feature-group \
  --column status --old-value "processing" --new-value "completed" \
  --filter-column environment --filter-value "production" \
  --batch-size 2000 \
  --no-dry-run
```

### 4. 데이터 품질 개선
null 값들을 기본값으로 설정:

```bash
fs bulk-update my-feature-group --column score \
  --old-value "null" --new-value "0.0" \
  --filter-null-only \
  --no-dry-run
```

## DRY-RUN 모드

기본적으로 `--dry-run` 모드로 실행되어 실제 변경 없이 다음 정보를 제공합니다:

### 출력 정보
- 변경 대상 레코드 수
- 예상 소요 시간
- 세부 분석 리포트
- 변경될 값들의 미리보기
- 처리할 파일 목록

### 예상 시간 보고서
```
📊 예상 소요 시간 보고서
====================
• 총 처리 대상 파일: 150개
• 예상 처리 시간: 25분 30초
• 변경 대상 레코드: 45,231개
• 예상 백업 용량: 1.2GB
```

## 에러 처리 및 복구

### 자동 백업 시스템
- 변경 전 자동으로 로컬 백업 생성
- 백업 위치: `./backups/` 폴더
- 백업 파일명: `backup_[feature-group]_[column]_[timestamp].parquet`

### 실패 리포트
처리 실패한 파일들에 대한 상세 리포트가 자동 생성됩니다:
- `failed_files_[feature-group]_[column]_[timestamp].json`
- `failed_files_[feature-group]_[column]_[timestamp].txt`

### 중복 데이터 처리
- `--deduplicate` (기본값): EventTime 기준으로 최신 데이터만 유지
- 중복 제거 후 업데이트 수행

## 성능 최적화 가이드

### 1. 파일 선별 최적화
```bash
# Athena를 통한 대상 파일만 선별
--filter-null-only  # null 값이 있는 파일만 처리
```

### 2. 배치 크기 조정
```bash
--batch-size 500   # 작은 배치 (안정성 우선)
--batch-size 2000  # 큰 배치 (성능 우선)
```

### 3. 필터 조건 활용
```bash
# 특정 조건의 레코드만 처리
--filter-column region --filter-value "us-east-1"
```

### 4. 병렬 처리
- 최대 5개 워커로 병렬 처리
- 대용량 데이터는 백그라운드 실행 권장 (`nohup`, `screen`)

## 안전성 기능

### 1. 기본 안전 설정
- 기본적으로 `--dry-run` 모드
- 실제 변경은 `--no-dry-run` 플래그 필요
- 자동 백업 생성

### 2. 데이터 무결성
- EventTime 자동 업데이트
- Feature Store 동기화 보장
- 중복 record_id 자동 제거

### 3. 롤백 지원
백업 파일을 이용한 수동 롤백 가능

## 권한 요구사항

### AWS 권한
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::sagemaker-*",
        "arn:aws:s3:::sagemaker-*/*"
      ]
    },
    {
      "Effect": "Allow", 
      "Action": [
        "athena:StartQueryExecution",
        "athena:GetQueryExecution",
        "athena:GetQueryResults"
      ],
      "Resource": "*"
    }
  ]
}
```

### 로컬 시스템
- 충분한 디스크 공간 (백업 파일 저장용)
- 파일 읽기/쓰기 권한

## 주의사항

### ⚠️ 중요 경고
1. **데이터 백업**: 중요한 데이터는 별도 백업 후 진행
2. **테스트 환경**: 프로덕션 적용 전 테스트 환경에서 검증
3. **리소스 사용량**: 대용량 데이터 처리 시 충분한 메모리 확보
4. **비용 고려**: S3 읽기/쓰기 비용 발생

### 💡 모범 사례
1. **단계적 접근**: 작은 데이터셋부터 테스트
2. **모니터링**: 처리 로그 및 실패 리포트 확인
3. **검증**: 업데이트 후 데이터 품질 검증
4. **문서화**: 변경 사항 기록 유지

## 트러블슈팅

### 일반적인 문제

#### 1. 메모리 부족
```bash
# 배치 크기 줄이기
--batch-size 200
```

#### 2. 권한 오류
- AWS 프로필 확인: `aws configure list`
- S3 버킷 접근 권한 확인

#### 3. Athena 쿼리 실패
```bash
# Athena 검증 건너뛰기
--skip-validation
```

#### 4. 네트워크 타임아웃
- 안정적인 네트워크 환경에서 실행
- 배치 크기 조정

### 로그 파일 위치
- 실행 로그: 콘솔 출력
- 실패 리포트: `./failed_files_*.json/txt`
- 백업 파일: `./backups/`

## 관련 명령어

- `fs analyze`: 피처 스토어 분석
- `fs clear`: 데이터 삭제 및 중복 제거
- `fs migrate`: 피처 그룹 간 데이터 이동
- `fs read-parquet`: Parquet 파일 분석