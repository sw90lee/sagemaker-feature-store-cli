# 7. export - 오프라인 스토어 데이터 내보내기

## 개요
Feature Group의 Offline Store(S3)에서 데이터를 조회하여 CSV, JSON, Parquet 형식으로 내보냅니다. Athena 쿼리를 통해 데이터를 추출하며 조건부 필터링, 컬럼 선택, 데이터 변환 등의 기능을 제공합니다.

## 기본 사용법
```bash
fs export FEATURE_GROUP_NAME OUTPUT_FILE [OPTIONS]
```

## 필수 인자
- `FEATURE_GROUP_NAME`: 내보낼 Feature Group 이름
- `OUTPUT_FILE`: 저장할 파일 경로

## 옵션
- `--format [csv|json|parquet]`: 출력 형식 (기본값: 파일 확장자로 자동 결정)
- `--columns TEXT`: 내보낼 컬럼들 (쉼표로 구분, 기본값: 전체)
- `--where TEXT`: SQL WHERE 조건절
- `--limit INTEGER`: 최대 레코드 수 제한
- `--compress/--no-compress`: 압축 여부 (기본값: False)
- `--online-compatible`: Online Store 호환 형식으로 변환
- `--column-mapping TEXT`: 컬럼명 매핑 (old:new 형식)
- `--dry-run`: 실제 실행 없이 쿼리만 확인
- `--query-timeout INTEGER`: Athena 쿼리 타임아웃 (초, 기본값: 300)

## 상세 사용 예시

### 1. 기본 전체 데이터 내보내기
```bash
# CSV 형식으로 전체 데이터 내보내기
fs export customer-profile customer_data.csv

# JSON 형식으로 내보내기
fs export customer-profile customer_data.json

# Parquet 형식으로 내보내기
fs export customer-profile customer_data.parquet
```

### 2. 특정 컬럼만 내보내기
```bash
fs export customer-profile selected_data.csv \
  --columns "customer_id,age,balance,category,event_time"
```

### 3. 조건부 데이터 내보내기
```bash
# 최근 30일 데이터만
fs export customer-profile recent_customers.csv \
  --where "event_time >= current_date - interval '30' day"

# 특정 카테고리와 잔액 조건
fs export customer-profile premium_customers.csv \
  --where "category = 'premium' AND balance > 1000" \
  --columns "customer_id,balance,last_login"

# 연령대별 필터링
fs export customer-profile young_customers.csv \
  --where "age BETWEEN 20 AND 30" \
  --limit 5000
```

### 4. 데이터 변환 및 매핑
```bash
# 컬럼명 변경하여 내보내기
fs export customer-profile mapped_data.csv \
  --column-mapping "customer_id:id,event_time:timestamp,category:segment"

# Online Store 호환 형식
fs export customer-profile online_format.json \
  --online-compatible \
  --columns "customer_id,age,balance"
```

### 5. 압축 및 대용량 처리
```bash
# 압축하여 내보내기
fs export large-feature-group compressed_data.csv.gz \
  --compress \
  --limit 100000

# Parquet으로 효율적 저장
fs export analytics-features analytics_data.parquet \
  --where "event_time >= '2024-01-01'"
```

## 고급 사용 시나리오

### 1. 시계열 데이터 분석 내보내기
```bash
#!/bin/bash
export_timeseries_analysis() {
  local feature_group=$1
  local start_date=$2
  local end_date=$3
  local output_dir=$4
  
  mkdir -p "$output_dir"
  
  echo "📊 Exporting time series data from $start_date to $end_date..."
  
  # 일별 데이터 내보내기
  current_date="$start_date"
  while [[ "$current_date" <= "$end_date" ]]; do
    output_file="$output_dir/daily_$(date -d "$current_date" +%Y%m%d).csv"
    
    echo "📅 Exporting data for $current_date..."
    fs export "$feature_group" "$output_file" \
      --where "DATE(event_time) = DATE('$current_date')" \
      --columns "customer_id,event_time,transaction_amount,product_category" \
      --compress
    
    if [ $? -eq 0 ]; then
      record_count=$(zcat "$output_file.gz" | wc -l)
      record_count=$((record_count - 1))  # 헤더 제외
      echo "✅ $current_date: $record_count records exported"
    else
      echo "❌ Failed to export data for $current_date"
    fi
    
    current_date=$(date -d "$current_date + 1 day" +%Y-%m-%d)
  done
  
  # 전체 기간 요약 데이터
  echo "📈 Creating summary report..."
  fs export "$feature_group" "$output_dir/period_summary.csv" \
    --where "event_time BETWEEN '$start_date' AND '$end_date'" \
    --columns "customer_id,SUM(transaction_amount) as total_amount,COUNT(*) as transaction_count" \
    --format csv
  
  echo "🎉 Time series export completed: $output_dir"
}

export_timeseries_analysis "transaction-features" "2024-01-01" "2024-01-31" "/exports/january_2024"
```

### 2. A/B 테스트 결과 내보내기
```bash
#!/bin/bash
export_ab_test_results() {
  local feature_group=$1
  local experiment_id=$2
  local output_dir=$3
  
  mkdir -p "$output_dir"
  
  echo "🧪 Exporting A/B test results for experiment: $experiment_id"
  
  # Control 그룹 데이터
  echo "📊 Exporting control group..."
  fs export "$feature_group" "$output_dir/control_group.csv" \
    --where "experiment_id = '$experiment_id' AND test_group = 'control'" \
    --columns "user_id,conversion_rate,engagement_score,revenue,event_time"
  
  # Treatment 그룹 데이터
  echo "📊 Exporting treatment group..."
  fs export "$feature_group" "$output_dir/treatment_group.csv" \
    --where "experiment_id = '$experiment_id' AND test_group = 'treatment'" \
    --columns "user_id,conversion_rate,engagement_score,revenue,event_time"
  
  # 전체 실험 요약
  echo "📈 Exporting experiment summary..."
  fs export "$feature_group" "$output_dir/experiment_summary.csv" \
    --where "experiment_id = '$experiment_id'" \
    --columns "test_group,AVG(conversion_rate) as avg_conversion,AVG(revenue) as avg_revenue,COUNT(*) as user_count"
  
  # 통계 분석을 위한 Python 스크립트 생성
  cat << 'EOF' > "$output_dir/analyze_results.py"
import pandas as pd
import numpy as np
from scipy import stats

# 데이터 로드
control = pd.read_csv('control_group.csv')
treatment = pd.read_csv('treatment_group.csv')

print("🧪 A/B Test Analysis Results")
print("=" * 50)

print(f"Control group size: {len(control)}")
print(f"Treatment group size: {len(treatment)}")

# 평균 비교
metrics = ['conversion_rate', 'engagement_score', 'revenue']
for metric in metrics:
    if metric in control.columns and metric in treatment.columns:
        control_mean = control[metric].mean()
        treatment_mean = treatment[metric].mean()
        
        # t-test 수행
        t_stat, p_value = stats.ttest_ind(control[metric].dropna(), 
                                          treatment[metric].dropna())
        
        print(f"\n{metric.upper()}:")
        print(f"  Control: {control_mean:.4f}")
        print(f"  Treatment: {treatment_mean:.4f}")
        print(f"  Difference: {((treatment_mean - control_mean) / control_mean * 100):.2f}%")
        print(f"  P-value: {p_value:.4f}")
        print(f"  Significant: {'Yes' if p_value < 0.05 else 'No'}")

print("\n✅ Analysis complete!")
EOF
  
  echo "🐍 Running statistical analysis..."
  cd "$output_dir" && python analyze_results.py
  
  echo "🎯 A/B test export completed: $output_dir"
}

export_ab_test_results "experiment-features" "exp_20240115" "/exports/ab_test_exp20240115"
```

### 3. 고객 세분화 데이터 내보내기
```bash
#!/bin/bash
export_customer_segmentation() {
  local feature_group=$1
  local output_dir=$2
  
  mkdir -p "$output_dir"
  
  echo "🎯 Exporting customer segmentation data..."
  
  # VIP 고객 (고액 거래, 높은 활동)
  fs export "$feature_group" "$output_dir/vip_customers.json" \
    --where "lifetime_value > 10000 AND last_activity >= current_date - interval '7' day" \
    --columns "customer_id,lifetime_value,transaction_frequency,preferred_categories" \
    --format json
  
  # 활성 고객
  fs export "$feature_group" "$output_dir/active_customers.csv" \
    --where "last_activity >= current_date - interval '30' day AND transaction_count > 5" \
    --columns "customer_id,age,location,purchase_behavior_score"
  
  # 휴면 고객 (재활성화 대상)
  fs export "$feature_group" "$output_dir/dormant_customers.csv" \
    --where "last_activity < current_date - interval '90' day AND lifetime_value > 1000" \
    --columns "customer_id,last_activity,lifetime_value,contact_preferences"
  
  # 신규 고객 (온보딩 대상)
  fs export "$feature_group" "$output_dir/new_customers.csv" \
    --where "registration_date >= current_date - interval '30' day" \
    --columns "customer_id,registration_date,first_purchase_date,acquisition_channel"
  
  # 위험 고객 (이탈 예측 대상)
  fs export "$feature_group" "$output_dir/at_risk_customers.csv" \
    --where "churn_probability > 0.7" \
    --columns "customer_id,churn_probability,last_activity,satisfaction_score"
  
  # 세분화 요약 리포트 생성
  cat << 'EOF' > "$output_dir/segmentation_report.py"
import json
import pandas as pd
from datetime import datetime

print("🎯 Customer Segmentation Report")
print("=" * 50)
print(f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

segments = {
    'VIP Customers': 'vip_customers.json',
    'Active Customers': 'active_customers.csv', 
    'Dormant Customers': 'dormant_customers.csv',
    'New Customers': 'new_customers.csv',
    'At-Risk Customers': 'at_risk_customers.csv'
}

total_customers = 0
segment_stats = {}

for segment_name, filename in segments.items():
    try:
        if filename.endswith('.json'):
            with open(filename, 'r') as f:
                data = json.load(f)
            count = len(data)
        else:
            df = pd.read_csv(filename)
            count = len(df)
        
        segment_stats[segment_name] = count
        total_customers += count
        print(f"{segment_name}: {count:,} customers")
        
    except FileNotFoundError:
        print(f"{segment_name}: No data available")
        segment_stats[segment_name] = 0

print(f"\nTotal customers across all segments: {total_customers:,}")

# 세그먼트별 비율 계산
if total_customers > 0:
    print("\nSegment Distribution:")
    for segment, count in segment_stats.items():
        percentage = (count / total_customers) * 100
        print(f"  {segment}: {percentage:.1f}%")

print("\n✅ Segmentation analysis complete!")
EOF
  
  cd "$output_dir" && python segmentation_report.py
  
  echo "🎉 Customer segmentation export completed: $output_dir"
}

export_customer_segmentation "customer-analytics" "/exports/customer_segments"
```

### 4. 머신러닝 모델 훈련 데이터 준비
```bash
#!/bin/bash
prepare_ml_training_data() {
  local feature_group=$1
  local model_type=$2
  local output_dir=$3
  local test_split_ratio=${4:-0.2}
  
  mkdir -p "$output_dir"
  
  case $model_type in
    "churn_prediction")
      target_column="churned"
      feature_columns="age,tenure_months,total_purchases,avg_order_value,support_tickets,last_activity_days"
      where_clause="registration_date <= current_date - interval '6' month"
      ;;
    "recommendation")
      target_column="purchased_items"
      feature_columns="user_id,age,location,browsing_history,purchase_categories,session_duration"
      where_clause="last_activity >= current_date - interval '90' day"
      ;;
    "fraud_detection")
      target_column="is_fraud"
      feature_columns="transaction_amount,merchant_category,hour_of_day,day_of_week,location_risk_score"
      where_clause="transaction_date >= current_date - interval '365' day"
      ;;
    *)
      echo "❌ Unknown model type: $model_type"
      return 1
      ;;
  esac
  
  echo "🤖 Preparing ML training data for: $model_type"
  echo "🎯 Target: $target_column"
  echo "📊 Features: $feature_columns"
  
  # 전체 데이터 내보내기
  full_data_file="$output_dir/full_dataset.csv"
  fs export "$feature_group" "$full_data_file" \
    --columns "$feature_columns,$target_column" \
    --where "$where_clause" \
    --compress
  
  if [ ! -f "${full_data_file}.gz" ]; then
    echo "❌ Failed to export training data"
    return 1
  fi
  
  # Python을 사용한 데이터 분할
  cat << EOF > "$output_dir/split_data.py"
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# 데이터 로드
print("📊 Loading dataset...")
df = pd.read_csv('${full_data_file}.gz', compression='gzip')

print(f"Dataset shape: {df.shape}")
print(f"Target column: '$target_column'")

# 기본 통계
print("\\nDataset Statistics:")
print(f"  Total records: {len(df):,}")
print(f"  Features: {len(df.columns) - 1}")

if '$target_column' in df.columns:
    target_dist = df['$target_column'].value_counts()
    print(f"  Target distribution:")
    for value, count in target_dist.items():
        percentage = (count / len(df)) * 100
        print(f"    {value}: {count:,} ({percentage:.1f}%)")

# 훈련/테스트 분할
print(f"\\n🔀 Splitting data (test ratio: $test_split_ratio)...")

if '$target_column' in df.columns:
    X = df.drop('$target_column', axis=1)
    y = df['$target_column']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=$test_split_ratio, random_state=42, stratify=y
    )
    
    # 훈련 데이터 저장
    train_data = pd.concat([X_train, y_train], axis=1)
    train_data.to_csv('train_data.csv', index=False)
    
    # 테스트 데이터 저장
    test_data = pd.concat([X_test, y_test], axis=1)
    test_data.to_csv('test_data.csv', index=False)
    
    print(f"✅ Training data: {len(train_data):,} records -> train_data.csv")
    print(f"✅ Test data: {len(test_data):,} records -> test_data.csv")
    
else:
    # 타겟 없는 경우 (비지도 학습)
    test_size = int(len(df) * $test_split_ratio)
    train_data = df[:-test_size]
    test_data = df[-test_size:]
    
    train_data.to_csv('train_data.csv', index=False)
    test_data.to_csv('test_data.csv', index=False)
    
    print(f"✅ Training data: {len(train_data):,} records -> train_data.csv")
    print(f"✅ Test data: {len(test_data):,} records -> test_data.csv")

# 피처 메타데이터 생성
feature_info = {
    'numeric_features': [],
    'categorical_features': [],
    'datetime_features': []
}

for col in df.columns:
    if col == '$target_column':
        continue
        
    if df[col].dtype in ['int64', 'float64']:
        feature_info['numeric_features'].append(col)
    elif 'time' in col.lower() or 'date' in col.lower():
        feature_info['datetime_features'].append(col)
    else:
        feature_info['categorical_features'].append(col)

import json
with open('feature_metadata.json', 'w') as f:
    json.dump(feature_info, f, indent=2)

print("\\n📋 Feature metadata saved to: feature_metadata.json")
print("🎉 ML training data preparation completed!")
EOF
  
  cd "$output_dir" && python split_data.py
  
  # 데이터 품질 리포트 생성
  cat << 'EOF' > "$output_dir/data_quality_report.py"
import pandas as pd
import numpy as np

def generate_quality_report(filename):
    df = pd.read_csv(filename)
    
    print(f"\n📊 Data Quality Report: {filename}")
    print("-" * 50)
    
    # 기본 정보
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # 누락값 분석
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        print(f"\n⚠ Missing values:")
        for col, missing_count in missing_data[missing_data > 0].items():
            percentage = (missing_count / len(df)) * 100
            print(f"  {col}: {missing_count} ({percentage:.1f}%)")
    else:
        print("\n✅ No missing values found")
    
    # 중복 레코드
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        print(f"\n⚠ Duplicate records: {duplicates}")
    else:
        print("\n✅ No duplicate records")
    
    # 숫자형 컬럼 통계
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"\n📈 Numeric columns summary:")
        print(df[numeric_cols].describe())
    
    return df.shape[0]

# 각 데이터셋 품질 확인
train_records = generate_quality_report('train_data.csv')
test_records = generate_quality_report('test_data.csv')

print(f"\n🎯 Final Summary:")
print(f"  Training records: {train_records:,}")
print(f"  Test records: {test_records:,}")
print(f"  Total: {train_records + test_records:,}")
EOF
  
  cd "$output_dir" && python data_quality_report.py
  
  echo "🤖 ML training data preparation completed: $output_dir"
}

prepare_ml_training_data "customer-features" "churn_prediction" "/ml_data/churn_model" 0.2
```

### 5. 데이터 웨어하우스 통합 내보내기
```bash
#!/bin/bash
export_for_data_warehouse() {
  local source_feature_groups=("customer-profile" "transaction-history" "product-catalog")
  local output_dir=$1
  local export_date=${2:-$(date +%Y-%m-%d)}
  
  mkdir -p "$output_dir"
  
  echo "🏢 Exporting data for data warehouse integration"
  echo "📅 Export date: $export_date"
  
  # 각 Feature Group에서 데이터 내보내기
  for fg in "${source_feature_groups[@]}"; do
    echo "📊 Exporting from $fg..."
    
    # Parquet 형식으로 효율적 저장
    fs export "$fg" "$output_dir/${fg//-/_}_${export_date}.parquet" \
      --where "DATE(event_time) = DATE('$export_date')" \
      --format parquet \
      --compress
    
    if [ $? -eq 0 ]; then
      # 메타데이터 생성
      record_count=$(python3 -c "
import pandas as pd
df = pd.read_parquet('$output_dir/${fg//-/_}_${export_date}.parquet')
print(len(df))
")
      
      echo "✅ $fg: $record_count records exported"
      
      # 스키마 정보 저장
      fs schema "$fg" -o json > "$output_dir/${fg//-/_}_schema.json"
      
    else
      echo "❌ Failed to export from $fg"
    fi
  done
  
  # 통합 메타데이터 생성
  cat << EOF > "$output_dir/export_manifest.json"
{
  "export_date": "$export_date",
  "export_timestamp": "$(date -Iseconds)",
  "feature_groups": [
$(for fg in "${source_feature_groups[@]}"; do
  echo "    {\"name\": \"$fg\", \"file\": \"${fg//-/_}_${export_date}.parquet\", \"schema\": \"${fg//-/_}_schema.json\"}"
  if [ "$fg" != "${source_feature_groups[-1]}" ]; then echo ","; fi
done)
  ]
}
EOF
  
  # SQL DDL 생성 (데이터 웨어하우스용)
  cat << 'EOF' > "$output_dir/create_tables.sql"
-- Data warehouse table creation script
-- Generated for SageMaker FeatureStore export

EOF
  
  for fg in "${source_feature_groups[@]}"; do
    table_name="${fg//-/_}"
    echo "-- Table for $fg" >> "$output_dir/create_tables.sql"
    echo "CREATE TABLE IF NOT EXISTS ${table_name} (" >> "$output_dir/create_tables.sql"
    
    # 스키마에서 컬럼 정의 생성
    python3 << EOF >> "$output_dir/create_tables.sql"
import json

with open('$output_dir/${fg//-/_}_schema.json', 'r') as f:
    schema = json.load(f)

columns = []
for feature in schema:
    name = feature['FeatureName']
    ftype = feature['FeatureType']
    
    if ftype == 'String':
        sql_type = 'VARCHAR(255)'
    elif ftype == 'Integral':
        sql_type = 'BIGINT'
    elif ftype == 'Fractional':
        sql_type = 'DOUBLE'
    else:
        sql_type = 'VARCHAR(255)'
    
    columns.append(f"    {name} {sql_type}")

print(",\\n".join(columns))
EOF
    
    echo "" >> "$output_dir/create_tables.sql"
    echo ");" >> "$output_dir/create_tables.sql"
    echo "" >> "$output_dir/create_tables.sql"
  done
  
  echo "🏢 Data warehouse export completed: $output_dir"
  echo "📋 Manifest: $output_dir/export_manifest.json"
  echo "🗃 DDL script: $output_dir/create_tables.sql"
}

export_for_data_warehouse "/exports/data_warehouse" "2024-01-15"
```

### 6. 실시간 내보내기 스케줄링
```bash
#!/bin/bash
setup_scheduled_export() {
  local feature_group=$1
  local export_schedule=$2  # hourly, daily, weekly
  local output_base_dir=$3
  
  echo "⏰ Setting up scheduled export for $feature_group ($export_schedule)"
  
  # 스케줄링 스크립트 생성
  script_path="/usr/local/bin/scheduled_export_${feature_group}.sh"
  
  cat << EOF > "$script_path"
#!/bin/bash
# Scheduled export script for $feature_group
# Schedule: $export_schedule

FEATURE_GROUP="$feature_group"
BASE_DIR="$output_base_dir"
LOG_DIR="\$BASE_DIR/logs"

mkdir -p "\$LOG_DIR"

# 시간 기반 디렉토리 생성
case "$export_schedule" in
  "hourly")
    export_dir="\$BASE_DIR/\$(date +%Y/%m/%d/%H)"
    time_filter="event_time >= current_timestamp - interval '1' hour"
    ;;
  "daily")
    export_dir="\$BASE_DIR/\$(date +%Y/%m/%d)"
    time_filter="DATE(event_time) = DATE(current_date - interval '1' day)"
    ;;
  "weekly")
    export_dir="\$BASE_DIR/\$(date +%Y/%V)"
    time_filter="event_time >= current_date - interval '7' day"
    ;;
esac

mkdir -p "\$export_dir"

# 로그 파일 설정
log_file="\$LOG_DIR/export_\$(date +%Y%m%d_%H%M%S).log"

echo "🚀 Starting scheduled export at \$(date)" | tee "\$log_file"
echo "📂 Export directory: \$export_dir" | tee -a "\$log_file"

# 데이터 내보내기
output_file="\$export_dir/\${FEATURE_GROUP}_\$(date +%Y%m%d_%H%M%S).parquet"

fs export "\$FEATURE_GROUP" "\$output_file" \\
  --where "\$time_filter" \\
  --format parquet \\
  --compress \\
  --query-timeout 600 2>&1 | tee -a "\$log_file"

if [ \${PIPESTATUS[0]} -eq 0 ]; then
  record_count=\$(python3 -c "
import pandas as pd
try:
  df = pd.read_parquet('\$output_file')
  print(len(df))
except:
  print(0)
")
  echo "✅ Export completed: \$record_count records" | tee -a "\$log_file"
  
  # 성공 메트릭 기록
  echo "export_success_count 1 \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
  echo "export_record_count \$record_count \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
else
  echo "❌ Export failed" | tee -a "\$log_file"
  echo "export_failure_count 1 \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
fi

# 오래된 파일 정리 (30일 초과)
find "\$BASE_DIR" -name "*.parquet" -mtime +30 -delete
find "\$LOG_DIR" -name "*.log" -mtime +7 -delete

echo "🏁 Scheduled export completed at \$(date)" | tee -a "\$log_file"
EOF
  
  chmod +x "$script_path"
  
  # Crontab 엔트리 생성
  case $export_schedule in
    "hourly")
      cron_expr="0 * * * *"
      ;;
    "daily")
      cron_expr="0 1 * * *"  # 매일 오전 1시
      ;;
    "weekly")
      cron_expr="0 2 * * 0"  # 매주 일요일 오전 2시
      ;;
  esac
  
  echo "📅 Cron expression: $cron_expr"
  echo "📝 To install the cron job, run:"
  echo "   (crontab -l 2>/dev/null; echo '$cron_expr $script_path') | crontab -"
  
  echo "✅ Scheduled export setup completed"
  echo "📋 Script location: $script_path"
}

setup_scheduled_export "customer-activity" "daily" "/exports/scheduled"
```

## 성능 최적화

### 1. Athena 쿼리 최적화
```bash
#!/bin/bash
optimize_export_query() {
  local feature_group=$1
  local output_file=$2
  
  echo "🚀 Running optimized export with query analysis..."
  
  # 먼저 dry-run으로 쿼리 계획 확인
  fs export "$feature_group" "$output_file" \
    --dry-run \
    --columns "customer_id,event_time,balance" \
    --where "event_time >= current_date - interval '7' day"
  
  echo "📊 Query plan analysis completed"
  
  # 파티션을 활용한 최적화된 쿼리
  fs export "$feature_group" "$output_file" \
    --where "year=2024 AND month=1 AND day>=15" \
    --columns "customer_id,balance,category" \
    --format parquet \
    --compress
}

optimize_export_query "customer-profile" "optimized_export.parquet"
```

### 2. 대용량 데이터 분할 처리
```bash
#!/bin/bash
chunked_export() {
  local feature_group=$1
  local output_dir=$2
  local chunk_days=${3:-7}
  local start_date=$4
  local end_date=$5
  
  mkdir -p "$output_dir"
  
  echo "📦 Chunked export: $chunk_days days per chunk"
  
  current_date="$start_date"
  chunk_number=1
  
  while [[ "$current_date" <= "$end_date" ]]; do
    chunk_end_date=$(date -d "$current_date + $chunk_days days" +%Y-%m-%d)
    if [[ "$chunk_end_date" > "$end_date" ]]; then
      chunk_end_date="$end_date"
    fi
    
    echo "📊 Chunk $chunk_number: $current_date to $chunk_end_date"
    
    fs export "$feature_group" "$output_dir/chunk_${chunk_number}.parquet" \
      --where "event_time BETWEEN '$current_date' AND '$chunk_end_date'" \
      --format parquet \
      --compress \
      --query-timeout 900
    
    if [ $? -eq 0 ]; then
      echo "✅ Chunk $chunk_number completed"
    else
      echo "❌ Chunk $chunk_number failed"
    fi
    
    current_date=$(date -d "$chunk_end_date + 1 day" +%Y-%m-%d)
    chunk_number=$((chunk_number + 1))
  done
  
  echo "🎉 Chunked export completed: $output_dir"
}

chunked_export "large-feature-group" "/exports/chunked" 7 "2024-01-01" "2024-03-31"
```

## 데이터 형식 최적화

### 1. 형식별 최적 사용 사례
```bash
#!/bin/bash
format_optimization_guide() {
  local feature_group=$1
  local base_output=$2
  
  echo "📋 Testing different export formats for optimization..."
  
  # CSV - 호환성 우선
  echo "📄 Exporting as CSV (highest compatibility)..."
  time fs export "$feature_group" "${base_output}.csv" \
    --limit 10000 \
    --compress
  
  # JSON - 구조화된 데이터
  echo "📋 Exporting as JSON (structured data)..."  
  time fs export "$feature_group" "${base_output}.json" \
    --limit 10000 \
    --online-compatible
  
  # Parquet - 분석 최적화
  echo "🗜 Exporting as Parquet (analytics optimized)..."
  time fs export "$feature_group" "${base_output}.parquet" \
    --limit 10000 \
    --format parquet
  
  # 파일 크기 비교
  echo "📊 File size comparison:"
  ls -lh "${base_output}".* | awk '{print $9, $5}'
}

format_optimization_guide "test-features" "/tmp/format_test"
```

## 오류 처리 및 문제 해결

### 1. Athena 쿼리 타임아웃 처리
```bash
#!/bin/bash
handle_query_timeout() {
  local feature_group=$1
  local output_file=$2
  local max_timeout=1800  # 30분
  
  echo "⏱ Handling potential query timeouts..."
  
  # 점진적으로 타임아웃 증가하며 재시도
  timeouts=(300 600 900 1800)
  
  for timeout in "${timeouts[@]}"; do
    echo "🔄 Trying with ${timeout}s timeout..."
    
    if fs export "$feature_group" "$output_file" \
       --query-timeout $timeout \
       --limit 100000; then
      echo "✅ Export succeeded with ${timeout}s timeout"
      return 0
    else
      echo "❌ Failed with ${timeout}s timeout"
    fi
  done
  
  echo "💥 All timeout attempts failed"
  return 1
}

handle_query_timeout "large-feature-group" "timeout_test.csv"
```

## 모범 사례

1. **적절한 형식 선택**: CSV(호환성), JSON(구조화), Parquet(성능)
2. **WHERE 절 최적화**: 파티션 키 활용, 인덱스 컬럼 사용
3. **컬럼 선택**: 필요한 컬럼만 선택하여 전송량 최소화
4. **압축 활용**: 대용량 데이터는 압축하여 스토리지 절약
5. **청크 단위 처리**: 매우 큰 데이터셋은 분할 처리
6. **쿼리 검증**: dry-run으로 쿼리 미리 확인

## 관련 명령어
- `fs list`: Feature Group 목록 확인
- `fs schema`: 스키마 정보 조회
- `fs bulk-get`: Online Store에서 대량 조회
- `fs analyze`: Offline Store 사용량 분석