# 7. export - ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°

## ê°œìš”
Feature Groupì˜ Offline Store(S3)ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ CSV, JSON, Parquet í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤. Athena ì¿¼ë¦¬ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ë©° ì¡°ê±´ë¶€ í•„í„°ë§, ì»¬ëŸ¼ ì„ íƒ, ë°ì´í„° ë³€í™˜ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
fs export FEATURE_GROUP_NAME OUTPUT_FILE [OPTIONS]
```

## í•„ìˆ˜ ì¸ì
- `FEATURE_GROUP_NAME`: ë‚´ë³´ë‚¼ Feature Group ì´ë¦„
- `OUTPUT_FILE`: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ

## ì˜µì…˜
- `--format [csv|json|parquet]`: ì¶œë ¥ í˜•ì‹ (ê¸°ë³¸ê°’: íŒŒì¼ í™•ì¥ìë¡œ ìë™ ê²°ì •)
- `--columns TEXT`: ë‚´ë³´ë‚¼ ì»¬ëŸ¼ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„, ê¸°ë³¸ê°’: ì „ì²´)
- `--where TEXT`: SQL WHERE ì¡°ê±´ì ˆ
- `--limit INTEGER`: ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ ì œí•œ
- `--compress/--no-compress`: ì••ì¶• ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
- `--online-compatible`: Online Store í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- `--column-mapping TEXT`: ì»¬ëŸ¼ëª… ë§¤í•‘ (old:new í˜•ì‹)
- `--dry-run`: ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì¿¼ë¦¬ë§Œ í™•ì¸
- `--query-timeout INTEGER`: Athena ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 300)

## ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
```bash
# CSV í˜•ì‹ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
fs export customer-profile customer_data.csv

# JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
fs export customer-profile customer_data.json

# Parquet í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
fs export customer-profile customer_data.parquet
```

### 2. íŠ¹ì • ì»¬ëŸ¼ë§Œ ë‚´ë³´ë‚´ê¸°
```bash
fs export customer-profile selected_data.csv \
  --columns "customer_id,age,balance,category,event_time"
```

### 3. ì¡°ê±´ë¶€ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
```bash
# ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ
fs export customer-profile recent_customers.csv \
  --where "event_time >= current_date - interval '30' day"

# íŠ¹ì • ì¹´í…Œê³ ë¦¬ì™€ ì”ì•¡ ì¡°ê±´
fs export customer-profile premium_customers.csv \
  --where "category = 'premium' AND balance > 1000" \
  --columns "customer_id,balance,last_login"

# ì—°ë ¹ëŒ€ë³„ í•„í„°ë§
fs export customer-profile young_customers.csv \
  --where "age BETWEEN 20 AND 30" \
  --limit 5000
```

### 4. ë°ì´í„° ë³€í™˜ ë° ë§¤í•‘
```bash
# ì»¬ëŸ¼ëª… ë³€ê²½í•˜ì—¬ ë‚´ë³´ë‚´ê¸°
fs export customer-profile mapped_data.csv \
  --column-mapping "customer_id:id,event_time:timestamp,category:segment"

# Online Store í˜¸í™˜ í˜•ì‹
fs export customer-profile online_format.json \
  --online-compatible \
  --columns "customer_id,age,balance"
```

### 5. ì••ì¶• ë° ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
```bash
# ì••ì¶•í•˜ì—¬ ë‚´ë³´ë‚´ê¸°
fs export large-feature-group compressed_data.csv.gz \
  --compress \
  --limit 100000

# Parquetìœ¼ë¡œ íš¨ìœ¨ì  ì €ì¥
fs export analytics-features analytics_data.parquet \
  --where "event_time >= '2024-01-01'"
```

## ê³ ê¸‰ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ë‚´ë³´ë‚´ê¸°
```bash
#!/bin/bash
export_timeseries_analysis() {
  local feature_group=$1
  local start_date=$2
  local end_date=$3
  local output_dir=$4
  
  mkdir -p "$output_dir"
  
  echo "ğŸ“Š Exporting time series data from $start_date to $end_date..."
  
  # ì¼ë³„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  current_date="$start_date"
  while [[ "$current_date" <= "$end_date" ]]; do
    output_file="$output_dir/daily_$(date -d "$current_date" +%Y%m%d).csv"
    
    echo "ğŸ“… Exporting data for $current_date..."
    fs export "$feature_group" "$output_file" \
      --where "DATE(event_time) = DATE('$current_date')" \
      --columns "customer_id,event_time,transaction_amount,product_category" \
      --compress
    
    if [ $? -eq 0 ]; then
      record_count=$(zcat "$output_file.gz" | wc -l)
      record_count=$((record_count - 1))  # í—¤ë” ì œì™¸
      echo "âœ… $current_date: $record_count records exported"
    else
      echo "âŒ Failed to export data for $current_date"
    fi
    
    current_date=$(date -d "$current_date + 1 day" +%Y-%m-%d)
  done
  
  # ì „ì²´ ê¸°ê°„ ìš”ì•½ ë°ì´í„°
  echo "ğŸ“ˆ Creating summary report..."
  fs export "$feature_group" "$output_dir/period_summary.csv" \
    --where "event_time BETWEEN '$start_date' AND '$end_date'" \
    --columns "customer_id,SUM(transaction_amount) as total_amount,COUNT(*) as transaction_count" \
    --format csv
  
  echo "ğŸ‰ Time series export completed: $output_dir"
}

export_timeseries_analysis "transaction-features" "2024-01-01" "2024-01-31" "/exports/january_2024"
```

### 2. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
```bash
#!/bin/bash
export_ab_test_results() {
  local feature_group=$1
  local experiment_id=$2
  local output_dir=$3
  
  mkdir -p "$output_dir"
  
  echo "ğŸ§ª Exporting A/B test results for experiment: $experiment_id"
  
  # Control ê·¸ë£¹ ë°ì´í„°
  echo "ğŸ“Š Exporting control group..."
  fs export "$feature_group" "$output_dir/control_group.csv" \
    --where "experiment_id = '$experiment_id' AND test_group = 'control'" \
    --columns "user_id,conversion_rate,engagement_score,revenue,event_time"
  
  # Treatment ê·¸ë£¹ ë°ì´í„°
  echo "ğŸ“Š Exporting treatment group..."
  fs export "$feature_group" "$output_dir/treatment_group.csv" \
    --where "experiment_id = '$experiment_id' AND test_group = 'treatment'" \
    --columns "user_id,conversion_rate,engagement_score,revenue,event_time"
  
  # ì „ì²´ ì‹¤í—˜ ìš”ì•½
  echo "ğŸ“ˆ Exporting experiment summary..."
  fs export "$feature_group" "$output_dir/experiment_summary.csv" \
    --where "experiment_id = '$experiment_id'" \
    --columns "test_group,AVG(conversion_rate) as avg_conversion,AVG(revenue) as avg_revenue,COUNT(*) as user_count"
  
  # í†µê³„ ë¶„ì„ì„ ìœ„í•œ Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
  cat << 'EOF' > "$output_dir/analyze_results.py"
import pandas as pd
import numpy as np
from scipy import stats

# ë°ì´í„° ë¡œë“œ
control = pd.read_csv('control_group.csv')
treatment = pd.read_csv('treatment_group.csv')

print("ğŸ§ª A/B Test Analysis Results")
print("=" * 50)

print(f"Control group size: {len(control)}")
print(f"Treatment group size: {len(treatment)}")

# í‰ê·  ë¹„êµ
metrics = ['conversion_rate', 'engagement_score', 'revenue']
for metric in metrics:
    if metric in control.columns and metric in treatment.columns:
        control_mean = control[metric].mean()
        treatment_mean = treatment[metric].mean()
        
        # t-test ìˆ˜í–‰
        t_stat, p_value = stats.ttest_ind(control[metric].dropna(), 
                                          treatment[metric].dropna())
        
        print(f"\n{metric.upper()}:")
        print(f"  Control: {control_mean:.4f}")
        print(f"  Treatment: {treatment_mean:.4f}")
        print(f"  Difference: {((treatment_mean - control_mean) / control_mean * 100):.2f}%")
        print(f"  P-value: {p_value:.4f}")
        print(f"  Significant: {'Yes' if p_value < 0.05 else 'No'}")

print("\nâœ… Analysis complete!")
EOF
  
  echo "ğŸ Running statistical analysis..."
  cd "$output_dir" && python analyze_results.py
  
  echo "ğŸ¯ A/B test export completed: $output_dir"
}

export_ab_test_results "experiment-features" "exp_20240115" "/exports/ab_test_exp20240115"
```

### 3. ê³ ê° ì„¸ë¶„í™” ë°ì´í„° ë‚´ë³´ë‚´ê¸°
```bash
#!/bin/bash
export_customer_segmentation() {
  local feature_group=$1
  local output_dir=$2
  
  mkdir -p "$output_dir"
  
  echo "ğŸ¯ Exporting customer segmentation data..."
  
  # VIP ê³ ê° (ê³ ì•¡ ê±°ë˜, ë†’ì€ í™œë™)
  fs export "$feature_group" "$output_dir/vip_customers.json" \
    --where "lifetime_value > 10000 AND last_activity >= current_date - interval '7' day" \
    --columns "customer_id,lifetime_value,transaction_frequency,preferred_categories" \
    --format json
  
  # í™œì„± ê³ ê°
  fs export "$feature_group" "$output_dir/active_customers.csv" \
    --where "last_activity >= current_date - interval '30' day AND transaction_count > 5" \
    --columns "customer_id,age,location,purchase_behavior_score"
  
  # íœ´ë©´ ê³ ê° (ì¬í™œì„±í™” ëŒ€ìƒ)
  fs export "$feature_group" "$output_dir/dormant_customers.csv" \
    --where "last_activity < current_date - interval '90' day AND lifetime_value > 1000" \
    --columns "customer_id,last_activity,lifetime_value,contact_preferences"
  
  # ì‹ ê·œ ê³ ê° (ì˜¨ë³´ë”© ëŒ€ìƒ)
  fs export "$feature_group" "$output_dir/new_customers.csv" \
    --where "registration_date >= current_date - interval '30' day" \
    --columns "customer_id,registration_date,first_purchase_date,acquisition_channel"
  
  # ìœ„í—˜ ê³ ê° (ì´íƒˆ ì˜ˆì¸¡ ëŒ€ìƒ)
  fs export "$feature_group" "$output_dir/at_risk_customers.csv" \
    --where "churn_probability > 0.7" \
    --columns "customer_id,churn_probability,last_activity,satisfaction_score"
  
  # ì„¸ë¶„í™” ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
  cat << 'EOF' > "$output_dir/segmentation_report.py"
import json
import pandas as pd
from datetime import datetime

print("ğŸ¯ Customer Segmentation Report")
print("=" * 50)
print(f"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print()

segments = {
    'VIP Customers': 'vip_customers.json',
    'Active Customers': 'active_customers.csv', 
    'Dormant Customers': 'dormant_customers.csv',
    'New Customers': 'new_customers.csv',
    'At-Risk Customers': 'at_risk_customers.csv'
}

total_customers = 0
segment_stats = {}

for segment_name, filename in segments.items():
    try:
        if filename.endswith('.json'):
            with open(filename, 'r') as f:
                data = json.load(f)
            count = len(data)
        else:
            df = pd.read_csv(filename)
            count = len(df)
        
        segment_stats[segment_name] = count
        total_customers += count
        print(f"{segment_name}: {count:,} customers")
        
    except FileNotFoundError:
        print(f"{segment_name}: No data available")
        segment_stats[segment_name] = 0

print(f"\nTotal customers across all segments: {total_customers:,}")

# ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¹„ìœ¨ ê³„ì‚°
if total_customers > 0:
    print("\nSegment Distribution:")
    for segment, count in segment_stats.items():
        percentage = (count / total_customers) * 100
        print(f"  {segment}: {percentage:.1f}%")

print("\nâœ… Segmentation analysis complete!")
EOF
  
  cd "$output_dir" && python segmentation_report.py
  
  echo "ğŸ‰ Customer segmentation export completed: $output_dir"
}

export_customer_segmentation "customer-analytics" "/exports/customer_segments"
```

### 4. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
```bash
#!/bin/bash
prepare_ml_training_data() {
  local feature_group=$1
  local model_type=$2
  local output_dir=$3
  local test_split_ratio=${4:-0.2}
  
  mkdir -p "$output_dir"
  
  case $model_type in
    "churn_prediction")
      target_column="churned"
      feature_columns="age,tenure_months,total_purchases,avg_order_value,support_tickets,last_activity_days"
      where_clause="registration_date <= current_date - interval '6' month"
      ;;
    "recommendation")
      target_column="purchased_items"
      feature_columns="user_id,age,location,browsing_history,purchase_categories,session_duration"
      where_clause="last_activity >= current_date - interval '90' day"
      ;;
    "fraud_detection")
      target_column="is_fraud"
      feature_columns="transaction_amount,merchant_category,hour_of_day,day_of_week,location_risk_score"
      where_clause="transaction_date >= current_date - interval '365' day"
      ;;
    *)
      echo "âŒ Unknown model type: $model_type"
      return 1
      ;;
  esac
  
  echo "ğŸ¤– Preparing ML training data for: $model_type"
  echo "ğŸ¯ Target: $target_column"
  echo "ğŸ“Š Features: $feature_columns"
  
  # ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  full_data_file="$output_dir/full_dataset.csv"
  fs export "$feature_group" "$full_data_file" \
    --columns "$feature_columns,$target_column" \
    --where "$where_clause" \
    --compress
  
  if [ ! -f "${full_data_file}.gz" ]; then
    echo "âŒ Failed to export training data"
    return 1
  fi
  
  # Pythonì„ ì‚¬ìš©í•œ ë°ì´í„° ë¶„í• 
  cat << EOF > "$output_dir/split_data.py"
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# ë°ì´í„° ë¡œë“œ
print("ğŸ“Š Loading dataset...")
df = pd.read_csv('${full_data_file}.gz', compression='gzip')

print(f"Dataset shape: {df.shape}")
print(f"Target column: '$target_column'")

# ê¸°ë³¸ í†µê³„
print("\\nDataset Statistics:")
print(f"  Total records: {len(df):,}")
print(f"  Features: {len(df.columns) - 1}")

if '$target_column' in df.columns:
    target_dist = df['$target_column'].value_counts()
    print(f"  Target distribution:")
    for value, count in target_dist.items():
        percentage = (count / len(df)) * 100
        print(f"    {value}: {count:,} ({percentage:.1f}%)")

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
print(f"\\nğŸ”€ Splitting data (test ratio: $test_split_ratio)...")

if '$target_column' in df.columns:
    X = df.drop('$target_column', axis=1)
    y = df['$target_column']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=$test_split_ratio, random_state=42, stratify=y
    )
    
    # í›ˆë ¨ ë°ì´í„° ì €ì¥
    train_data = pd.concat([X_train, y_train], axis=1)
    train_data.to_csv('train_data.csv', index=False)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
    test_data = pd.concat([X_test, y_test], axis=1)
    test_data.to_csv('test_data.csv', index=False)
    
    print(f"âœ… Training data: {len(train_data):,} records -> train_data.csv")
    print(f"âœ… Test data: {len(test_data):,} records -> test_data.csv")
    
else:
    # íƒ€ê²Ÿ ì—†ëŠ” ê²½ìš° (ë¹„ì§€ë„ í•™ìŠµ)
    test_size = int(len(df) * $test_split_ratio)
    train_data = df[:-test_size]
    test_data = df[-test_size:]
    
    train_data.to_csv('train_data.csv', index=False)
    test_data.to_csv('test_data.csv', index=False)
    
    print(f"âœ… Training data: {len(train_data):,} records -> train_data.csv")
    print(f"âœ… Test data: {len(test_data):,} records -> test_data.csv")

# í”¼ì²˜ ë©”íƒ€ë°ì´í„° ìƒì„±
feature_info = {
    'numeric_features': [],
    'categorical_features': [],
    'datetime_features': []
}

for col in df.columns:
    if col == '$target_column':
        continue
        
    if df[col].dtype in ['int64', 'float64']:
        feature_info['numeric_features'].append(col)
    elif 'time' in col.lower() or 'date' in col.lower():
        feature_info['datetime_features'].append(col)
    else:
        feature_info['categorical_features'].append(col)

import json
with open('feature_metadata.json', 'w') as f:
    json.dump(feature_info, f, indent=2)

print("\\nğŸ“‹ Feature metadata saved to: feature_metadata.json")
print("ğŸ‰ ML training data preparation completed!")
EOF
  
  cd "$output_dir" && python split_data.py
  
  # ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
  cat << 'EOF' > "$output_dir/data_quality_report.py"
import pandas as pd
import numpy as np

def generate_quality_report(filename):
    df = pd.read_csv(filename)
    
    print(f"\nğŸ“Š Data Quality Report: {filename}")
    print("-" * 50)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # ëˆ„ë½ê°’ ë¶„ì„
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        print(f"\nâš  Missing values:")
        for col, missing_count in missing_data[missing_data > 0].items():
            percentage = (missing_count / len(df)) * 100
            print(f"  {col}: {missing_count} ({percentage:.1f}%)")
    else:
        print("\nâœ… No missing values found")
    
    # ì¤‘ë³µ ë ˆì½”ë“œ
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        print(f"\nâš  Duplicate records: {duplicates}")
    else:
        print("\nâœ… No duplicate records")
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ í†µê³„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"\nğŸ“ˆ Numeric columns summary:")
        print(df[numeric_cols].describe())
    
    return df.shape[0]

# ê° ë°ì´í„°ì…‹ í’ˆì§ˆ í™•ì¸
train_records = generate_quality_report('train_data.csv')
test_records = generate_quality_report('test_data.csv')

print(f"\nğŸ¯ Final Summary:")
print(f"  Training records: {train_records:,}")
print(f"  Test records: {test_records:,}")
print(f"  Total: {train_records + test_records:,}")
EOF
  
  cd "$output_dir" && python data_quality_report.py
  
  echo "ğŸ¤– ML training data preparation completed: $output_dir"
}

prepare_ml_training_data "customer-features" "churn_prediction" "/ml_data/churn_model" 0.2
```

### 5. ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ í†µí•© ë‚´ë³´ë‚´ê¸°
```bash
#!/bin/bash
export_for_data_warehouse() {
  local source_feature_groups=("customer-profile" "transaction-history" "product-catalog")
  local output_dir=$1
  local export_date=${2:-$(date +%Y-%m-%d)}
  
  mkdir -p "$output_dir"
  
  echo "ğŸ¢ Exporting data for data warehouse integration"
  echo "ğŸ“… Export date: $export_date"
  
  # ê° Feature Groupì—ì„œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  for fg in "${source_feature_groups[@]}"; do
    echo "ğŸ“Š Exporting from $fg..."
    
    # Parquet í˜•ì‹ìœ¼ë¡œ íš¨ìœ¨ì  ì €ì¥
    fs export "$fg" "$output_dir/${fg//-/_}_${export_date}.parquet" \
      --where "DATE(event_time) = DATE('$export_date')" \
      --format parquet \
      --compress
    
    if [ $? -eq 0 ]; then
      # ë©”íƒ€ë°ì´í„° ìƒì„±
      record_count=$(python3 -c "
import pandas as pd
df = pd.read_parquet('$output_dir/${fg//-/_}_${export_date}.parquet')
print(len(df))
")
      
      echo "âœ… $fg: $record_count records exported"
      
      # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì €ì¥
      fs schema "$fg" -o json > "$output_dir/${fg//-/_}_schema.json"
      
    else
      echo "âŒ Failed to export from $fg"
    fi
  done
  
  # í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±
  cat << EOF > "$output_dir/export_manifest.json"
{
  "export_date": "$export_date",
  "export_timestamp": "$(date -Iseconds)",
  "feature_groups": [
$(for fg in "${source_feature_groups[@]}"; do
  echo "    {\"name\": \"$fg\", \"file\": \"${fg//-/_}_${export_date}.parquet\", \"schema\": \"${fg//-/_}_schema.json\"}"
  if [ "$fg" != "${source_feature_groups[-1]}" ]; then echo ","; fi
done)
  ]
}
EOF
  
  # SQL DDL ìƒì„± (ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ìš©)
  cat << 'EOF' > "$output_dir/create_tables.sql"
-- Data warehouse table creation script
-- Generated for SageMaker FeatureStore export

EOF
  
  for fg in "${source_feature_groups[@]}"; do
    table_name="${fg//-/_}"
    echo "-- Table for $fg" >> "$output_dir/create_tables.sql"
    echo "CREATE TABLE IF NOT EXISTS ${table_name} (" >> "$output_dir/create_tables.sql"
    
    # ìŠ¤í‚¤ë§ˆì—ì„œ ì»¬ëŸ¼ ì •ì˜ ìƒì„±
    python3 << EOF >> "$output_dir/create_tables.sql"
import json

with open('$output_dir/${fg//-/_}_schema.json', 'r') as f:
    schema = json.load(f)

columns = []
for feature in schema:
    name = feature['FeatureName']
    ftype = feature['FeatureType']
    
    if ftype == 'String':
        sql_type = 'VARCHAR(255)'
    elif ftype == 'Integral':
        sql_type = 'BIGINT'
    elif ftype == 'Fractional':
        sql_type = 'DOUBLE'
    else:
        sql_type = 'VARCHAR(255)'
    
    columns.append(f"    {name} {sql_type}")

print(",\\n".join(columns))
EOF
    
    echo "" >> "$output_dir/create_tables.sql"
    echo ");" >> "$output_dir/create_tables.sql"
    echo "" >> "$output_dir/create_tables.sql"
  done
  
  echo "ğŸ¢ Data warehouse export completed: $output_dir"
  echo "ğŸ“‹ Manifest: $output_dir/export_manifest.json"
  echo "ğŸ—ƒ DDL script: $output_dir/create_tables.sql"
}

export_for_data_warehouse "/exports/data_warehouse" "2024-01-15"
```

### 6. ì‹¤ì‹œê°„ ë‚´ë³´ë‚´ê¸° ìŠ¤ì¼€ì¤„ë§
```bash
#!/bin/bash
setup_scheduled_export() {
  local feature_group=$1
  local export_schedule=$2  # hourly, daily, weekly
  local output_base_dir=$3
  
  echo "â° Setting up scheduled export for $feature_group ($export_schedule)"
  
  # ìŠ¤ì¼€ì¤„ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
  script_path="/usr/local/bin/scheduled_export_${feature_group}.sh"
  
  cat << EOF > "$script_path"
#!/bin/bash
# Scheduled export script for $feature_group
# Schedule: $export_schedule

FEATURE_GROUP="$feature_group"
BASE_DIR="$output_base_dir"
LOG_DIR="\$BASE_DIR/logs"

mkdir -p "\$LOG_DIR"

# ì‹œê°„ ê¸°ë°˜ ë””ë ‰í† ë¦¬ ìƒì„±
case "$export_schedule" in
  "hourly")
    export_dir="\$BASE_DIR/\$(date +%Y/%m/%d/%H)"
    time_filter="event_time >= current_timestamp - interval '1' hour"
    ;;
  "daily")
    export_dir="\$BASE_DIR/\$(date +%Y/%m/%d)"
    time_filter="DATE(event_time) = DATE(current_date - interval '1' day)"
    ;;
  "weekly")
    export_dir="\$BASE_DIR/\$(date +%Y/%V)"
    time_filter="event_time >= current_date - interval '7' day"
    ;;
esac

mkdir -p "\$export_dir"

# ë¡œê·¸ íŒŒì¼ ì„¤ì •
log_file="\$LOG_DIR/export_\$(date +%Y%m%d_%H%M%S).log"

echo "ğŸš€ Starting scheduled export at \$(date)" | tee "\$log_file"
echo "ğŸ“‚ Export directory: \$export_dir" | tee -a "\$log_file"

# ë°ì´í„° ë‚´ë³´ë‚´ê¸°
output_file="\$export_dir/\${FEATURE_GROUP}_\$(date +%Y%m%d_%H%M%S).parquet"

fs export "\$FEATURE_GROUP" "\$output_file" \\
  --where "\$time_filter" \\
  --format parquet \\
  --compress \\
  --query-timeout 600 2>&1 | tee -a "\$log_file"

if [ \${PIPESTATUS[0]} -eq 0 ]; then
  record_count=\$(python3 -c "
import pandas as pd
try:
  df = pd.read_parquet('\$output_file')
  print(len(df))
except:
  print(0)
")
  echo "âœ… Export completed: \$record_count records" | tee -a "\$log_file"
  
  # ì„±ê³µ ë©”íŠ¸ë¦­ ê¸°ë¡
  echo "export_success_count 1 \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
  echo "export_record_count \$record_count \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
else
  echo "âŒ Export failed" | tee -a "\$log_file"
  echo "export_failure_count 1 \$(date +%s)" >> "\$LOG_DIR/metrics.txt"
fi

# ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ (30ì¼ ì´ˆê³¼)
find "\$BASE_DIR" -name "*.parquet" -mtime +30 -delete
find "\$LOG_DIR" -name "*.log" -mtime +7 -delete

echo "ğŸ Scheduled export completed at \$(date)" | tee -a "\$log_file"
EOF
  
  chmod +x "$script_path"
  
  # Crontab ì—”íŠ¸ë¦¬ ìƒì„±
  case $export_schedule in
    "hourly")
      cron_expr="0 * * * *"
      ;;
    "daily")
      cron_expr="0 1 * * *"  # ë§¤ì¼ ì˜¤ì „ 1ì‹œ
      ;;
    "weekly")
      cron_expr="0 2 * * 0"  # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 2ì‹œ
      ;;
  esac
  
  echo "ğŸ“… Cron expression: $cron_expr"
  echo "ğŸ“ To install the cron job, run:"
  echo "   (crontab -l 2>/dev/null; echo '$cron_expr $script_path') | crontab -"
  
  echo "âœ… Scheduled export setup completed"
  echo "ğŸ“‹ Script location: $script_path"
}

setup_scheduled_export "customer-activity" "daily" "/exports/scheduled"
```

## ì„±ëŠ¥ ìµœì í™”

### 1. Athena ì¿¼ë¦¬ ìµœì í™”
```bash
#!/bin/bash
optimize_export_query() {
  local feature_group=$1
  local output_file=$2
  
  echo "ğŸš€ Running optimized export with query analysis..."
  
  # ë¨¼ì € dry-runìœ¼ë¡œ ì¿¼ë¦¬ ê³„íš í™•ì¸
  fs export "$feature_group" "$output_file" \
    --dry-run \
    --columns "customer_id,event_time,balance" \
    --where "event_time >= current_date - interval '7' day"
  
  echo "ğŸ“Š Query plan analysis completed"
  
  # íŒŒí‹°ì…˜ì„ í™œìš©í•œ ìµœì í™”ëœ ì¿¼ë¦¬
  fs export "$feature_group" "$output_file" \
    --where "year=2024 AND month=1 AND day>=15" \
    --columns "customer_id,balance,category" \
    --format parquet \
    --compress
}

optimize_export_query "customer-profile" "optimized_export.parquet"
```

### 2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„í•  ì²˜ë¦¬
```bash
#!/bin/bash
chunked_export() {
  local feature_group=$1
  local output_dir=$2
  local chunk_days=${3:-7}
  local start_date=$4
  local end_date=$5
  
  mkdir -p "$output_dir"
  
  echo "ğŸ“¦ Chunked export: $chunk_days days per chunk"
  
  current_date="$start_date"
  chunk_number=1
  
  while [[ "$current_date" <= "$end_date" ]]; do
    chunk_end_date=$(date -d "$current_date + $chunk_days days" +%Y-%m-%d)
    if [[ "$chunk_end_date" > "$end_date" ]]; then
      chunk_end_date="$end_date"
    fi
    
    echo "ğŸ“Š Chunk $chunk_number: $current_date to $chunk_end_date"
    
    fs export "$feature_group" "$output_dir/chunk_${chunk_number}.parquet" \
      --where "event_time BETWEEN '$current_date' AND '$chunk_end_date'" \
      --format parquet \
      --compress \
      --query-timeout 900
    
    if [ $? -eq 0 ]; then
      echo "âœ… Chunk $chunk_number completed"
    else
      echo "âŒ Chunk $chunk_number failed"
    fi
    
    current_date=$(date -d "$chunk_end_date + 1 day" +%Y-%m-%d)
    chunk_number=$((chunk_number + 1))
  done
  
  echo "ğŸ‰ Chunked export completed: $output_dir"
}

chunked_export "large-feature-group" "/exports/chunked" 7 "2024-01-01" "2024-03-31"
```

## ë°ì´í„° í˜•ì‹ ìµœì í™”

### 1. í˜•ì‹ë³„ ìµœì  ì‚¬ìš© ì‚¬ë¡€
```bash
#!/bin/bash
format_optimization_guide() {
  local feature_group=$1
  local base_output=$2
  
  echo "ğŸ“‹ Testing different export formats for optimization..."
  
  # CSV - í˜¸í™˜ì„± ìš°ì„ 
  echo "ğŸ“„ Exporting as CSV (highest compatibility)..."
  time fs export "$feature_group" "${base_output}.csv" \
    --limit 10000 \
    --compress
  
  # JSON - êµ¬ì¡°í™”ëœ ë°ì´í„°
  echo "ğŸ“‹ Exporting as JSON (structured data)..."  
  time fs export "$feature_group" "${base_output}.json" \
    --limit 10000 \
    --online-compatible
  
  # Parquet - ë¶„ì„ ìµœì í™”
  echo "ğŸ—œ Exporting as Parquet (analytics optimized)..."
  time fs export "$feature_group" "${base_output}.parquet" \
    --limit 10000 \
    --format parquet
  
  # íŒŒì¼ í¬ê¸° ë¹„êµ
  echo "ğŸ“Š File size comparison:"
  ls -lh "${base_output}".* | awk '{print $9, $5}'
}

format_optimization_guide "test-features" "/tmp/format_test"
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¬¸ì œ í•´ê²°

### 1. Athena ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
```bash
#!/bin/bash
handle_query_timeout() {
  local feature_group=$1
  local output_file=$2
  local max_timeout=1800  # 30ë¶„
  
  echo "â± Handling potential query timeouts..."
  
  # ì ì§„ì ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ì¦ê°€í•˜ë©° ì¬ì‹œë„
  timeouts=(300 600 900 1800)
  
  for timeout in "${timeouts[@]}"; do
    echo "ğŸ”„ Trying with ${timeout}s timeout..."
    
    if fs export "$feature_group" "$output_file" \
       --query-timeout $timeout \
       --limit 100000; then
      echo "âœ… Export succeeded with ${timeout}s timeout"
      return 0
    else
      echo "âŒ Failed with ${timeout}s timeout"
    fi
  done
  
  echo "ğŸ’¥ All timeout attempts failed"
  return 1
}

handle_query_timeout "large-feature-group" "timeout_test.csv"
```

## ëª¨ë²” ì‚¬ë¡€

1. **ì ì ˆí•œ í˜•ì‹ ì„ íƒ**: CSV(í˜¸í™˜ì„±), JSON(êµ¬ì¡°í™”), Parquet(ì„±ëŠ¥)
2. **WHERE ì ˆ ìµœì í™”**: íŒŒí‹°ì…˜ í‚¤ í™œìš©, ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì‚¬ìš©
3. **ì»¬ëŸ¼ ì„ íƒ**: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì „ì†¡ëŸ‰ ìµœì†Œí™”
4. **ì••ì¶• í™œìš©**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì••ì¶•í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ ì ˆì•½
5. **ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬**: ë§¤ìš° í° ë°ì´í„°ì…‹ì€ ë¶„í•  ì²˜ë¦¬
6. **ì¿¼ë¦¬ ê²€ì¦**: dry-runìœ¼ë¡œ ì¿¼ë¦¬ ë¯¸ë¦¬ í™•ì¸

## ê´€ë ¨ ëª…ë ¹ì–´
- `fs list`: Feature Group ëª©ë¡ í™•ì¸
- `fs schema`: ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
- `fs bulk-get`: Online Storeì—ì„œ ëŒ€ëŸ‰ ì¡°íšŒ
- `fs analyze`: Offline Store ì‚¬ìš©ëŸ‰ ë¶„ì„