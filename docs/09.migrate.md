# 9. migrate - í”¼ì²˜ ê·¸ë£¹ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜

## ê°œìš”
í•œ Feature Groupì˜ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ Feature Groupìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤. ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„±ì„ ê²€ì‚¬í•˜ê³ , ì•ˆì „í•˜ê²Œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ë©°, ì‹¤íŒ¨í•œ ë ˆì½”ë“œë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

## ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
fs migrate SOURCE_FEATURE_GROUP TARGET_FEATURE_GROUP [OPTIONS]
```

## í•„ìˆ˜ ì¸ì
- `SOURCE_FEATURE_GROUP`: ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ì†ŒìŠ¤ Feature Group
- `TARGET_FEATURE_GROUP`: ë°ì´í„°ë¥¼ ì €ì¥í•  íƒ€ê²Ÿ Feature Group

## ì˜µì…˜
- `--clear-target`: ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ íƒ€ê²Ÿ ë°ì´í„° ì‚­ì œ
- `--batch-size INTEGER`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)
- `--max-workers INTEGER`: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)
- `--dry-run`: ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì—†ì´ ê³„íšë§Œ í™•ì¸
- `--filter-query TEXT`: Athena WHERE ì ˆì„ ì‚¬ìš©í•œ ë°ì´í„° í•„í„°ë§

## ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
fs migrate old-customer-profile new-customer-profile
```

**ì‹¤í–‰ ê³¼ì •:**
1. ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ê²€ì‚¬
2. ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìˆ˜ë¦½
3. ì‚¬ìš©ì í™•ì¸
4. ë°ì´í„° ì „ì†¡ ì‹¤í–‰

### 2. íƒ€ê²Ÿ ë°ì´í„° ì‚­ì œ í›„ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
fs migrate source-features target-features --clear-target
```

### 3. ì¡°ê±´ë¶€ ë§ˆì´ê·¸ë ˆì´ì…˜ (ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ)
```bash
fs migrate customer-history customer-history-v2 \
  --filter-query "WHERE event_time >= current_date - interval '30' day"
```

### 4. ì„±ëŠ¥ ìµœì í™”ëœ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
fs migrate large-dataset optimized-dataset \
  --batch-size 500 \
  --max-workers 10 \
  --clear-target
```

### 5. ê³„íš í™•ì¸ (ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì•ˆí•¨)
```bash
fs migrate source-fg target-fg --dry-run
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš
============================================================
ì†ŒìŠ¤: source-fg (online+offline)
íƒ€ê²Ÿ: target-fg (online+offline)
ì „ëµ: offline_to_online
ì£¼ìš” ë°ì´í„° ì†ŒìŠ¤: offline
ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: 50,000
ë°°ì¹˜ í¬ê¸°: 100
ë™ì‹œ ì›Œì»¤ ìˆ˜: 4

ğŸ” Dry-run ëª¨ë“œ: ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ì€ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
```

## ê³ ê¸‰ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. ìŠ¤í‚¤ë§ˆ ì—…ê·¸ë ˆì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
#!/bin/bash
schema_upgrade_migration() {
  local old_fg=$1
  local new_fg=$2
  local backup_location=$3
  
  echo "ğŸ”„ ìŠ¤í‚¤ë§ˆ ì—…ê·¸ë ˆì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜: $old_fg â†’ $new_fg"
  
  # 1. ë°±ì—… ìƒì„±
  echo "ğŸ“¦ 1ë‹¨ê³„: ê¸°ì¡´ ë°ì´í„° ë°±ì—…"
  timestamp=$(date +%Y%m%d_%H%M%S)
  backup_file="/tmp/${old_fg}_backup_${timestamp}.json"
  
  fs export "$old_fg" "$backup_file" \
    --format json \
    --compress
  
  if [ $? -ne 0 ]; then
    echo "âŒ ë°±ì—… ì‹¤íŒ¨, ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ë‹¨"
    return 1
  fi
  
  # S3ì—ë„ ë°±ì—…
  if [ -n "$backup_location" ]; then
    aws s3 cp "${backup_file}.gz" "$backup_location/schema_upgrade_backup_${timestamp}.json.gz"
  fi
  
  # 2. ìƒˆ Feature Group ìƒíƒœ í™•ì¸
  echo "ğŸ” 2ë‹¨ê³„: íƒ€ê²Ÿ Feature Group ê²€ì¦"
  fs list | grep "$new_fg" || {
    echo "âŒ íƒ€ê²Ÿ Feature Groupì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: $new_fg"
    return 1
  }
  
  # 3. ìŠ¤í‚¤ë§ˆ ë¹„êµ
  echo "ğŸ“‹ 3ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ì°¨ì´ ë¶„ì„"
  old_schema="/tmp/${old_fg}_schema.json"
  new_schema="/tmp/${new_fg}_schema.json"
  
  fs schema "$old_fg" -o json > "$old_schema"
  fs schema "$new_fg" -o json > "$new_schema"
  
  # Pythonì„ ì‚¬ìš©í•œ ìŠ¤í‚¤ë§ˆ ë¹„êµ
  python3 << EOF
import json

with open('$old_schema', 'r') as f:
    old_features = {item['FeatureName']: item for item in json.load(f)}
    
with open('$new_schema', 'r') as f:
    new_features = {item['FeatureName']: item for item in json.load(f)}

print("ğŸ“Š ìŠ¤í‚¤ë§ˆ ë¹„êµ ê²°ê³¼:")
print(f"  ê¸°ì¡´ í”¼ì²˜ ìˆ˜: {len(old_features)}")
print(f"  ìƒˆ í”¼ì²˜ ìˆ˜: {len(new_features)}")

# ì¶”ê°€ëœ í”¼ì²˜
added = set(new_features.keys()) - set(old_features.keys())
if added:
    print(f"  âœ… ì¶”ê°€ëœ í”¼ì²˜: {list(added)}")

# ì œê±°ëœ í”¼ì²˜
removed = set(old_features.keys()) - set(new_features.keys())
if removed:
    print(f"  âš ï¸ ì œê±°ëœ í”¼ì²˜: {list(removed)}")

# ë³€ê²½ëœ í”¼ì²˜ íƒ€ì…
changed = []
for name in set(old_features.keys()) & set(new_features.keys()):
    if old_features[name]['FeatureType'] != new_features[name]['FeatureType']:
        changed.append((name, old_features[name]['FeatureType'], new_features[name]['FeatureType']))

if changed:
    print(f"  ğŸ”„ íƒ€ì… ë³€ê²½ëœ í”¼ì²˜:")
    for name, old_type, new_type in changed:
        print(f"    - {name}: {old_type} â†’ {new_type}")

if removed:
    print("âš ï¸ ì œê±°ëœ í”¼ì²˜ê°€ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ì†ì‹¤ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    exit(1)
EOF
  
  if [ $? -ne 0 ]; then
    echo "âŒ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ìŠ¤í‚¤ë§ˆ ë³€ê²½ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # 4. ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
  echo "ğŸš€ 4ë‹¨ê³„: ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"
  fs migrate "$old_fg" "$new_fg" \
    --clear-target \
    --batch-size 200 \
    --max-workers 6
  
  migration_result=$?
  
  # 5. ê²€ì¦
  if [ $migration_result -eq 0 ]; then
    echo "âœ… 5ë‹¨ê³„: ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"
    
    # ë ˆì½”ë“œ ìˆ˜ ë¹„êµ
    original_count=$(jq length "$backup_file" 2>/dev/null || echo "unknown")
    echo "ì›ë³¸ ë ˆì½”ë“œ ìˆ˜: $original_count"
    
    # ìƒ˜í”Œ ë°ì´í„° ê²€ì¦
    sample_file="/tmp/${new_fg}_sample.json"
    fs export "$new_fg" "$sample_file" --limit 10 --format json
    
    if [ -s "$sample_file" ]; then
      echo "âœ… íƒ€ê²Ÿ Feature Groupì—ì„œ ë°ì´í„° ì¡°íšŒ ì„±ê³µ"
    else
      echo "âš ï¸ íƒ€ê²Ÿ Feature Groupì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    fi
    
    echo "ğŸ‰ ìŠ¤í‚¤ë§ˆ ì—…ê·¸ë ˆì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!"
  else
    echo "âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨"
    return 1
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -f "$old_schema" "$new_schema" "$sample_file"
  
  echo "ğŸ“ ë°±ì—… íŒŒì¼: $backup_file"
}

# ì‚¬ìš© ì˜ˆì‹œ
schema_upgrade_migration "customer-v1" "customer-v2" "s3://backups/schema-upgrades/"
```

### 2. í™˜ê²½ ê°„ ë°ì´í„° ë³µì œ
```bash
#!/bin/bash
cross_environment_migration() {
  local source_profile=$1
  local target_profile=$2
  local feature_group=$3
  local source_region=${4:-us-east-1}
  local target_region=${5:-us-west-2}
  
  echo "ğŸŒ í™˜ê²½ ê°„ ë§ˆì´ê·¸ë ˆì´ì…˜: $feature_group"
  echo "  ì†ŒìŠ¤: $source_profile ($source_region)"
  echo "  íƒ€ê²Ÿ: $target_profile ($target_region)"
  
  # 1. ì†ŒìŠ¤ í™˜ê²½ì—ì„œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  echo "ğŸ“¤ 1ë‹¨ê³„: ì†ŒìŠ¤ í™˜ê²½ì—ì„œ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"
  export_file="/tmp/${feature_group}_cross_env_export.json"
  
  AWS_PROFILE="$source_profile" fs --region "$source_region" \
    export "$feature_group" "$export_file" \
    --format json \
    --compress
  
  if [ ! -f "${export_file}.gz" ]; then
    echo "âŒ ì†ŒìŠ¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨"
    return 1
  fi
  
  # 2. ìŠ¤í‚¤ë§ˆ ë³µì‚¬
  echo "ğŸ“‹ 2ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ì •ë³´ ë³µì‚¬"
  schema_file="/tmp/${feature_group}_schema.json"
  
  AWS_PROFILE="$source_profile" fs --region "$source_region" \
    schema "$feature_group" -o json > "$schema_file"
  
  # 3. íƒ€ê²Ÿ í™˜ê²½ì— Feature Group ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
  echo "ğŸ—ï¸ 3ë‹¨ê³„: íƒ€ê²Ÿ Feature Group í™•ì¸/ìƒì„±"
  
  AWS_PROFILE="$target_profile" fs --region "$target_region" \
    list | grep -q "$feature_group" || {
    
    echo "íƒ€ê²Ÿ Feature Groupì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤:"
    echo "AWS_PROFILE=$target_profile fs --region $target_region create $feature_group \\"
    echo "  --schema-file $schema_file \\"
    echo "  --role-arn 'TARGET_ROLE_ARN' \\"
    echo "  --s3-uri 'TARGET_S3_URI'"
    
    read -p "íƒ€ê²Ÿ Feature Groupì„ ìƒì„±í–ˆìŠµë‹ˆê¹Œ? (y/n): " created
    if [ "$created" != "y" ]; then
      echo "âŒ íƒ€ê²Ÿ Feature Group ìƒì„±ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤."
      return 1
    fi
  }
  
  # 4. ë°ì´í„° ì••ì¶• í•´ì œ ë° ê°€ê³µ
  echo "ğŸ”„ 4ë‹¨ê³„: ë°ì´í„° ë³€í™˜"
  transformed_file="/tmp/${feature_group}_transformed.json"
  
  gunzip -c "${export_file}.gz" > "$export_file"
  
  # í•„ìš”ì‹œ ë°ì´í„° ë³€í™˜ (ì˜ˆ: íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ì •, í•„ë“œ ë§¤í•‘ ë“±)
  python3 << EOF
import json

with open('$export_file', 'r') as f:
    data = json.load(f)

# í™˜ê²½ë³„ ë°ì´í„° ë³€í™˜ ë¡œì§ (ì˜ˆì‹œ)
transformed_data = []
for record in data:
    # í™˜ê²½ ì •ë³´ ì¶”ê°€
    record['migration_source_env'] = '$source_profile'
    record['migration_target_env'] = '$target_profile'
    record['migration_timestamp'] = '$(date -u +"%Y-%m-%dT%H:%M:%SZ")'
    
    transformed_data.append(record)

with open('$transformed_file', 'w') as f:
    json.dump(transformed_data, f, indent=2)

print(f"âœ… {len(transformed_data)}ê°œ ë ˆì½”ë“œ ë³€í™˜ ì™„ë£Œ")
EOF
  
  # 5. íƒ€ê²Ÿ í™˜ê²½ì— ë°ì´í„° ì—…ë¡œë“œ
  echo "ğŸ“¥ 5ë‹¨ê³„: íƒ€ê²Ÿ í™˜ê²½ì— ë°ì´í„° ì—…ë¡œë“œ"
  
  AWS_PROFILE="$target_profile" fs --region "$target_region" \
    bulk-put "$feature_group" "$transformed_file" \
    --batch-size 200 \
    --output-file "/tmp/cross_env_upload.log"
  
  if [ $? -eq 0 ]; then
    echo "âœ… í™˜ê²½ ê°„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!"
    
    # ê²€ì¦
    echo "ğŸ” 6ë‹¨ê³„: ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"
    target_sample="/tmp/${feature_group}_target_sample.json"
    
    AWS_PROFILE="$target_profile" fs --region "$target_region" \
      export "$feature_group" "$target_sample" \
      --limit 5 --format json
    
    if [ -s "$target_sample" ]; then
      echo "âœ… íƒ€ê²Ÿ í™˜ê²½ ê²€ì¦ ì„±ê³µ"
      echo "ìƒ˜í”Œ ë°ì´í„°:"
      jq '.[0]' "$target_sample" 2>/dev/null || echo "JSON íŒŒì‹± ì˜¤ë¥˜"
    fi
  else
    echo "âŒ íƒ€ê²Ÿ í™˜ê²½ ì—…ë¡œë“œ ì‹¤íŒ¨"
    return 1
  fi
  
  # ì •ë¦¬
  rm -f "$export_file" "${export_file}.gz" "$transformed_file" "$schema_file" "$target_sample"
}

# ì‚¬ìš© ì˜ˆì‹œ
cross_environment_migration "dev-profile" "prod-profile" "customer-features" "us-east-1" "us-west-2"
```

### 3. ë°ì´í„° ì•„ì¹´ì´ë¹™ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
#!/bin/bash
archive_migration() {
  local active_fg=$1
  local archive_fg=$2
  local archive_age_days=${3:-90}
  local archive_s3_path=$4
  
  echo "ğŸ“¦ ë°ì´í„° ì•„ì¹´ì´ë¹™ ë§ˆì´ê·¸ë ˆì´ì…˜: $active_fg â†’ $archive_fg"
  echo "ì•„ì¹´ì´ë¸Œ ê¸°ì¤€: ${archive_age_days}ì¼ ì´ìƒëœ ë°ì´í„°"
  
  # 1. ì•„ì¹´ì´ë¹™í•  ë°ì´í„° ë²”ìœ„ ê³„ì‚°
  cutoff_date=$(date -d "$archive_age_days days ago" +%Y-%m-%d)
  echo "ğŸ“… ì•„ì¹´ì´ë¸Œ ê¸°ì¤€ì¼: $cutoff_date"
  
  # 2. Archive Feature Group ì¡´ì¬ í™•ì¸
  echo "ğŸ” Archive Feature Group í™•ì¸"
  if ! fs list | grep -q "$archive_fg"; then
    echo "âŒ Archive Feature Groupì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: $archive_fg"
    echo "ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”:"
    echo "fs create $archive_fg --schema-file active_schema.json --role-arn ROLE --s3-uri $archive_s3_path"
    return 1
  fi
  
  # 3. ì•„ì¹´ì´ë¹™í•  ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  echo "ğŸ“¤ ì•„ì¹´ì´ë¸Œ ëŒ€ìƒ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"
  archive_export="/tmp/${active_fg}_archive_export.json"
  
  fs export "$active_fg" "$archive_export" \
    --where "WHERE event_time < '$cutoff_date'" \
    --format json \
    --compress
  
  if [ ! -f "${archive_export}.gz" ]; then
    echo "â„¹ï¸ ì•„ì¹´ì´ë¸Œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    return 0
  fi
  
  # 4. ì•„ì¹´ì´ë¸Œ ë°ì´í„° ê°œìˆ˜ í™•ì¸
  archive_count=$(gunzip -c "${archive_export}.gz" | jq length)
  echo "ğŸ“Š ì•„ì¹´ì´ë¸Œ ëŒ€ìƒ ë ˆì½”ë“œ: $archive_countê°œ"
  
  if [ "$archive_count" -eq 0 ]; then
    echo "â„¹ï¸ ì•„ì¹´ì´ë¸Œí•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."
    return 0
  fi
  
  # 5. ì‚¬ìš©ì í™•ì¸
  echo ""
  echo "âš ï¸ ë‹¤ìŒ ì‘ì—…ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤:"
  echo "  1. $archive_countê°œ ë ˆì½”ë“œë¥¼ $archive_fgë¡œ ì´ë™"
  echo "  2. $active_fgì—ì„œ í•´ë‹¹ ë°ì´í„° ì‚­ì œ"
  echo ""
  
  read -p "ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
  if [ "$confirmation" != "yes" ]; then
    echo "âŒ ì•„ì¹´ì´ë¹™ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # 6. Archive Feature Groupìœ¼ë¡œ ë°ì´í„° ì´ë™
  echo "ğŸ“¥ Archive Feature Groupìœ¼ë¡œ ë°ì´í„° ì´ë™"
  gunzip "${archive_export}.gz"
  
  fs bulk-put "$archive_fg" "$archive_export" \
    --batch-size 100 \
    --output-file "/tmp/archive_migration.log"
  
  if [ $? -ne 0 ]; then
    echo "âŒ ì•„ì¹´ì´ë¸Œ ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨"
    return 1
  fi
  
  # 7. ì›ë³¸ì—ì„œ ë°ì´í„° ì‚­ì œ (Offline Storeë§Œ)
  echo "ğŸ—‘ï¸ ì›ë³¸ì—ì„œ ì•„ì¹´ì´ë¸Œëœ ë°ì´í„° ì‚­ì œ"
  
  # ì‹¤ì œë¡œëŠ” S3ì—ì„œ íŠ¹ì • íŒŒí‹°ì…˜ ì‚­ì œ ë˜ëŠ” clear ëª…ë ¹ ì‚¬ìš©
  echo "âš ï¸ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:"
  echo "  1. $active_fgì˜ Offline Storeì—ì„œ $cutoff_date ì´ì „ ë°ì´í„° ì‚­ì œ"
  echo "  2. í•„ìš”ì‹œ fs clear $active_fg --offline-only ì‹¤í–‰"
  
  # 8. ê²€ì¦
  echo "âœ… ì•„ì¹´ì´ë¹™ ì™„ë£Œ ê²€ì¦"
  archive_sample="/tmp/${archive_fg}_sample.json"
  
  fs export "$archive_fg" "$archive_sample" \
    --limit 3 --format json
  
  if [ -s "$archive_sample" ]; then
    echo "âœ… ì•„ì¹´ì´ë¸Œ ë°ì´í„° ê²€ì¦ ì„±ê³µ"
    archived_count=$(jq length "$archive_sample" 2>/dev/null || echo "unknown")
    echo "ì•„ì¹´ì´ë¸Œ ìƒ˜í”Œ ë ˆì½”ë“œ: $archived_countê°œ"
  fi
  
  # ì •ë¦¬
  rm -f "$archive_export" "$archive_sample"
  
  echo "ğŸ‰ ë°ì´í„° ì•„ì¹´ì´ë¹™ ì™„ë£Œ: $archive_countê°œ ë ˆì½”ë“œ"
}

# ì‚¬ìš© ì˜ˆì‹œ
archive_migration "active-transactions" "archived-transactions" 180 "s3://archive-bucket/transactions/"
```

### 4. ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ (íŠ¹ì • ê³ ê°ë§Œ)
```bash
#!/bin/bash
partial_migration() {
  local source_fg=$1
  local target_fg=$2
  local customer_list_file=$3
  
  echo "ğŸ‘¥ ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜: íŠ¹ì • ê³ ê° ë°ì´í„°ë§Œ"
  echo "ì†ŒìŠ¤: $source_fg"
  echo "íƒ€ê²Ÿ: $target_fg"
  echo "ê³ ê° ëª©ë¡: $customer_list_file"
  
  # 1. ê³ ê° ëª©ë¡ ê²€ì¦
  if [ ! -f "$customer_list_file" ]; then
    echo "âŒ ê³ ê° ëª©ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $customer_list_file"
    return 1
  fi
  
  customer_count=$(wc -l < "$customer_list_file")
  echo "ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ ê³ ê° ìˆ˜: $customer_count"
  
  # 2. IN ì ˆì„ ìœ„í•œ ê³ ê° ID ë¬¸ìì—´ ìƒì„±
  customer_ids=$(python3 << EOF
import csv

customers = []
with open('$customer_list_file', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        if row and row[0].strip():  # ë¹ˆ ì¤„ ì œì™¸
            customers.append(f"'{row[0].strip()}'")

# IN ì ˆìš© ë¬¸ìì—´ ìƒì„± (ìµœëŒ€ 1000ê°œì”© ì²˜ë¦¬)
if len(customers) > 1000:
    print("âš ï¸ ê³ ê° ìˆ˜ê°€ 1000ëª…ì„ ì´ˆê³¼í•©ë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    customers = customers[:1000]
    print(f"ì²˜ìŒ 1000ëª…ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

print(','.join(customers))
EOF
)
  
  if [ -z "$customer_ids" ]; then
    echo "âŒ ìœ íš¨í•œ ê³ ê° IDê°€ ì—†ìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # 3. í•„í„° ì¿¼ë¦¬ ìƒì„±
  filter_query="WHERE customer_id IN ($customer_ids)"
  
  # 4. ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
  echo "ğŸš€ ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"
  fs migrate "$source_fg" "$target_fg" \
    --filter-query "$filter_query" \
    --batch-size 50 \
    --max-workers 3
  
  if [ $? -eq 0 ]; then
    echo "âœ… ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ"
    
    # ê²€ì¦: ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê³ ê° ìˆ˜ í™•ì¸
    echo "ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"
    migrated_customers="/tmp/migrated_customers.json"
    
    fs export "$target_fg" "$migrated_customers" \
      --columns "customer_id" \
      --format json
    
    if [ -s "$migrated_customers" ]; then
      migrated_count=$(jq 'unique_by(.customer_id) | length' "$migrated_customers" 2>/dev/null || echo "unknown")
      echo "ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê³ ìœ  ê³ ê° ìˆ˜: $migrated_count"
    fi
    
    rm -f "$migrated_customers"
  else
    echo "âŒ ë¶€ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨"
    return 1
  fi
}

# ê³ ê° ëª©ë¡ íŒŒì¼ ìƒì„± ì˜ˆì‹œ (CSV)
cat << 'EOF' > /tmp/target_customers.csv
cust_001
cust_002
cust_003
cust_vip_001
cust_premium_005
EOF

partial_migration "all-customers" "selected-customers" "/tmp/target_customers.csv"
```

### 5. ì‹¤ì‹œê°„ ë™ê¸°í™” ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
#!/bin/bash
realtime_sync_migration() {
  local source_fg=$1
  local target_fg=$2
  local sync_interval_minutes=${3:-60}
  local max_iterations=${4:-0}  # 0 = ë¬´í•œ
  
  echo "ğŸ”„ ì‹¤ì‹œê°„ ë™ê¸°í™” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘"
  echo "ì†ŒìŠ¤: $source_fg"
  echo "íƒ€ê²Ÿ: $target_fg"
  echo "ë™ê¸°í™” ê°„ê²©: ${sync_interval_minutes}ë¶„"
  
  # ë™ê¸°í™” ìƒíƒœ íŒŒì¼
  sync_state_file="/tmp/sync_state_${source_fg}_${target_fg}.json"
  
  # ì´ˆê¸° ìƒíƒœ ì„¤ì •
  if [ ! -f "$sync_state_file" ]; then
    cat << EOF > "$sync_state_file"
{
  "last_sync_timestamp": "1970-01-01T00:00:00Z",
  "total_synced": 0,
  "sync_iterations": 0,
  "last_sync_status": "initialized"
}
EOF
  fi
  
  iteration=0
  
  while [ $max_iterations -eq 0 ] || [ $iteration -lt $max_iterations ]; do
    iteration=$((iteration + 1))
    echo ""
    echo "ğŸ”„ ë™ê¸°í™” ë°˜ë³µ #$iteration ($(date))"
    
    # ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì½ê¸°
    last_sync=$(jq -r '.last_sync_timestamp' "$sync_state_file")
    current_time=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    
    echo "ë§ˆì§€ë§‰ ë™ê¸°í™”: $last_sync"
    echo "í˜„ì¬ ì‹œê°„: $current_time"
    
    # ì¦ë¶„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
    incremental_filter="WHERE event_time > '$last_sync'"
    
    echo "ğŸ” ì¦ë¶„ ë°ì´í„° í™•ì¸ ì¤‘..."
    fs migrate "$source_fg" "$target_fg" \
      --filter-query "$incremental_filter" \
      --batch-size 100 \
      --max-workers 3 \
      --dry-run
    
    read -p "ì¦ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n/q=ì¢…ë£Œ): " action
    
    case $action in
      "y"|"Y")
        echo "ğŸ“¥ ì¦ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰..."
        
        if fs migrate "$source_fg" "$target_fg" \
           --filter-query "$incremental_filter" \
           --batch-size 100 \
           --max-workers 3; then
          
          # ìƒíƒœ ì—…ë°ì´íŠ¸
          jq --arg timestamp "$current_time" \
             --arg status "success" \
             --argjson iteration $iteration \
             '.last_sync_timestamp = $timestamp | .last_sync_status = $status | .sync_iterations = $iteration' \
             "$sync_state_file" > "/tmp/sync_state_temp.json"
          
          mv "/tmp/sync_state_temp.json" "$sync_state_file"
          
          echo "âœ… ë™ê¸°í™” ì™„ë£Œ"
        else
          echo "âŒ ë™ê¸°í™” ì‹¤íŒ¨"
          
          # ì‹¤íŒ¨ ìƒíƒœ ê¸°ë¡
          jq --arg status "failed" \
             --argjson iteration $iteration \
             '.last_sync_status = $status | .sync_iterations = $iteration' \
             "$sync_state_file" > "/tmp/sync_state_temp.json"
          
          mv "/tmp/sync_state_temp.json" "$sync_state_file"
        fi
        ;;
      "q"|"Q")
        echo "ğŸ›‘ ë™ê¸°í™” ì¢…ë£Œ"
        break
        ;;
      *)
        echo "â­ï¸ ì´ë²ˆ ë™ê¸°í™” ê±´ë„ˆë›°ê¸°"
        ;;
    esac
    
    # ë‹¤ìŒ ë™ê¸°í™”ê¹Œì§€ ëŒ€ê¸°
    if [ $max_iterations -eq 0 ] || [ $iteration -lt $max_iterations ]; then
      echo "â³ ë‹¤ìŒ ë™ê¸°í™”ê¹Œì§€ ${sync_interval_minutes}ë¶„ ëŒ€ê¸° ì¤‘..."
      sleep $((sync_interval_minutes * 60))
    fi
  done
  
  echo "ğŸ ì‹¤ì‹œê°„ ë™ê¸°í™” ì™„ë£Œ"
  echo "ë™ê¸°í™” ìƒíƒœ: $sync_state_file"
}

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì˜ˆì‹œ
# nohup realtime_sync_migration "source-fg" "target-fg" 30 > sync.log 2>&1 &
realtime_sync_migration "live-features" "backup-features" 60 10  # 10íšŒ ë°˜ë³µ
```

### 6. ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
#!/bin/bash
quality_improvement_migration() {
  local source_fg=$1
  local target_fg=$2
  local quality_rules_file=${3:-"/tmp/quality_rules.json"}
  
  echo "âœ¨ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë§ˆì´ê·¸ë ˆì´ì…˜"
  echo "ì†ŒìŠ¤: $source_fg"
  echo "íƒ€ê²Ÿ: $target_fg"
  
  # ê¸°ë³¸ í’ˆì§ˆ ê·œì¹™ ìƒì„±
  if [ ! -f "$quality_rules_file" ]; then
    cat << 'EOF' > "$quality_rules_file"
{
  "rules": [
    {
      "name": "remove_null_values",
      "description": "NULL ê°’ ì œê±°",
      "enabled": true
    },
    {
      "name": "normalize_strings",
      "description": "ë¬¸ìì—´ ì •ê·œí™” (trim, case)",
      "enabled": true
    },
    {
      "name": "validate_ranges",
      "description": "ìˆ«ì ë²”ìœ„ ê²€ì¦",
      "enabled": true,
      "config": {
        "age": {"min": 0, "max": 120},
        "balance": {"min": 0, "max": 1000000}
      }
    },
    {
      "name": "deduplicate",
      "description": "ì¤‘ë³µ ì œê±°",
      "enabled": true
    }
  ]
}
EOF
  fi
  
  # 1. ì›ë³¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
  echo "ğŸ“¤ ì›ë³¸ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"
  raw_data="/tmp/${source_fg}_raw.json"
  
  fs export "$source_fg" "$raw_data" \
    --format json
  
  if [ ! -s "$raw_data" ]; then
    echo "âŒ ì›ë³¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # 2. ë°ì´í„° í’ˆì§ˆ ê°œì„  ì²˜ë¦¬
  echo "ğŸ”§ ë°ì´í„° í’ˆì§ˆ ê°œì„  ì²˜ë¦¬"
  cleaned_data="/tmp/${source_fg}_cleaned.json"
  
  python3 << EOF
import json
import re
from datetime import datetime

# í’ˆì§ˆ ê·œì¹™ ë¡œë“œ
with open('$quality_rules_file', 'r') as f:
    quality_config = json.load(f)

# ì›ë³¸ ë°ì´í„° ë¡œë“œ
with open('$raw_data', 'r') as f:
    raw_records = json.load(f)

print(f"ğŸ“Š ì›ë³¸ ë ˆì½”ë“œ ìˆ˜: {len(raw_records)}")

cleaned_records = []
quality_stats = {
    "total_input": len(raw_records),
    "null_values_removed": 0,
    "strings_normalized": 0,
    "range_violations_fixed": 0,
    "duplicates_removed": 0,
    "total_output": 0
}

seen_records = set()  # ì¤‘ë³µ ì œê±°ìš©

for record in raw_records:
    cleaned_record = record.copy()
    record_modified = False
    
    # ê·œì¹™ 1: NULL ê°’ ì œê±°
    if any(rule["name"] == "remove_null_values" and rule["enabled"] for rule in quality_config["rules"]):
        original_fields = len(cleaned_record)
        cleaned_record = {k: v for k, v in cleaned_record.items() 
                         if v is not None and str(v).strip() != '' and str(v).lower() != 'null'}
        if len(cleaned_record) < original_fields:
            quality_stats["null_values_removed"] += 1
            record_modified = True
    
    # ê·œì¹™ 2: ë¬¸ìì—´ ì •ê·œí™”
    if any(rule["name"] == "normalize_strings" and rule["enabled"] for rule in quality_config["rules"]):
        for key, value in cleaned_record.items():
            if isinstance(value, str) and key not in ['event_time']:  # event_timeì€ ì œì™¸
                original_value = value
                # ì•ë’¤ ê³µë°± ì œê±°, ì¼ê´€ëœ ì¼€ì´ìŠ¤ (ì´ë¦„ ì œì™¸)
                if key in ['category', 'status', 'type']:
                    cleaned_record[key] = value.strip().lower()
                else:
                    cleaned_record[key] = value.strip()
                
                if cleaned_record[key] != original_value:
                    record_modified = True
        
        if record_modified:
            quality_stats["strings_normalized"] += 1
    
    # ê·œì¹™ 3: ìˆ«ì ë²”ìœ„ ê²€ì¦
    range_rule = next((rule for rule in quality_config["rules"] 
                      if rule["name"] == "validate_ranges" and rule["enabled"]), None)
    
    if range_rule and "config" in range_rule:
        for field, range_config in range_rule["config"].items():
            if field in cleaned_record:
                try:
                    value = float(cleaned_record[field])
                    min_val = range_config.get("min", float('-inf'))
                    max_val = range_config.get("max", float('inf'))
                    
                    if value < min_val:
                        cleaned_record[field] = str(min_val)
                        quality_stats["range_violations_fixed"] += 1
                        record_modified = True
                    elif value > max_val:
                        cleaned_record[field] = str(max_val)
                        quality_stats["range_violations_fixed"] += 1
                        record_modified = True
                        
                except (ValueError, TypeError):
                    pass  # ìˆ«ìê°€ ì•„ë‹Œ ê°’ì€ ê±´ë„ˆë›°ê¸°
    
    # ê·œì¹™ 4: ì¤‘ë³µ ì œê±° (record identifier ê¸°ì¤€)
    if any(rule["name"] == "deduplicate" and rule["enabled"] for rule in quality_config["rules"]):
        record_key = cleaned_record.get('customer_id', str(cleaned_record))
        if record_key not in seen_records:
            seen_records.add(record_key)
            cleaned_records.append(cleaned_record)
        else:
            quality_stats["duplicates_removed"] += 1
    else:
        cleaned_records.append(cleaned_record)

quality_stats["total_output"] = len(cleaned_records)

# ì •ë¦¬ëœ ë°ì´í„° ì €ì¥
with open('$cleaned_data', 'w') as f:
    json.dump(cleaned_records, f, indent=2)

# í’ˆì§ˆ ê°œì„  ë¦¬í¬íŠ¸
print("\nâœ¨ ë°ì´í„° í’ˆì§ˆ ê°œì„  ë¦¬í¬íŠ¸:")
print(f"  ì…ë ¥ ë ˆì½”ë“œ: {quality_stats['total_input']:,}")
print(f"  ì¶œë ¥ ë ˆì½”ë“œ: {quality_stats['total_output']:,}")
print(f"  NULL ê°’ ì •ë¦¬: {quality_stats['null_values_removed']:,}")
print(f"  ë¬¸ìì—´ ì •ê·œí™”: {quality_stats['strings_normalized']:,}")
print(f"  ë²”ìœ„ ìœ„ë°˜ ìˆ˜ì •: {quality_stats['range_violations_fixed']:,}")
print(f"  ì¤‘ë³µ ì œê±°: {quality_stats['duplicates_removed']:,}")

improvement_rate = (quality_stats['null_values_removed'] + 
                   quality_stats['strings_normalized'] + 
                   quality_stats['range_violations_fixed']) / quality_stats['total_input'] * 100

print(f"  í’ˆì§ˆ ê°œì„ ìœ¨: {improvement_rate:.1f}%")
EOF
  
  # 3. ì •ë¦¬ëœ ë°ì´í„°ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì—…ë¡œë“œ
  echo "ğŸ“¥ í’ˆì§ˆ ê°œì„ ëœ ë°ì´í„°ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì—…ë¡œë“œ"
  
  fs bulk-put "$target_fg" "$cleaned_data" \
    --batch-size 200 \
    --output-file "/tmp/quality_migration.log"
  
  if [ $? -eq 0 ]; then
    echo "âœ… í’ˆì§ˆ í–¥ìƒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ"
    
    # ê²€ì¦
    echo "ğŸ” í’ˆì§ˆ ê²€ì¦"
    target_sample="/tmp/${target_fg}_quality_sample.json"
    
    fs export "$target_fg" "$target_sample" \
      --limit 5 --format json
    
    if [ -s "$target_sample" ]; then
      echo "âœ… íƒ€ê²Ÿ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìƒ˜í”Œ:"
      jq '.[0]' "$target_sample" 2>/dev/null
    fi
    
    rm -f "$target_sample"
  else
    echo "âŒ í’ˆì§ˆ í–¥ìƒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨"
    return 1
  fi
  
  # ì •ë¦¬
  rm -f "$raw_data" "$cleaned_data"
  
  echo "ğŸ‰ ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!"
}

quality_improvement_migration "raw-customer-data" "clean-customer-data"
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ëŒ€ìš©ëŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ìµœì í™”
```bash
#!/bin/bash
optimize_large_migration() {
  local source_fg=$1
  local target_fg=$2
  local total_records=${3:-1000000}
  
  echo "âš¡ ëŒ€ìš©ëŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ëŠ¥ ìµœì í™”"
  
  # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ë°˜ ì„¤ì • ê²°ì •
  cpu_cores=$(nproc)
  memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  
  echo "ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:"
  echo "  CPU ì½”ì–´: $cpu_cores"
  echo "  ë©”ëª¨ë¦¬: ${memory_gb}GB"
  echo "  ì˜ˆìƒ ë ˆì½”ë“œ: $total_records"
  
  # ìµœì  ì„¤ì • ê³„ì‚°
  if [ $total_records -gt 5000000 ]; then
    # 500ë§Œ+ ë ˆì½”ë“œ
    optimal_batch_size=1000
    optimal_workers=$((cpu_cores * 3))
  elif [ $total_records -gt 1000000 ]; then
    # 100ë§Œ+ ë ˆì½”ë“œ
    optimal_batch_size=500
    optimal_workers=$((cpu_cores * 2))
  else
    # 100ë§Œ ë¯¸ë§Œ
    optimal_batch_size=200
    optimal_workers=$cpu_cores
  fi
  
  # ë©”ëª¨ë¦¬ ì œí•œ ì ìš©
  max_workers_by_memory=$((memory_gb / 2))  # 2GB per worker
  if [ $optimal_workers -gt $max_workers_by_memory ]; then
    optimal_workers=$max_workers_by_memory
  fi
  
  # ìµœì†Œ/ìµœëŒ€ ì œí•œ
  if [ $optimal_workers -lt 2 ]; then
    optimal_workers=2
  elif [ $optimal_workers -gt 20 ]; then
    optimal_workers=20
  fi
  
  echo "ğŸ¯ ìµœì í™”ëœ ì„¤ì •:"
  echo "  ë°°ì¹˜ í¬ê¸°: $optimal_batch_size"
  echo "  ì›Œì»¤ ìˆ˜: $optimal_workers"
  
  # ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
  records_per_second=$((optimal_workers * 50))  # workerë‹¹ ì´ˆë‹¹ 50ê°œ ê°€ì •
  estimated_seconds=$((total_records / records_per_second))
  estimated_minutes=$((estimated_seconds / 60))
  
  echo "  ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: ${estimated_minutes}ë¶„ (${estimated_seconds}ì´ˆ)"
  
  # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
  echo "ğŸš€ ìµœì í™”ëœ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘"
  
  start_time=$(date +%s)
  
  fs migrate "$source_fg" "$target_fg" \
    --batch-size $optimal_batch_size \
    --max-workers $optimal_workers \
    --clear-target
  
  end_time=$(date +%s)
  actual_duration=$((end_time - start_time))
  actual_minutes=$((actual_duration / 60))
  
  echo "ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:"
  echo "  ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„: ${actual_minutes}ë¶„ (${actual_duration}ì´ˆ)"
  echo "  ì˜ˆìƒ ëŒ€ë¹„: $((actual_duration * 100 / estimated_seconds))%"
  
  if [ $actual_duration -lt $estimated_seconds ]; then
    echo "âœ… ì˜ˆìƒë³´ë‹¤ ë¹ ë¥´ê²Œ ì™„ë£Œ!"
  else
    echo "âš ï¸ ì˜ˆìƒë³´ë‹¤ ì˜¤ë˜ ê±¸ë ¸ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë‚˜ API ì œí•œì„ í™•ì¸í•˜ì„¸ìš”."
  fi
}

optimize_large_migration "huge-dataset" "optimized-dataset" 2000000
```

## ë§ˆì´ê·¸ë ˆì´ì…˜ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤

| ì†ŒìŠ¤ â†’ íƒ€ê²Ÿ | Online Only | Offline Only | Online + Offline |
|-------------|-------------|--------------|------------------|
| **Online Only** | âŒ ì§€ì› ì•ˆë¨* | âŒ ì§€ì› ì•ˆë¨ | âŒ ì§€ì› ì•ˆë¨* |
| **Offline Only** | âœ… ì§€ì›ë¨ | âœ… ì§€ì›ë¨ | âœ… ì§€ì›ë¨ |
| **Online + Offline** | âœ… ì§€ì›ë¨ | âœ… ì§€ì›ë¨ | âœ… ì§€ì›ë¨ |

*ì˜¨ë¼ì¸ ì „ìš©ì—ì„œ ëª¨ë“  ë ˆì½”ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì§ì ‘ì ì¸ ë°©ë²•ì´ ì—†ìŒ

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ì™€ í•´ê²°ì±…

```bash
# ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± ì˜¤ë¥˜
âŒ íƒ€ê²Ÿì— ìˆì§€ë§Œ ì†ŒìŠ¤ì— ì—†ëŠ” í”¼ì²˜: {'new_field'}
# í•´ê²°: íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆì—ì„œ ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° ë˜ëŠ” ì†ŒìŠ¤ì— í•„ë“œ ì¶”ê°€

# Athena í…Œì´ë¸” ì°¾ê¸° ì˜¤ë¥˜  
âŒ Athena í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: feature-group-name
# í•´ê²°: Feature Groupì´ Offline Storeë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸

# ê¶Œí•œ ì˜¤ë¥˜
âŒ AccessDenied: User is not authorized to perform: athena:StartQueryExecution  
# í•´ê²°: IAM ì •ì±…ì— Athena ê¶Œí•œ ì¶”ê°€

# íƒ€ê²Ÿ Feature Group ìƒíƒœ ì˜¤ë¥˜
âŒ Target í”¼ì²˜ ê·¸ë£¹ 'target-fg' ìƒíƒœê°€ 'Creating'ì…ë‹ˆë‹¤
# í•´ê²°: Feature Groupì´ 'Created' ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°
```

### 2. ë§ˆì´ê·¸ë ˆì´ì…˜ ë³µêµ¬
```bash
#!/bin/bash
migration_recovery() {
  local failed_migration_log=$1
  local source_fg=$2
  local target_fg=$3
  
  echo "ğŸš¨ ë§ˆì´ê·¸ë ˆì´ì…˜ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤"
  
  # ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¶”ì¶œ
  if [ -f "$failed_migration_log" ]; then
    echo "ğŸ“‹ ì‹¤íŒ¨ ë¡œê·¸ ë¶„ì„: $failed_migration_log"
    
    # ì‹¤íŒ¨ ë ˆì½”ë“œ ê°œìˆ˜ í™•ì¸
    failed_count=$(grep -c "ì‹¤íŒ¨" "$failed_migration_log" 2>/dev/null || echo 0)
    echo "ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ìˆ˜: $failed_count"
    
    if [ $failed_count -gt 0 ]; then
      echo "ğŸ”„ ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¬ì‹œë„"
      # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì¬ì‹œë„
      fs migrate "$source_fg" "$target_fg" \
        --batch-size 10 \
        --max-workers 1
    fi
  fi
  
  echo "âœ… ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ"
}
```

## ëª¨ë²” ì‚¬ë¡€

1. **ì‚¬ì „ ê²€ì¦**: í•­ìƒ `--dry-run`ìœ¼ë¡œ ê³„íš í™•ì¸
2. **ë°±ì—…**: ì¤‘ìš”í•œ ë°ì´í„°ëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ ë°±ì—…
3. **ì ì§„ì  ì ‘ê·¼**: ì‘ì€ ë°°ì¹˜ë¡œ ì‹œì‘í•´ì„œ ì ì§„ì  í™•ëŒ€
4. **ëª¨ë‹ˆí„°ë§**: ì§„í–‰ë¥ ê³¼ ì˜¤ë¥˜ìœ¨ ì§€ì†ì  ê´€ì°°
5. **ê²€ì¦**: ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
6. **ë¡¤ë°± ê³„íš**: ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ë°©ë²• ì¤€ë¹„

## ê´€ë ¨ ëª…ë ¹ì–´
- `fs create`: íƒ€ê²Ÿ Feature Group ìƒì„±
- `fs schema`: ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸
- `fs clear`: íƒ€ê²Ÿ ë°ì´í„° ì‚¬ì „ ì‚­ì œ
- `fs export`: ë°±ì—… ë° ê²€ì¦ìš© ë°ì´í„° ë‚´ë³´ë‚´ê¸°