# 9. migrate - 피처 그룹 간 데이터 마이그레이션

## 개요
한 Feature Group의 데이터를 다른 Feature Group으로 마이그레이션합니다. 스키마 호환성을 검사하고, 안전하게 데이터를 전송하며, 실패한 레코드를 추적합니다.

## 기본 사용법
```bash
fs migrate SOURCE_FEATURE_GROUP TARGET_FEATURE_GROUP [OPTIONS]
```

## 필수 인자
- `SOURCE_FEATURE_GROUP`: 데이터를 가져올 소스 Feature Group
- `TARGET_FEATURE_GROUP`: 데이터를 저장할 타겟 Feature Group

## 옵션
- `--clear-target`: 마이그레이션 전 타겟 데이터 삭제
- `--batch-size INTEGER`: 배치 크기 (기본값: 100)
- `--max-workers INTEGER`: 최대 워커 수 (기본값: 4)
- `--dry-run`: 실제 마이그레이션 없이 계획만 확인
- `--filter-query TEXT`: Athena WHERE 절을 사용한 데이터 필터링

## 상세 사용 예시

### 1. 기본 마이그레이션
```bash
fs migrate old-customer-profile new-customer-profile
```

**실행 과정:**
1. 스키마 호환성 검사
2. 마이그레이션 계획 수립
3. 사용자 확인
4. 데이터 전송 실행

### 2. 타겟 데이터 삭제 후 마이그레이션
```bash
fs migrate source-features target-features --clear-target
```

### 3. 조건부 마이그레이션 (최근 30일 데이터만)
```bash
fs migrate customer-history customer-history-v2 \
  --filter-query "WHERE event_time >= current_date - interval '30' day"
```

### 4. 성능 최적화된 마이그레이션
```bash
fs migrate large-dataset optimized-dataset \
  --batch-size 500 \
  --max-workers 10 \
  --clear-target
```

### 5. 계획 확인 (실제 마이그레이션 안함)
```bash
fs migrate source-fg target-fg --dry-run
```

**출력 예시:**
```
📋 마이그레이션 계획
============================================================
소스: source-fg (online+offline)
타겟: target-fg (online+offline)
전략: offline_to_online
주요 데이터 소스: offline
예상 레코드 수: 50,000
배치 크기: 100
동시 워커 수: 4

🔍 Dry-run 모드: 실제 마이그레이션은 수행되지 않습니다.
```

## 고급 사용 시나리오

### 1. 스키마 업그레이드 마이그레이션
```bash
#!/bin/bash
schema_upgrade_migration() {
  local old_fg=$1
  local new_fg=$2
  local backup_location=$3
  
  echo "🔄 스키마 업그레이드 마이그레이션: $old_fg → $new_fg"
  
  # 1. 백업 생성
  echo "📦 1단계: 기존 데이터 백업"
  timestamp=$(date +%Y%m%d_%H%M%S)
  backup_file="/tmp/${old_fg}_backup_${timestamp}.json"
  
  fs export "$old_fg" "$backup_file" \
    --format json \
    --compress
  
  if [ $? -ne 0 ]; then
    echo "❌ 백업 실패, 마이그레이션 중단"
    return 1
  fi
  
  # S3에도 백업
  if [ -n "$backup_location" ]; then
    aws s3 cp "${backup_file}.gz" "$backup_location/schema_upgrade_backup_${timestamp}.json.gz"
  fi
  
  # 2. 새 Feature Group 상태 확인
  echo "🔍 2단계: 타겟 Feature Group 검증"
  fs list | grep "$new_fg" || {
    echo "❌ 타겟 Feature Group이 존재하지 않습니다: $new_fg"
    return 1
  }
  
  # 3. 스키마 비교
  echo "📋 3단계: 스키마 차이 분석"
  old_schema="/tmp/${old_fg}_schema.json"
  new_schema="/tmp/${new_fg}_schema.json"
  
  fs schema "$old_fg" -o json > "$old_schema"
  fs schema "$new_fg" -o json > "$new_schema"
  
  # Python을 사용한 스키마 비교
  python3 << EOF
import json

with open('$old_schema', 'r') as f:
    old_features = {item['FeatureName']: item for item in json.load(f)}
    
with open('$new_schema', 'r') as f:
    new_features = {item['FeatureName']: item for item in json.load(f)}

print("📊 스키마 비교 결과:")
print(f"  기존 피처 수: {len(old_features)}")
print(f"  새 피처 수: {len(new_features)}")

# 추가된 피처
added = set(new_features.keys()) - set(old_features.keys())
if added:
    print(f"  ✅ 추가된 피처: {list(added)}")

# 제거된 피처
removed = set(old_features.keys()) - set(new_features.keys())
if removed:
    print(f"  ⚠️ 제거된 피처: {list(removed)}")

# 변경된 피처 타입
changed = []
for name in set(old_features.keys()) & set(new_features.keys()):
    if old_features[name]['FeatureType'] != new_features[name]['FeatureType']:
        changed.append((name, old_features[name]['FeatureType'], new_features[name]['FeatureType']))

if changed:
    print(f"  🔄 타입 변경된 피처:")
    for name, old_type, new_type in changed:
        print(f"    - {name}: {old_type} → {new_type}")

if removed:
    print("⚠️ 제거된 피처가 있습니다. 데이터 손실이 발생할 수 있습니다.")
    exit(1)
EOF
  
  if [ $? -ne 0 ]; then
    echo "❌ 호환되지 않는 스키마 변경이 감지되었습니다."
    return 1
  fi
  
  # 4. 마이그레이션 실행
  echo "🚀 4단계: 데이터 마이그레이션 실행"
  fs migrate "$old_fg" "$new_fg" \
    --clear-target \
    --batch-size 200 \
    --max-workers 6
  
  migration_result=$?
  
  # 5. 검증
  if [ $migration_result -eq 0 ]; then
    echo "✅ 5단계: 마이그레이션 검증"
    
    # 레코드 수 비교
    original_count=$(jq length "$backup_file" 2>/dev/null || echo "unknown")
    echo "원본 레코드 수: $original_count"
    
    # 샘플 데이터 검증
    sample_file="/tmp/${new_fg}_sample.json"
    fs export "$new_fg" "$sample_file" --limit 10 --format json
    
    if [ -s "$sample_file" ]; then
      echo "✅ 타겟 Feature Group에서 데이터 조회 성공"
    else
      echo "⚠️ 타겟 Feature Group에서 데이터를 찾을 수 없습니다"
    fi
    
    echo "🎉 스키마 업그레이드 마이그레이션 완료!"
  else
    echo "❌ 마이그레이션 실패"
    return 1
  fi
  
  # 임시 파일 정리
  rm -f "$old_schema" "$new_schema" "$sample_file"
  
  echo "📁 백업 파일: $backup_file"
}

# 사용 예시
schema_upgrade_migration "customer-v1" "customer-v2" "s3://backups/schema-upgrades/"
```

### 2. 환경 간 데이터 복제
```bash
#!/bin/bash
cross_environment_migration() {
  local source_profile=$1
  local target_profile=$2
  local feature_group=$3
  local source_region=${4:-us-east-1}
  local target_region=${5:-us-west-2}
  
  echo "🌍 환경 간 마이그레이션: $feature_group"
  echo "  소스: $source_profile ($source_region)"
  echo "  타겟: $target_profile ($target_region)"
  
  # 1. 소스 환경에서 데이터 내보내기
  echo "📤 1단계: 소스 환경에서 데이터 내보내기"
  export_file="/tmp/${feature_group}_cross_env_export.json"
  
  AWS_PROFILE="$source_profile" fs --region "$source_region" \
    export "$feature_group" "$export_file" \
    --format json \
    --compress
  
  if [ ! -f "${export_file}.gz" ]; then
    echo "❌ 소스 데이터 내보내기 실패"
    return 1
  fi
  
  # 2. 스키마 복사
  echo "📋 2단계: 스키마 정보 복사"
  schema_file="/tmp/${feature_group}_schema.json"
  
  AWS_PROFILE="$source_profile" fs --region "$source_region" \
    schema "$feature_group" -o json > "$schema_file"
  
  # 3. 타겟 환경에 Feature Group 생성 (존재하지 않는 경우)
  echo "🏗️ 3단계: 타겟 Feature Group 확인/생성"
  
  AWS_PROFILE="$target_profile" fs --region "$target_region" \
    list | grep -q "$feature_group" || {
    
    echo "타겟 Feature Group이 없습니다. 수동으로 생성해야 합니다:"
    echo "AWS_PROFILE=$target_profile fs --region $target_region create $feature_group \\"
    echo "  --schema-file $schema_file \\"
    echo "  --role-arn 'TARGET_ROLE_ARN' \\"
    echo "  --s3-uri 'TARGET_S3_URI'"
    
    read -p "타겟 Feature Group을 생성했습니까? (y/n): " created
    if [ "$created" != "y" ]; then
      echo "❌ 타겟 Feature Group 생성을 취소했습니다."
      return 1
    fi
  }
  
  # 4. 데이터 압축 해제 및 가공
  echo "🔄 4단계: 데이터 변환"
  transformed_file="/tmp/${feature_group}_transformed.json"
  
  gunzip -c "${export_file}.gz" > "$export_file"
  
  # 필요시 데이터 변환 (예: 타임스탬프 조정, 필드 매핑 등)
  python3 << EOF
import json

with open('$export_file', 'r') as f:
    data = json.load(f)

# 환경별 데이터 변환 로직 (예시)
transformed_data = []
for record in data:
    # 환경 정보 추가
    record['migration_source_env'] = '$source_profile'
    record['migration_target_env'] = '$target_profile'
    record['migration_timestamp'] = '$(date -u +"%Y-%m-%dT%H:%M:%SZ")'
    
    transformed_data.append(record)

with open('$transformed_file', 'w') as f:
    json.dump(transformed_data, f, indent=2)

print(f"✅ {len(transformed_data)}개 레코드 변환 완료")
EOF
  
  # 5. 타겟 환경에 데이터 업로드
  echo "📥 5단계: 타겟 환경에 데이터 업로드"
  
  AWS_PROFILE="$target_profile" fs --region "$target_region" \
    bulk-put "$feature_group" "$transformed_file" \
    --batch-size 200 \
    --output-file "/tmp/cross_env_upload.log"
  
  if [ $? -eq 0 ]; then
    echo "✅ 환경 간 마이그레이션 완료!"
    
    # 검증
    echo "🔍 6단계: 마이그레이션 검증"
    target_sample="/tmp/${feature_group}_target_sample.json"
    
    AWS_PROFILE="$target_profile" fs --region "$target_region" \
      export "$feature_group" "$target_sample" \
      --limit 5 --format json
    
    if [ -s "$target_sample" ]; then
      echo "✅ 타겟 환경 검증 성공"
      echo "샘플 데이터:"
      jq '.[0]' "$target_sample" 2>/dev/null || echo "JSON 파싱 오류"
    fi
  else
    echo "❌ 타겟 환경 업로드 실패"
    return 1
  fi
  
  # 정리
  rm -f "$export_file" "${export_file}.gz" "$transformed_file" "$schema_file" "$target_sample"
}

# 사용 예시
cross_environment_migration "dev-profile" "prod-profile" "customer-features" "us-east-1" "us-west-2"
```

### 3. 데이터 아카이빙 마이그레이션
```bash
#!/bin/bash
archive_migration() {
  local active_fg=$1
  local archive_fg=$2
  local archive_age_days=${3:-90}
  local archive_s3_path=$4
  
  echo "📦 데이터 아카이빙 마이그레이션: $active_fg → $archive_fg"
  echo "아카이브 기준: ${archive_age_days}일 이상된 데이터"
  
  # 1. 아카이빙할 데이터 범위 계산
  cutoff_date=$(date -d "$archive_age_days days ago" +%Y-%m-%d)
  echo "📅 아카이브 기준일: $cutoff_date"
  
  # 2. Archive Feature Group 존재 확인
  echo "🔍 Archive Feature Group 확인"
  if ! fs list | grep -q "$archive_fg"; then
    echo "❌ Archive Feature Group이 존재하지 않습니다: $archive_fg"
    echo "💡 다음 명령으로 생성하세요:"
    echo "fs create $archive_fg --schema-file active_schema.json --role-arn ROLE --s3-uri $archive_s3_path"
    return 1
  fi
  
  # 3. 아카이빙할 데이터 내보내기
  echo "📤 아카이브 대상 데이터 내보내기"
  archive_export="/tmp/${active_fg}_archive_export.json"
  
  fs export "$active_fg" "$archive_export" \
    --where "WHERE event_time < '$cutoff_date'" \
    --format json \
    --compress
  
  if [ ! -f "${archive_export}.gz" ]; then
    echo "ℹ️ 아카이브할 데이터가 없습니다."
    return 0
  fi
  
  # 4. 아카이브 데이터 개수 확인
  archive_count=$(gunzip -c "${archive_export}.gz" | jq length)
  echo "📊 아카이브 대상 레코드: $archive_count개"
  
  if [ "$archive_count" -eq 0 ]; then
    echo "ℹ️ 아카이브할 레코드가 없습니다."
    return 0
  fi
  
  # 5. 사용자 확인
  echo ""
  echo "⚠️ 다음 작업이 수행됩니다:"
  echo "  1. $archive_count개 레코드를 $archive_fg로 이동"
  echo "  2. $active_fg에서 해당 데이터 삭제"
  echo ""
  
  read -p "계속하시겠습니까? (yes/no): " confirmation
  if [ "$confirmation" != "yes" ]; then
    echo "❌ 아카이빙이 취소되었습니다."
    return 1
  fi
  
  # 6. Archive Feature Group으로 데이터 이동
  echo "📥 Archive Feature Group으로 데이터 이동"
  gunzip "${archive_export}.gz"
  
  fs bulk-put "$archive_fg" "$archive_export" \
    --batch-size 100 \
    --output-file "/tmp/archive_migration.log"
  
  if [ $? -ne 0 ]; then
    echo "❌ 아카이브 데이터 업로드 실패"
    return 1
  fi
  
  # 7. 원본에서 데이터 삭제 (Offline Store만)
  echo "🗑️ 원본에서 아카이브된 데이터 삭제"
  
  # 실제로는 S3에서 특정 파티션 삭제 또는 clear 명령 사용
  echo "⚠️ 수동으로 다음을 수행해야 합니다:"
  echo "  1. $active_fg의 Offline Store에서 $cutoff_date 이전 데이터 삭제"
  echo "  2. 필요시 fs clear $active_fg --offline-only 실행"
  
  # 8. 검증
  echo "✅ 아카이빙 완료 검증"
  archive_sample="/tmp/${archive_fg}_sample.json"
  
  fs export "$archive_fg" "$archive_sample" \
    --limit 3 --format json
  
  if [ -s "$archive_sample" ]; then
    echo "✅ 아카이브 데이터 검증 성공"
    archived_count=$(jq length "$archive_sample" 2>/dev/null || echo "unknown")
    echo "아카이브 샘플 레코드: $archived_count개"
  fi
  
  # 정리
  rm -f "$archive_export" "$archive_sample"
  
  echo "🎉 데이터 아카이빙 완료: $archive_count개 레코드"
}

# 사용 예시
archive_migration "active-transactions" "archived-transactions" 180 "s3://archive-bucket/transactions/"
```

### 4. 부분 마이그레이션 (특정 고객만)
```bash
#!/bin/bash
partial_migration() {
  local source_fg=$1
  local target_fg=$2
  local customer_list_file=$3
  
  echo "👥 부분 마이그레이션: 특정 고객 데이터만"
  echo "소스: $source_fg"
  echo "타겟: $target_fg"
  echo "고객 목록: $customer_list_file"
  
  # 1. 고객 목록 검증
  if [ ! -f "$customer_list_file" ]; then
    echo "❌ 고객 목록 파일을 찾을 수 없습니다: $customer_list_file"
    return 1
  fi
  
  customer_count=$(wc -l < "$customer_list_file")
  echo "📊 마이그레이션 대상 고객 수: $customer_count"
  
  # 2. IN 절을 위한 고객 ID 문자열 생성
  customer_ids=$(python3 << EOF
import csv

customers = []
with open('$customer_list_file', 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        if row and row[0].strip():  # 빈 줄 제외
            customers.append(f"'{row[0].strip()}'")

# IN 절용 문자열 생성 (최대 1000개씩 처리)
if len(customers) > 1000:
    print("⚠️ 고객 수가 1000명을 초과합니다. 배치 처리가 필요합니다.")
    customers = customers[:1000]
    print(f"처음 1000명만 처리합니다.")

print(','.join(customers))
EOF
)
  
  if [ -z "$customer_ids" ]; then
    echo "❌ 유효한 고객 ID가 없습니다."
    return 1
  fi
  
  # 3. 필터 쿼리 생성
  filter_query="WHERE customer_id IN ($customer_ids)"
  
  # 4. 부분 마이그레이션 실행
  echo "🚀 부분 마이그레이션 실행"
  fs migrate "$source_fg" "$target_fg" \
    --filter-query "$filter_query" \
    --batch-size 50 \
    --max-workers 3
  
  if [ $? -eq 0 ]; then
    echo "✅ 부분 마이그레이션 완료"
    
    # 검증: 마이그레이션된 고객 수 확인
    echo "🔍 마이그레이션 검증"
    migrated_customers="/tmp/migrated_customers.json"
    
    fs export "$target_fg" "$migrated_customers" \
      --columns "customer_id" \
      --format json
    
    if [ -s "$migrated_customers" ]; then
      migrated_count=$(jq 'unique_by(.customer_id) | length' "$migrated_customers" 2>/dev/null || echo "unknown")
      echo "마이그레이션된 고유 고객 수: $migrated_count"
    fi
    
    rm -f "$migrated_customers"
  else
    echo "❌ 부분 마이그레이션 실패"
    return 1
  fi
}

# 고객 목록 파일 생성 예시 (CSV)
cat << 'EOF' > /tmp/target_customers.csv
cust_001
cust_002
cust_003
cust_vip_001
cust_premium_005
EOF

partial_migration "all-customers" "selected-customers" "/tmp/target_customers.csv"
```

### 5. 실시간 동기화 마이그레이션
```bash
#!/bin/bash
realtime_sync_migration() {
  local source_fg=$1
  local target_fg=$2
  local sync_interval_minutes=${3:-60}
  local max_iterations=${4:-0}  # 0 = 무한
  
  echo "🔄 실시간 동기화 마이그레이션 시작"
  echo "소스: $source_fg"
  echo "타겟: $target_fg"
  echo "동기화 간격: ${sync_interval_minutes}분"
  
  # 동기화 상태 파일
  sync_state_file="/tmp/sync_state_${source_fg}_${target_fg}.json"
  
  # 초기 상태 설정
  if [ ! -f "$sync_state_file" ]; then
    cat << EOF > "$sync_state_file"
{
  "last_sync_timestamp": "1970-01-01T00:00:00Z",
  "total_synced": 0,
  "sync_iterations": 0,
  "last_sync_status": "initialized"
}
EOF
  fi
  
  iteration=0
  
  while [ $max_iterations -eq 0 ] || [ $iteration -lt $max_iterations ]; do
    iteration=$((iteration + 1))
    echo ""
    echo "🔄 동기화 반복 #$iteration ($(date))"
    
    # 마지막 동기화 시간 읽기
    last_sync=$(jq -r '.last_sync_timestamp' "$sync_state_file")
    current_time=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    
    echo "마지막 동기화: $last_sync"
    echo "현재 시간: $current_time"
    
    # 증분 데이터 마이그레이션
    incremental_filter="WHERE event_time > '$last_sync'"
    
    echo "🔍 증분 데이터 확인 중..."
    fs migrate "$source_fg" "$target_fg" \
      --filter-query "$incremental_filter" \
      --batch-size 100 \
      --max-workers 3 \
      --dry-run
    
    read -p "증분 마이그레이션을 실행하시겠습니까? (y/n/q=종료): " action
    
    case $action in
      "y"|"Y")
        echo "📥 증분 마이그레이션 실행..."
        
        if fs migrate "$source_fg" "$target_fg" \
           --filter-query "$incremental_filter" \
           --batch-size 100 \
           --max-workers 3; then
          
          # 상태 업데이트
          jq --arg timestamp "$current_time" \
             --arg status "success" \
             --argjson iteration $iteration \
             '.last_sync_timestamp = $timestamp | .last_sync_status = $status | .sync_iterations = $iteration' \
             "$sync_state_file" > "/tmp/sync_state_temp.json"
          
          mv "/tmp/sync_state_temp.json" "$sync_state_file"
          
          echo "✅ 동기화 완료"
        else
          echo "❌ 동기화 실패"
          
          # 실패 상태 기록
          jq --arg status "failed" \
             --argjson iteration $iteration \
             '.last_sync_status = $status | .sync_iterations = $iteration' \
             "$sync_state_file" > "/tmp/sync_state_temp.json"
          
          mv "/tmp/sync_state_temp.json" "$sync_state_file"
        fi
        ;;
      "q"|"Q")
        echo "🛑 동기화 종료"
        break
        ;;
      *)
        echo "⏭️ 이번 동기화 건너뛰기"
        ;;
    esac
    
    # 다음 동기화까지 대기
    if [ $max_iterations -eq 0 ] || [ $iteration -lt $max_iterations ]; then
      echo "⏳ 다음 동기화까지 ${sync_interval_minutes}분 대기 중..."
      sleep $((sync_interval_minutes * 60))
    fi
  done
  
  echo "🏁 실시간 동기화 완료"
  echo "동기화 상태: $sync_state_file"
}

# 백그라운드에서 실행 예시
# nohup realtime_sync_migration "source-fg" "target-fg" 30 > sync.log 2>&1 &
realtime_sync_migration "live-features" "backup-features" 60 10  # 10회 반복
```

### 6. 데이터 품질 향상 마이그레이션
```bash
#!/bin/bash
quality_improvement_migration() {
  local source_fg=$1
  local target_fg=$2
  local quality_rules_file=${3:-"/tmp/quality_rules.json"}
  
  echo "✨ 데이터 품질 향상 마이그레이션"
  echo "소스: $source_fg"
  echo "타겟: $target_fg"
  
  # 기본 품질 규칙 생성
  if [ ! -f "$quality_rules_file" ]; then
    cat << 'EOF' > "$quality_rules_file"
{
  "rules": [
    {
      "name": "remove_null_values",
      "description": "NULL 값 제거",
      "enabled": true
    },
    {
      "name": "normalize_strings",
      "description": "문자열 정규화 (trim, case)",
      "enabled": true
    },
    {
      "name": "validate_ranges",
      "description": "숫자 범위 검증",
      "enabled": true,
      "config": {
        "age": {"min": 0, "max": 120},
        "balance": {"min": 0, "max": 1000000}
      }
    },
    {
      "name": "deduplicate",
      "description": "중복 제거",
      "enabled": true
    }
  ]
}
EOF
  fi
  
  # 1. 원본 데이터 내보내기
  echo "📤 원본 데이터 내보내기"
  raw_data="/tmp/${source_fg}_raw.json"
  
  fs export "$source_fg" "$raw_data" \
    --format json
  
  if [ ! -s "$raw_data" ]; then
    echo "❌ 원본 데이터를 가져올 수 없습니다."
    return 1
  fi
  
  # 2. 데이터 품질 개선 처리
  echo "🔧 데이터 품질 개선 처리"
  cleaned_data="/tmp/${source_fg}_cleaned.json"
  
  python3 << EOF
import json
import re
from datetime import datetime

# 품질 규칙 로드
with open('$quality_rules_file', 'r') as f:
    quality_config = json.load(f)

# 원본 데이터 로드
with open('$raw_data', 'r') as f:
    raw_records = json.load(f)

print(f"📊 원본 레코드 수: {len(raw_records)}")

cleaned_records = []
quality_stats = {
    "total_input": len(raw_records),
    "null_values_removed": 0,
    "strings_normalized": 0,
    "range_violations_fixed": 0,
    "duplicates_removed": 0,
    "total_output": 0
}

seen_records = set()  # 중복 제거용

for record in raw_records:
    cleaned_record = record.copy()
    record_modified = False
    
    # 규칙 1: NULL 값 제거
    if any(rule["name"] == "remove_null_values" and rule["enabled"] for rule in quality_config["rules"]):
        original_fields = len(cleaned_record)
        cleaned_record = {k: v for k, v in cleaned_record.items() 
                         if v is not None and str(v).strip() != '' and str(v).lower() != 'null'}
        if len(cleaned_record) < original_fields:
            quality_stats["null_values_removed"] += 1
            record_modified = True
    
    # 규칙 2: 문자열 정규화
    if any(rule["name"] == "normalize_strings" and rule["enabled"] for rule in quality_config["rules"]):
        for key, value in cleaned_record.items():
            if isinstance(value, str) and key not in ['event_time']:  # event_time은 제외
                original_value = value
                # 앞뒤 공백 제거, 일관된 케이스 (이름 제외)
                if key in ['category', 'status', 'type']:
                    cleaned_record[key] = value.strip().lower()
                else:
                    cleaned_record[key] = value.strip()
                
                if cleaned_record[key] != original_value:
                    record_modified = True
        
        if record_modified:
            quality_stats["strings_normalized"] += 1
    
    # 규칙 3: 숫자 범위 검증
    range_rule = next((rule for rule in quality_config["rules"] 
                      if rule["name"] == "validate_ranges" and rule["enabled"]), None)
    
    if range_rule and "config" in range_rule:
        for field, range_config in range_rule["config"].items():
            if field in cleaned_record:
                try:
                    value = float(cleaned_record[field])
                    min_val = range_config.get("min", float('-inf'))
                    max_val = range_config.get("max", float('inf'))
                    
                    if value < min_val:
                        cleaned_record[field] = str(min_val)
                        quality_stats["range_violations_fixed"] += 1
                        record_modified = True
                    elif value > max_val:
                        cleaned_record[field] = str(max_val)
                        quality_stats["range_violations_fixed"] += 1
                        record_modified = True
                        
                except (ValueError, TypeError):
                    pass  # 숫자가 아닌 값은 건너뛰기
    
    # 규칙 4: 중복 제거 (record identifier 기준)
    if any(rule["name"] == "deduplicate" and rule["enabled"] for rule in quality_config["rules"]):
        record_key = cleaned_record.get('customer_id', str(cleaned_record))
        if record_key not in seen_records:
            seen_records.add(record_key)
            cleaned_records.append(cleaned_record)
        else:
            quality_stats["duplicates_removed"] += 1
    else:
        cleaned_records.append(cleaned_record)

quality_stats["total_output"] = len(cleaned_records)

# 정리된 데이터 저장
with open('$cleaned_data', 'w') as f:
    json.dump(cleaned_records, f, indent=2)

# 품질 개선 리포트
print("\n✨ 데이터 품질 개선 리포트:")
print(f"  입력 레코드: {quality_stats['total_input']:,}")
print(f"  출력 레코드: {quality_stats['total_output']:,}")
print(f"  NULL 값 정리: {quality_stats['null_values_removed']:,}")
print(f"  문자열 정규화: {quality_stats['strings_normalized']:,}")
print(f"  범위 위반 수정: {quality_stats['range_violations_fixed']:,}")
print(f"  중복 제거: {quality_stats['duplicates_removed']:,}")

improvement_rate = (quality_stats['null_values_removed'] + 
                   quality_stats['strings_normalized'] + 
                   quality_stats['range_violations_fixed']) / quality_stats['total_input'] * 100

print(f"  품질 개선율: {improvement_rate:.1f}%")
EOF
  
  # 3. 정리된 데이터를 타겟으로 업로드
  echo "📥 품질 개선된 데이터를 타겟으로 업로드"
  
  fs bulk-put "$target_fg" "$cleaned_data" \
    --batch-size 200 \
    --output-file "/tmp/quality_migration.log"
  
  if [ $? -eq 0 ]; then
    echo "✅ 품질 향상 마이그레이션 완료"
    
    # 검증
    echo "🔍 품질 검증"
    target_sample="/tmp/${target_fg}_quality_sample.json"
    
    fs export "$target_fg" "$target_sample" \
      --limit 5 --format json
    
    if [ -s "$target_sample" ]; then
      echo "✅ 타겟 데이터 품질 검증 샘플:"
      jq '.[0]' "$target_sample" 2>/dev/null
    fi
    
    rm -f "$target_sample"
  else
    echo "❌ 품질 향상 마이그레이션 실패"
    return 1
  fi
  
  # 정리
  rm -f "$raw_data" "$cleaned_data"
  
  echo "🎉 데이터 품질 향상 마이그레이션 완료!"
}

quality_improvement_migration "raw-customer-data" "clean-customer-data"
```

## 성능 최적화

### 1. 대용량 마이그레이션 최적화
```bash
#!/bin/bash
optimize_large_migration() {
  local source_fg=$1
  local target_fg=$2
  local total_records=${3:-1000000}
  
  echo "⚡ 대용량 마이그레이션 성능 최적화"
  
  # 시스템 리소스 기반 설정 결정
  cpu_cores=$(nproc)
  memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  
  echo "📊 시스템 리소스:"
  echo "  CPU 코어: $cpu_cores"
  echo "  메모리: ${memory_gb}GB"
  echo "  예상 레코드: $total_records"
  
  # 최적 설정 계산
  if [ $total_records -gt 5000000 ]; then
    # 500만+ 레코드
    optimal_batch_size=1000
    optimal_workers=$((cpu_cores * 3))
  elif [ $total_records -gt 1000000 ]; then
    # 100만+ 레코드
    optimal_batch_size=500
    optimal_workers=$((cpu_cores * 2))
  else
    # 100만 미만
    optimal_batch_size=200
    optimal_workers=$cpu_cores
  fi
  
  # 메모리 제한 적용
  max_workers_by_memory=$((memory_gb / 2))  # 2GB per worker
  if [ $optimal_workers -gt $max_workers_by_memory ]; then
    optimal_workers=$max_workers_by_memory
  fi
  
  # 최소/최대 제한
  if [ $optimal_workers -lt 2 ]; then
    optimal_workers=2
  elif [ $optimal_workers -gt 20 ]; then
    optimal_workers=20
  fi
  
  echo "🎯 최적화된 설정:"
  echo "  배치 크기: $optimal_batch_size"
  echo "  워커 수: $optimal_workers"
  
  # 예상 처리 시간 계산
  records_per_second=$((optimal_workers * 50))  # worker당 초당 50개 가정
  estimated_seconds=$((total_records / records_per_second))
  estimated_minutes=$((estimated_seconds / 60))
  
  echo "  예상 처리 시간: ${estimated_minutes}분 (${estimated_seconds}초)"
  
  # 마이그레이션 실행
  echo "🚀 최적화된 마이그레이션 시작"
  
  start_time=$(date +%s)
  
  fs migrate "$source_fg" "$target_fg" \
    --batch-size $optimal_batch_size \
    --max-workers $optimal_workers \
    --clear-target
  
  end_time=$(date +%s)
  actual_duration=$((end_time - start_time))
  actual_minutes=$((actual_duration / 60))
  
  echo "📊 성능 결과:"
  echo "  실제 처리 시간: ${actual_minutes}분 (${actual_duration}초)"
  echo "  예상 대비: $((actual_duration * 100 / estimated_seconds))%"
  
  if [ $actual_duration -lt $estimated_seconds ]; then
    echo "✅ 예상보다 빠르게 완료!"
  else
    echo "⚠️ 예상보다 오래 걸렸습니다. 네트워크나 API 제한을 확인하세요."
  fi
}

optimize_large_migration "huge-dataset" "optimized-dataset" 2000000
```

## 마이그레이션 호환성 매트릭스

| 소스 → 타겟 | Online Only | Offline Only | Online + Offline |
|-------------|-------------|--------------|------------------|
| **Online Only** | ❌ 지원 안됨* | ❌ 지원 안됨 | ❌ 지원 안됨* |
| **Offline Only** | ✅ 지원됨 | ✅ 지원됨 | ✅ 지원됨 |
| **Online + Offline** | ✅ 지원됨 | ✅ 지원됨 | ✅ 지원됨 |

*온라인 전용에서 모든 레코드를 추출하는 직접적인 방법이 없음

## 오류 처리 및 문제 해결

### 1. 일반적인 오류와 해결책

```bash
# 스키마 호환성 오류
❌ 타겟에 있지만 소스에 없는 피처: {'new_field'}
# 해결: 타겟 스키마에서 불필요한 필드 제거 또는 소스에 필드 추가

# Athena 테이블 찾기 오류  
❌ Athena 테이블을 찾을 수 없습니다: feature-group-name
# 해결: Feature Group이 Offline Store를 가지고 있는지 확인

# 권한 오류
❌ AccessDenied: User is not authorized to perform: athena:StartQueryExecution  
# 해결: IAM 정책에 Athena 권한 추가

# 타겟 Feature Group 상태 오류
❌ Target 피처 그룹 'target-fg' 상태가 'Creating'입니다
# 해결: Feature Group이 'Created' 상태가 될 때까지 대기
```

### 2. 마이그레이션 복구
```bash
#!/bin/bash
migration_recovery() {
  local failed_migration_log=$1
  local source_fg=$2
  local target_fg=$3
  
  echo "🚨 마이그레이션 복구 프로세스"
  
  # 실패한 레코드 추출
  if [ -f "$failed_migration_log" ]; then
    echo "📋 실패 로그 분석: $failed_migration_log"
    
    # 실패 레코드 개수 확인
    failed_count=$(grep -c "실패" "$failed_migration_log" 2>/dev/null || echo 0)
    echo "실패한 레코드 수: $failed_count"
    
    if [ $failed_count -gt 0 ]; then
      echo "🔄 실패한 레코드 재시도"
      # 작은 배치 크기로 재시도
      fs migrate "$source_fg" "$target_fg" \
        --batch-size 10 \
        --max-workers 1
    fi
  fi
  
  echo "✅ 복구 프로세스 완료"
}
```

## 모범 사례

1. **사전 검증**: 항상 `--dry-run`으로 계획 확인
2. **백업**: 중요한 데이터는 마이그레이션 전 백업
3. **점진적 접근**: 작은 배치로 시작해서 점진적 확대
4. **모니터링**: 진행률과 오류율 지속적 관찰
5. **검증**: 마이그레이션 후 데이터 무결성 확인
6. **롤백 계획**: 실패 시 롤백 방법 준비

## 관련 명령어
- `fs create`: 타겟 Feature Group 생성
- `fs schema`: 스키마 호환성 확인
- `fs clear`: 타겟 데이터 사전 삭제
- `fs export`: 백업 및 검증용 데이터 내보내기