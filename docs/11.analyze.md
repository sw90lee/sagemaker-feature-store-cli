# 11. analyze - 피처 스토어 분석

## 개요
Feature Store의 Offline Store(S3) 스토리지 사용량과 비용을 분석합니다. 파일 유형, 월별 증가 추이, 스토리지 클래스별 분석, 비용 추정 등 포괄적인 분석을 제공합니다.

⚠️ **주의**: 이 명령어는 Offline Store(S3)만 분석합니다. Online Store(DynamoDB)는 분석하지 않습니다.

## 기본 사용법
```bash
fs analyze FEATURE_GROUP_NAME [OPTIONS]
```

또는 S3 위치 직접 지정:
```bash
fs analyze --bucket BUCKET_NAME --prefix PREFIX [OPTIONS]
```

## 필수 인자 (둘 중 하나)
- `FEATURE_GROUP_NAME`: 분석할 Feature Group 이름
- `--bucket` + `--prefix`: S3 위치 직접 지정

## 옵션
- `--export PATH`: 결과를 CSV 파일로 내보내기
- `--output-format [table|json]`: 출력 형식 (기본값: table)

## 상세 사용 예시

### 1. Feature Group 분석
```bash
fs analyze customer-profile
```

**출력 예시:**
```
============================================================
Feature Store 오프라인 스토어 분석: s3://my-bucket/feature-store/
피처 그룹: customer-profile
⚠️  주의: 오프라인 스토어(S3)만 분석됩니다
============================================================

📊 전체 통계
─────────────────
총 파일 수: 1,234
총 용량:
  - Bytes: 12,345,678,901
  - MB: 11,773.46
  - GB: 11.50
  - TB: 0.01

📁 파일 유형별 분석
─────────────────
유형              개수        크기(MB)           비율
─────────────────
parquet         1,200      11,500.20         97.7%
json                30         250.15          2.1%
csv                  4          23.11          0.2%
```

### 2. S3 위치 직접 분석
```bash
fs analyze --bucket my-feature-bucket --prefix customer-data/
```

### 3. 결과를 CSV로 내보내기
```bash
fs analyze customer-profile --export customer_analysis.csv
```

### 4. JSON 형식으로 출력
```bash
fs analyze customer-profile --output-format json
```

## 고급 사용 시나리오

### 1. 모든 Feature Group 분석 및 비교
```bash
#!/bin/bash
analyze_all_feature_groups() {
  local output_dir=${1:-"/tmp/feature_analysis"}
  local threshold_gb=${2:-1.0}
  
  mkdir -p "$output_dir"
  
  echo "📊 전체 Feature Group 분석 시작"
  echo "출력 디렉토리: $output_dir"
  echo "크기 임계값: ${threshold_gb}GB 이상만 분석"
  
  # 모든 Feature Group 목록 가져오기
  feature_groups=($(fs list -o json | jq -r '.[].FeatureGroupName'))
  
  if [ ${#feature_groups[@]} -eq 0 ]; then
    echo "❌ Feature Group을 찾을 수 없습니다."
    return 1
  fi
  
  echo "분석 대상: ${#feature_groups[@]}개 Feature Group"
  
  # 분석 결과 요약 파일 초기화
  summary_file="$output_dir/summary_report.csv"
  echo "Feature_Group,Total_Files,Size_GB,Monthly_Cost_USD,Analysis_Date" > "$summary_file"
  
  analyzed_count=0
  skipped_count=0
  large_fgs=()
  
  for fg in "${feature_groups[@]}"; do
    echo ""
    echo "🔍 분석 중: $fg"
    
    # 개별 Feature Group 분석
    individual_report="$output_dir/${fg}_analysis.csv"
    
    if fs analyze "$fg" \
       --export "$individual_report" \
       --output-format json > "$output_dir/${fg}_analysis.json" 2>/dev/null; then
      
      # JSON 결과에서 주요 정보 추출
      size_gb=$(jq -r '.total_size_gb // 0' "$output_dir/${fg}_analysis.json")
      total_files=$(jq -r '.total_files // 0' "$output_dir/${fg}_analysis.json")
      monthly_cost=$(jq -r '.estimated_monthly_cost // 0' "$output_dir/${fg}_analysis.json")
      analysis_date=$(jq -r '.analysis_timestamp // "N/A"' "$output_dir/${fg}_analysis.json")
      
      echo "  크기: ${size_gb}GB, 파일: ${total_files}개, 월 비용: $${monthly_cost}"
      
      # 요약 파일에 추가
      echo "$fg,$total_files,$size_gb,$monthly_cost,$analysis_date" >> "$summary_file"
      
      # 임계값 이상인 경우 표시
      if (( $(echo "$size_gb >= $threshold_gb" | bc -l) )); then
        large_fgs+=("$fg:${size_gb}GB")
        echo "  📈 큰 Feature Group 감지!"
      fi
      
      analyzed_count=$((analyzed_count + 1))
    else
      echo "  ⚠️ 분석 실패 (Offline Store 없음 또는 권한 부족)"
      skipped_count=$((skipped_count + 1))
    fi
    
    # API 호출 제한 고려한 딜레이
    sleep 1
  done
  
  # 전체 요약 리포트 생성
  echo ""
  echo "📋 전체 분석 완료 요약"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "총 Feature Group 수: ${#feature_groups[@]}"
  echo "분석 성공: $analyzed_count"
  echo "분석 실패: $skipped_count"
  
  if [ ${#large_fgs[@]} -gt 0 ]; then
    echo ""
    echo "🐘 큰 Feature Groups (${threshold_gb}GB 이상):"
    for large_fg in "${large_fgs[@]}"; do
      IFS=':' read -r fg_name size <<< "$large_fg"
      echo "  - $fg_name: $size"
    done
  fi
  
  # Python을 사용한 집계 분석
  if [ -s "$summary_file" ]; then
    echo ""
    echo "📊 집계 통계 (Python 분석):"
    
    python3 << EOF
import pandas as pd
import numpy as np

try:
    df = pd.read_csv('$summary_file')
    
    if len(df) > 0:
        total_size = df['Size_GB'].sum()
        total_cost = df['Monthly_Cost_USD'].sum()
        avg_size = df['Size_GB'].mean()
        median_size = df['Size_GB'].median()
        
        print(f"  총 스토리지: {total_size:.2f}GB")
        print(f"  총 월간 비용: ${total_cost:.2f}")
        print(f"  평균 크기: {avg_size:.2f}GB")
        print(f"  중간값 크기: {median_size:.2f}GB")
        
        # 상위 5개 Feature Group
        top5 = df.nlargest(5, 'Size_GB')
        print(f"\n  💰 비용 상위 5개:")
        for _, row in top5.iterrows():
            print(f"    {row['Feature_Group']}: {row['Size_GB']:.2f}GB (${row['Monthly_Cost_USD']:.2f}/월)")
            
    else:
        print("  분석할 데이터가 없습니다.")
        
except Exception as e:
    print(f"  집계 분석 오류: {e}")
EOF
  fi
  
  echo ""
  echo "📁 분석 결과 파일들:"
  echo "  요약 리포트: $summary_file" 
  echo "  개별 분석: $output_dir/*_analysis.csv"
  echo "  JSON 데이터: $output_dir/*_analysis.json"
  
  echo ""
  echo "🎉 전체 Feature Group 분석 완료!"
}

# 사용 예시
analyze_all_feature_groups "/exports/feature_analysis" 5.0
```

### 2. 비용 최적화 권장 사항 생성
```bash
#!/bin/bash
cost_optimization_analysis() {
  local feature_group=$1
  local target_cost_reduction=${2:-20}  # 20% 비용 절감 목표
  
  echo "💰 비용 최적화 분석: $feature_group"
  echo "목표 비용 절감: ${target_cost_reduction}%"
  
  # 현재 분석 실행
  analysis_file="/tmp/${feature_group}_cost_analysis.json"
  
  if ! fs analyze "$feature_group" \
     --output-format json > "$analysis_file" 2>/dev/null; then
    echo "❌ Feature Group 분석 실패"
    return 1
  fi
  
  # Python을 사용한 최적화 분석
  python3 << EOF
import json
from datetime import datetime, timedelta

# 분석 데이터 로드
with open('$analysis_file', 'r') as f:
    data = json.load(f)

current_size_gb = data.get('total_size_gb', 0)
current_monthly_cost = data.get('estimated_monthly_cost', 0)
total_files = data.get('total_files', 0)
file_types = data.get('file_types', {})
monthly_stats = data.get('monthly_stats', [])

print(f"📊 현재 상황 분석")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print(f"현재 크기: {current_size_gb:.2f}GB")
print(f"현재 월간 비용: ${current_monthly_cost:.2f}")
print(f"총 파일 수: {total_files:,}")

target_savings = current_monthly_cost * $target_cost_reduction / 100
print(f"목표 절약 금액: ${target_savings:.2f}/월 (${target_savings * 12:.2f}/년)")

print(f"\n🔧 최적화 권장사항")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")

recommendations = []
potential_savings = 0

# 1. 스토리지 클래스 최적화
print(f"1. 스토리지 클래스 최적화:")
if current_size_gb > 1:  # 1GB 이상인 경우
    # Standard IA로 이전 시 절약액
    standard_ia_savings = current_size_gb * (0.023 - 0.0125)  # Standard vs IA
    print(f"   → Standard IA로 이전: ${standard_ia_savings:.2f}/월 절약")
    recommendations.append(("Storage Class IA", standard_ia_savings))
    
    # Glacier로 이전 시 절약액 (자주 접근하지 않는 경우)
    if len(monthly_stats) > 6:  # 6개월 이상 데이터가 있는 경우
        glacier_savings = current_size_gb * (0.023 - 0.004)  # Standard vs Glacier
        print(f"   → Glacier로 이전 (오래된 데이터): ${glacier_savings:.2f}/월 절약")
        recommendations.append(("Storage Class Glacier", glacier_savings * 0.5))  # 50%만 이전 가정

# 2. 데이터 압축 분석
print(f"\n2. 데이터 압축:")
parquet_size = file_types.get('parquet', {}).get('size', 0)
json_size = file_types.get('json', {}).get('size', 0)

if json_size > parquet_size * 0.1:  # JSON이 전체의 10% 이상인 경우
    json_gb = json_size / (1024**3)
    compression_savings = json_gb * 0.023 * 0.3  # 30% 압축률 가정
    print(f"   → JSON을 Parquet으로 변환: ${compression_savings:.2f}/월 절약")
    recommendations.append(("Data Compression", compression_savings))

# 3. 데이터 생명주기 관리
print(f"\n3. 데이터 생명주기 관리:")
if len(monthly_stats) > 12:  # 1년 이상 데이터
    old_data_gb = sum([stat['size_gb'] for stat in monthly_stats[:-12]])  # 1년 이전 데이터
    lifecycle_savings = old_data_gb * 0.023 * 0.8  # 80% 비용 절감 (Glacier/Archive)
    print(f"   → 1년 이상 데이터 아카이브: ${lifecycle_savings:.2f}/월 절약")
    recommendations.append(("Data Lifecycle", lifecycle_savings))

# 4. 데이터 중복 제거
print(f"\n4. 데이터 최적화:")
if total_files > 10000:  # 파일이 많은 경우
    dedup_savings = current_monthly_cost * 0.1  # 10% 중복 제거 가정
    print(f"   → 중복 데이터 제거: ${dedup_savings:.2f}/월 절약")
    recommendations.append(("Data Deduplication", dedup_savings))

# 권장사항 우선순위 정리
recommendations.sort(key=lambda x: x[1], reverse=True)
total_potential_savings = sum([r[1] for r in recommendations])

print(f"\n📈 최적화 요약")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print(f"현재 월간 비용: ${current_monthly_cost:.2f}")
print(f"잠재적 절약액: ${total_potential_savings:.2f}/월")
print(f"절약률: {(total_potential_savings/current_monthly_cost)*100 if current_monthly_cost > 0 else 0:.1f}%")

if total_potential_savings >= target_savings:
    print(f"✅ 목표 절약 금액 달성 가능!")
else:
    print(f"⚠️  목표에 ${target_savings - total_potential_savings:.2f} 부족")

print(f"\n🎯 우선순위별 권장사항:")
for i, (method, savings) in enumerate(recommendations[:5], 1):
    print(f"{i}. {method}: ${savings:.2f}/월")
EOF
  
  echo ""
  echo "📋 실행 가능한 최적화 단계:"
  echo "1. AWS S3 Intelligent Tiering 활성화"
  echo "2. 오래된 데이터에 대한 Lifecycle 정책 설정" 
  echo "3. 데이터 형식 최적화 (JSON → Parquet)"
  echo "4. 정기적인 데이터 정리 및 중복 제거"
  
  # 정리
  rm -f "$analysis_file"
}

cost_optimization_analysis "large-feature-group" 30
```

### 3. 데이터 증가 추세 예측
```bash
#!/bin/bash
growth_trend_analysis() {
  local feature_group=$1
  local prediction_months=${2:-6}
  
  echo "📈 데이터 증가 추세 분석: $feature_group"
  echo "예측 기간: ${prediction_months}개월"
  
  # 분석 실행
  analysis_file="/tmp/${feature_group}_growth.json"
  
  if ! fs analyze "$feature_group" \
     --output-format json > "$analysis_file" 2>/dev/null; then
    echo "❌ Feature Group 분석 실패"
    return 1
  fi
  
  # Python을 사용한 추세 분석
  python3 << EOF
import json
import numpy as np
import matplotlib
matplotlib.use('Agg')  # GUI 없는 환경에서 사용
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import pandas as pd

# 분석 데이터 로드
with open('$analysis_file', 'r') as f:
    data = json.load(f)

monthly_stats = data.get('monthly_stats', [])
if len(monthly_stats) < 3:
    print("❌ 추세 분석에 필요한 데이터가 부족합니다 (최소 3개월 필요)")
    exit(1)

# 데이터 준비
df = pd.DataFrame(monthly_stats)
df['month_num'] = range(len(df))
df['size_gb'] = df['size_gb'].astype(float)

print(f"📊 기존 데이터 분석 ({len(monthly_stats)}개월)")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")

# 기본 통계
total_growth = df['size_gb'].iloc[-1] - df['size_gb'].iloc[0] if len(df) > 1 else 0
avg_monthly_growth = total_growth / (len(df) - 1) if len(df) > 1 else 0
growth_rate = (df['size_gb'].iloc[-1] / df['size_gb'].iloc[0] - 1) * 100 if df['size_gb'].iloc[0] > 0 else 0

print(f"첫 달 크기: {df['size_gb'].iloc[0]:.2f}GB")
print(f"최근 달 크기: {df['size_gb'].iloc[-1]:.2f}GB")
print(f"총 증가량: {total_growth:.2f}GB")
print(f"월평균 증가: {avg_monthly_growth:.2f}GB")
print(f"증가율: {growth_rate:.1f}%")

# 추세선 계산 (선형 회귀)
if len(df) > 2:
    from sklearn.linear_model import LinearRegression
    import warnings
    warnings.filterwarnings('ignore')
    
    X = df[['month_num']]
    y = df['size_gb']
    
    model = LinearRegression()
    model.fit(X, y)
    
    # 예측
    future_months = range(len(df), len(df) + $prediction_months)
    future_X = [[month] for month in future_months]
    predictions = model.predict(future_X)
    
    print(f"\n🔮 ${prediction_months}개월 예측 (선형 추세)")
    print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    
    current_size = df['size_gb'].iloc[-1]
    predicted_size = predictions[-1]
    predicted_growth = predicted_size - current_size
    
    for i, (month_offset, pred_size) in enumerate(zip(future_months, predictions), 1):
        print(f"{i}개월 후: {pred_size:.2f}GB")
    
    print(f"\n예상 증가량: {predicted_growth:.2f}GB")
    print(f"예상 증가율: {(predicted_growth/current_size)*100:.1f}%")
    
    # 비용 예측
    current_cost = data.get('estimated_monthly_cost', 0)
    cost_per_gb = current_cost / current_size if current_size > 0 else 0.023
    predicted_cost = predicted_size * cost_per_gb
    cost_increase = predicted_cost - current_cost
    
    print(f"\n💰 비용 예측")
    print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    print(f"현재 월간 비용: ${current_cost:.2f}")
    print(f"예상 월간 비용: ${predicted_cost:.2f}")
    print(f"비용 증가: ${cost_increase:.2f}/월")
    print(f"연간 추가 비용: ${cost_increase * 12:.2f}")
    
    # 경고 및 권장사항
    print(f"\n⚠️  경고 및 권장사항")
    print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    
    if growth_rate > 50:  # 50% 이상 증가
        print(f"🚨 높은 증가율 ({growth_rate:.1f}%) 감지!")
        print(f"   → 데이터 정리 정책 검토 필요")
        print(f"   → 스토리지 최적화 고려")
    
    if predicted_cost > current_cost * 2:
        print(f"💸 비용 급증 예상 ({(predicted_cost/current_cost)*100:.0f}%)")
        print(f"   → 예산 계획 업데이트 필요")
        print(f"   → 데이터 아카이빙 정책 수립")
    
    if predicted_size > 100:  # 100GB 초과 예상
        print(f"📦 대용량 데이터 예상 ({predicted_size:.1f}GB)")
        print(f"   → 데이터 파티셔닝 고려")
        print(f"   → 성능 최적화 검토")

else:
    print("⚠️ 추세 분석을 위한 데이터가 부족합니다")
EOF
  
  # 그래프 생성 (선택사항)
  if command -v python3 >/dev/null && python3 -c "import matplotlib" 2>/dev/null; then
    echo ""
    echo "📊 추세 그래프 생성 중..."
    
    python3 << EOF
import json
import matplotlib.pyplot as plt
import pandas as pd

with open('$analysis_file', 'r') as f:
    data = json.load(f)

monthly_stats = data.get('monthly_stats', [])
if len(monthly_stats) >= 3:
    df = pd.DataFrame(monthly_stats)
    
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(df)), df['size_gb'], 'b-o', label='실제 데이터')
    plt.title('$feature_group 데이터 증가 추세')
    plt.xlabel('월')
    plt.ylabel('크기 (GB)')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.savefig('/tmp/${feature_group}_growth_chart.png', dpi=150)
    print("그래프 저장: /tmp/${feature_group}_growth_chart.png")
EOF
  fi
  
  # 정리
  rm -f "$analysis_file"
}

growth_trend_analysis "analytics-features" 12
```

### 4. 스토리지 클래스 최적화 분석
```bash
#!/bin/bash
storage_class_optimization() {
  local feature_group=$1
  local access_pattern=${2:-"infrequent"}  # frequent, infrequent, archive
  
  echo "💾 스토리지 클래스 최적화 분석: $feature_group"
  echo "접근 패턴: $access_pattern"
  
  # 현재 분석
  current_analysis="/tmp/${feature_group}_current.json"
  
  if ! fs analyze "$feature_group" \
     --output-format json > "$current_analysis" 2>/dev/null; then
    echo "❌ Feature Group 분석 실패"
    return 1
  fi
  
  # 최적화 시나리오 분석
  python3 << EOF
import json

# 현재 데이터 로드
with open('$current_analysis', 'r') as f:
    current = json.load(f)

current_size_gb = current.get('total_size_gb', 0)
current_cost = current.get('estimated_monthly_cost', 0)

# 스토리지 클래스별 가격 (USD per GB per month)
storage_pricing = {
    'STANDARD': 0.023,
    'STANDARD_IA': 0.0125,
    'INTELLIGENT_TIERING': 0.0125,  # 평균
    'GLACIER': 0.004,
    'GLACIER_IR': 0.0036,
    'DEEP_ARCHIVE': 0.00099
}

# 접근 패턴별 권장 클래스
access_recommendations = {
    'frequent': ['STANDARD', 'INTELLIGENT_TIERING'],
    'infrequent': ['STANDARD_IA', 'INTELLIGENT_TIERING', 'GLACIER_IR'],
    'archive': ['GLACIER', 'DEEP_ARCHIVE']
}

print(f"📊 현재 상황")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print(f"데이터 크기: {current_size_gb:.2f}GB")
print(f"현재 비용: ${current_cost:.2f}/월 (${current_cost*12:.2f}/년)")
print(f"추정 스토리지 클래스: STANDARD")

print(f"\n💡 접근 패턴별 최적화 시나리오")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")

scenarios = []

for pattern in ['frequent', 'infrequent', 'archive']:
    print(f"\n📋 {pattern.upper()} 접근 패턴:")
    
    for storage_class in access_recommendations[pattern]:
        monthly_cost = current_size_gb * storage_pricing[storage_class]
        annual_cost = monthly_cost * 12
        savings = current_cost - monthly_cost
        annual_savings = savings * 12
        
        print(f"  {storage_class}:")
        print(f"    월간 비용: ${monthly_cost:.2f}")
        print(f"    연간 비용: ${annual_cost:.2f}")
        print(f"    월간 절약: ${savings:.2f}")
        print(f"    연간 절약: ${annual_savings:.2f}")
        
        scenarios.append({
            'pattern': pattern,
            'class': storage_class,
            'monthly_cost': monthly_cost,
            'monthly_savings': savings,
            'annual_savings': annual_savings
        })

# 최적 시나리오 찾기
if '$access_pattern' in access_recommendations:
    best_scenarios = [s for s in scenarios if s['pattern'] == '$access_pattern']
    best_scenario = max(best_scenarios, key=lambda x: x['monthly_savings'])
    
    print(f"\n🎯 권장 최적화 (${access_pattern} 패턴)")
    print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    print(f"최적 스토리지 클래스: {best_scenario['class']}")
    print(f"예상 월간 절약: ${best_scenario['monthly_savings']:.2f}")
    print(f"예상 연간 절약: ${best_scenario['annual_savings']:.2f}")
    print(f"절약률: {(best_scenario['monthly_savings']/current_cost)*100:.1f}%")

print(f"\n📋 구현 단계")
print(f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
print(f"1. S3 Lifecycle 정책 생성")
print(f"2. 기존 데이터 전환 (점진적)")
print(f"3. 새 데이터 자동 분류")
print(f"4. 모니터링 및 최적화")

# S3 Lifecycle 정책 예시 생성
bucket = current.get('bucket', 'your-bucket')
prefix = current.get('prefix', 'your-prefix')

lifecycle_policy = {
    "Rules": [
        {
            "ID": "FeatureStoreOptimization",
            "Status": "Enabled",
            "Filter": {"Prefix": prefix},
            "Transitions": []
        }
    ]
}

# 접근 패턴에 따른 전환 규칙 추가
if '$access_pattern' == 'infrequent':
    lifecycle_policy["Rules"][0]["Transitions"] = [
        {"Days": 30, "StorageClass": "STANDARD_IA"},
        {"Days": 90, "StorageClass": "GLACIER"}
    ]
elif '$access_pattern' == 'archive':
    lifecycle_policy["Rules"][0]["Transitions"] = [
        {"Days": 1, "StorageClass": "GLACIER"},
        {"Days": 90, "StorageClass": "DEEP_ARCHIVE"}
    ]

print(f"\n📄 S3 Lifecycle 정책 예시:")
print(json.dumps(lifecycle_policy, indent=2))

EOF
  
  echo ""
  echo "🔧 실행 명령어 예시:"
  echo "aws s3api put-bucket-lifecycle-configuration \\"
  echo "  --bucket YOUR_BUCKET \\"
  echo "  --lifecycle-configuration file://lifecycle-policy.json"
  
  # 정리
  rm -f "$current_analysis"
}

storage_class_optimization "historical-data" "archive"
```

### 5. 정기 분석 및 리포팅 시스템
```bash
#!/bin/bash
setup_periodic_analysis() {
  local feature_groups=("$@")
  local report_frequency=${1:-"weekly"}  # daily, weekly, monthly
  
  if [ ${#feature_groups[@]} -eq 0 ]; then
    echo "❌ 분석할 Feature Group을 지정해주세요"
    return 1
  fi
  
  echo "📅 정기 분석 시스템 설정"
  echo "주기: $report_frequency"
  echo "대상 Feature Groups: ${feature_groups[*]}"
  
  # 분석 스크립트 생성
  analysis_script="/usr/local/bin/feature_store_periodic_analysis.sh"
  
  cat << 'SCRIPT_EOF' > "$analysis_script"
#!/bin/bash
# 자동 생성된 Feature Store 정기 분석 스크립트

REPORT_DIR="/var/reports/feature_store"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
REPORT_DATE=$(date +"%Y-%m-%d")

mkdir -p "$REPORT_DIR"

# 로그 설정
exec 1> >(tee -a "$REPORT_DIR/analysis_log.txt")
exec 2>&1

echo "🚀 Feature Store 정기 분석 시작: $(date)"
echo "리포트 ID: $TIMESTAMP"
echo ""

# 분석할 Feature Groups 목록
FEATURE_GROUPS=("FEATURE_GROUPS_PLACEHOLDER")

# 전체 요약 초기화
SUMMARY_FILE="$REPORT_DIR/summary_$TIMESTAMP.csv"
echo "Feature_Group,Total_Files,Size_GB,Monthly_Cost_USD,Status,Analysis_Time" > "$SUMMARY_FILE"

total_size=0
total_cost=0
analysis_success=0
analysis_failed=0

# 각 Feature Group 분석
for fg in "${FEATURE_GROUPS[@]}"; do
    echo "🔍 분석 중: $fg"
    
    fg_report_dir="$REPORT_DIR/$fg"
    mkdir -p "$fg_report_dir"
    
    individual_csv="$fg_report_dir/${fg}_${TIMESTAMP}.csv"
    individual_json="$fg_report_dir/${fg}_${TIMESTAMP}.json"
    
    if fs analyze "$fg" \
       --export "$individual_csv" \
       --output-format json > "$individual_json" 2>/dev/null; then
        
        # JSON에서 정보 추출
        size_gb=$(jq -r '.total_size_gb // 0' "$individual_json")
        files=$(jq -r '.total_files // 0' "$individual_json")  
        cost=$(jq -r '.estimated_monthly_cost // 0' "$individual_json")
        
        total_size=$(echo "$total_size + $size_gb" | bc)
        total_cost=$(echo "$total_cost + $cost" | bc)
        analysis_success=$((analysis_success + 1))
        
        echo "  ✅ 성공: ${size_gb}GB, ${files}개 파일, $${cost}/월"
        echo "$fg,$files,$size_gb,$cost,SUCCESS,$(date -Iseconds)" >> "$SUMMARY_FILE"
        
    else
        echo "  ❌ 실패: 분석 불가"
        analysis_failed=$((analysis_failed + 1))
        echo "$fg,0,0,0,FAILED,$(date -Iseconds)" >> "$SUMMARY_FILE"
    fi
    
    echo ""
done

# 전체 요약 리포트 생성
SUMMARY_REPORT="$REPORT_DIR/executive_summary_$TIMESTAMP.txt"

cat << EOF > "$SUMMARY_REPORT"
📊 Feature Store 정기 분석 리포트
========================================
생성일: $REPORT_DATE
리포트 ID: $TIMESTAMP

📈 전체 현황
--------
분석 대상: ${#FEATURE_GROUPS[@]}개 Feature Groups
분석 성공: $analysis_success
분석 실패: $analysis_failed
총 스토리지: ${total_size}GB
총 월간 비용: \$${total_cost}
예상 연간 비용: \$$(echo "$total_cost * 12" | bc)

📁 상세 리포트
--------
- 요약 데이터: $SUMMARY_FILE
- 개별 분석: $REPORT_DIR/*/

EOF

# 임계값 기반 알림
if (( $(echo "$total_cost > 100" | bc -l) )); then
    echo "⚠️  비용 알림: 월간 비용이 $100를 초과했습니다 ($${total_cost})" >> "$SUMMARY_REPORT"
fi

if (( $(echo "$total_size > 500" | bc -l) )); then
    echo "📦 용량 알림: 총 스토리지가 500GB를 초과했습니다 (${total_size}GB)" >> "$SUMMARY_REPORT"
fi

echo "✅ 정기 분석 완료: $(date)"
echo "📋 요약 리포트: $SUMMARY_REPORT"

# 이메일 발송 (선택사항)
if command -v mail >/dev/null && [ -n "$REPORT_EMAIL" ]; then
    mail -s "Feature Store 분석 리포트 - $REPORT_DATE" "$REPORT_EMAIL" < "$SUMMARY_REPORT"
    echo "📧 리포트 이메일 발송: $REPORT_EMAIL"
fi

SCRIPT_EOF
  
  # Feature Groups 목록을 스크립트에 삽입
  feature_groups_str=$(printf '"%s" ' "${feature_groups[@]}")
  sed -i "s/FEATURE_GROUPS_PLACEHOLDER/$feature_groups_str/g" "$analysis_script"
  
  chmod +x "$analysis_script"
  
  # Cron 작업 설정
  case $report_frequency in
    "daily")
      cron_expr="0 6 * * *"  # 매일 오전 6시
      ;;
    "weekly")
      cron_expr="0 6 * * 1"  # 매주 월요일 오전 6시
      ;;
    "monthly")
      cron_expr="0 6 1 * *"  # 매월 1일 오전 6시
      ;;
    *)
      cron_expr="0 6 * * 1"  # 기본값: 주간
      ;;
  esac
  
  echo ""
  echo "✅ 정기 분석 시스템 설정 완료"
  echo "📋 설정 내용:"
  echo "  스크립트: $analysis_script"
  echo "  실행 주기: $report_frequency ($cron_expr)"
  echo "  리포트 저장: /var/reports/feature_store/"
  
  echo ""
  echo "🔧 Cron 작업 설치:"
  echo "sudo mkdir -p /var/reports/feature_store"
  echo "sudo chown \$(whoami) /var/reports/feature_store"
  echo "(crontab -l 2>/dev/null; echo '$cron_expr $analysis_script') | crontab -"
  
  echo ""
  echo "📧 이메일 알림 설정 (선택사항):"
  echo "export REPORT_EMAIL=your-email@company.com"
}

# 사용 예시
setup_periodic_analysis "weekly" "customer-features" "transaction-history" "user-analytics"
```

### 6. 멀티 리전 분석
```bash
#!/bin/bash
multi_region_analysis() {
  local regions=("$@")
  
  if [ ${#regions[@]} -eq 0 ]; then
    regions=("us-east-1" "us-west-2" "ap-northeast-2")
  fi
  
  echo "🌍 멀티 리전 Feature Store 분석"
  echo "대상 리전: ${regions[*]}"
  
  total_global_size=0
  total_global_cost=0
  region_summary=()
  
  for region in "${regions[@]}"; do
    echo ""
    echo "📍 리전 분석: $region"
    echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    
    # 해당 리전의 Feature Groups 목록 가져오기
    region_fgs=($(AWS_REGION="$region" fs list -o json | jq -r '.[].FeatureGroupName' 2>/dev/null || echo ""))
    
    if [ ${#region_fgs[@]} -eq 0 ]; then
      echo "  ℹ️ 해당 리전에 Feature Group이 없습니다."
      region_summary+=("$region:0:0:0")
      continue
    fi
    
    echo "  Feature Groups: ${#region_fgs[@]}개"
    
    region_size=0
    region_cost=0
    region_analyzed=0
    
    for fg in "${region_fgs[@]}"; do
      echo "    🔍 $fg 분석 중..."
      
      # 임시 파일
      temp_analysis="/tmp/${region}_${fg}_analysis.json"
      
      if AWS_REGION="$region" fs analyze "$fg" \
         --output-format json > "$temp_analysis" 2>/dev/null; then
        
        fg_size=$(jq -r '.total_size_gb // 0' "$temp_analysis")
        fg_cost=$(jq -r '.estimated_monthly_cost // 0' "$temp_analysis")
        
        region_size=$(echo "$region_size + $fg_size" | bc)
        region_cost=$(echo "$region_cost + $fg_cost" | bc)
        region_analyzed=$((region_analyzed + 1))
        
        echo "      ✅ ${fg_size}GB, $${fg_cost}/월"
        
        rm -f "$temp_analysis"
      else
        echo "      ⚠️ 분석 실패"
      fi
    done
    
    total_global_size=$(echo "$total_global_size + $region_size" | bc)
    total_global_cost=$(echo "$total_global_cost + $region_cost" | bc)
    
    echo "  📊 $region 요약:"
    echo "    총 크기: ${region_size}GB"  
    echo "    월간 비용: $${region_cost}"
    echo "    분석 성공: $region_analyzed/${#region_fgs[@]}"
    
    region_summary+=("$region:${#region_fgs[@]}:$region_size:$region_cost")
  done
  
  # 글로벌 요약
  echo ""
  echo "🌐 글로벌 요약"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "총 리전 수: ${#regions[@]}"
  echo "총 스토리지: ${total_global_size}GB"
  echo "총 월간 비용: $${total_global_cost}"
  echo "예상 연간 비용: $$(echo "$total_global_cost * 12" | bc)"
  
  # 리전별 비교
  echo ""
  echo "📋 리전별 비교"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  printf "%-15s %10s %15s %15s\n" "리전" "FG 수" "크기(GB)" "비용(/월)"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  for summary in "${region_summary[@]}"; do
    IFS=':' read -r region fg_count size cost <<< "$summary"
    printf "%-15s %10s %15s %15s\n" "$region" "$fg_count" "$size" "\$${cost}"
  done
  
  # 최적화 권장사항
  echo ""
  echo "💡 글로벌 최적화 권장사항"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  # 가장 비용이 높은 리전 찾기
  max_cost=0
  max_region=""
  for summary in "${region_summary[@]}"; do
    IFS=':' read -r region fg_count size cost <<< "$summary"
    if (( $(echo "$cost > $max_cost" | bc -l) )); then
      max_cost=$cost
      max_region=$region
    fi
  done
  
  echo "1. 최대 비용 리전: $max_region (\$${max_cost}/월)"
  echo "   → 해당 리전 우선 최적화 권장"
  
  if (( $(echo "$total_global_cost > 500" | bc -l) )); then
    echo "2. 고비용 환경 감지 (\$${total_global_cost}/월)"
    echo "   → 글로벌 데이터 생명주기 정책 수립"
  fi
  
  echo "3. 리전 간 데이터 중복 확인"
  echo "   → 불필요한 리전 간 복제 제거"
  
  echo "4. 리전별 접근 패턴 분석"
  echo "   → 지역 특성에 맞는 스토리지 클래스 적용"
}

# 사용 예시
multi_region_analysis "us-east-1" "us-west-2" "eu-west-1" "ap-northeast-2"
```

## 성능 및 제한사항

### 1. 분석 성능 최적화
```bash
#!/bin/bash
optimize_analysis_performance() {
  local feature_group=$1
  
  echo "⚡ 분석 성능 최적화: $feature_group"
  
  # 사전 체크: 데이터 크기 추정
  echo "📏 데이터 크기 사전 추정 중..."
  
  # Feature Group 정보로부터 S3 위치 확인
  fg_info=$(fs list -o json | jq --arg fg "$feature_group" \
    '.[] | select(.FeatureGroupName == $fg)')
  
  if [ -z "$fg_info" ]; then
    echo "❌ Feature Group을 찾을 수 없습니다"
    return 1
  fi
  
  # Offline Store 존재 여부 확인
  has_offline=$(echo "$fg_info" | jq -r '.IngestMode | contains("Offline")')
  
  if [ "$has_offline" != "true" ]; then
    echo "ℹ️ Offline Store가 없어서 분석할 수 없습니다"
    return 0
  fi
  
  # 분석 실행 시간 측정
  echo "⏱️ 성능 측정 시작..."
  start_time=$(date +%s.%N)
  
  if fs analyze "$feature_group" > "/tmp/perf_test_output.txt" 2>&1; then
    end_time=$(date +%s.%N)
    duration=$(echo "$end_time - $start_time" | bc)
    
    # 결과 분석
    total_files=$(grep "총 파일 수:" "/tmp/perf_test_output.txt" | grep -o '[0-9,]*' | tr -d ',')
    total_gb=$(grep "GB:" "/tmp/perf_test_output.txt" | head -1 | grep -o '[0-9,.]*' | tr -d ',')
    
    echo "📊 성능 결과:"
    echo "  처리 시간: ${duration}초"
    echo "  처리한 파일: ${total_files:-0}개"
    echo "  데이터 크기: ${total_gb:-0}GB"
    
    if [ -n "$total_files" ] && [ "$total_files" -gt 0 ]; then
      files_per_sec=$(echo "scale=2; $total_files / $duration" | bc)
      echo "  처리 속도: ${files_per_sec} 파일/초"
    fi
    
    # 성능 권장사항
    echo ""
    echo "🚀 성능 최적화 권장사항:"
    
    if [ -n "$duration" ] && (( $(echo "$duration > 60" | bc -l) )); then
      echo "  - 분석 시간이 깁니다 (${duration}초)"
      echo "  - 더 작은 범위로 분석 고려"
      echo "  - S3 Transfer Acceleration 활성화 검토"
    fi
    
    if [ -n "$total_files" ] && [ "$total_files" -gt 100000 ]; then
      echo "  - 파일 수가 많습니다 (${total_files}개)"
      echo "  - 파일 통합을 통한 최적화 고려"
    fi
    
  else
    echo "❌ 성능 테스트 실패"
    cat "/tmp/perf_test_output.txt"
  fi
  
  # 정리
  rm -f "/tmp/perf_test_output.txt"
}
```

## 오류 처리 및 문제 해결

### 1. 일반적인 분석 오류
```bash
# S3 권한 오류
❌ S3 액세스 오류: Access Denied
# 해결: S3 버킷에 대한 ListBucket, GetObject 권한 필요

# Feature Group 없음
❌ 피처 그룹 'non-existent' 정보를 가져올 수 없습니다: ResourceNotFound
# 해결: Feature Group 이름 확인

# Offline Store 없음
❌ 지정된 위치에서 파일을 찾을 수 없습니다
# 해결: Feature Group이 Offline Store를 가지고 있는지 확인

# 큰 데이터셋 분석 시간 초과
❌ 분석이 너무 오래 걸립니다
# 해결: 더 작은 범위로 분할 분석 또는 샘플링 사용
```

### 2. 분석 복구 및 재시도
```bash
#!/bin/bash
retry_failed_analysis() {
  local feature_group=$1
  local max_retries=${2:-3}
  
  echo "🔄 분석 재시도: $feature_group"
  
  for attempt in $(seq 1 $max_retries); do
    echo "시도 $attempt/$max_retries..."
    
    if fs analyze "$feature_group" --output-format json > "/tmp/retry_$attempt.json" 2>/dev/null; then
      echo "✅ $attempt번째 시도 성공"
      cat "/tmp/retry_$attempt.json"
      rm -f "/tmp/retry_$attempt.json"
      return 0
    else
      echo "❌ $attempt번째 시도 실패"
      if [ $attempt -lt $max_retries ]; then
        sleep_time=$((attempt * 10))
        echo "  ${sleep_time}초 대기 후 재시도..."
        sleep $sleep_time
      fi
    fi
  done
  
  echo "💥 모든 재시도 실패"
  return 1
}
```

## 모범 사례

1. **정기적 분석**: 월별 또는 주별로 정기적인 분석 실행
2. **비용 모니터링**: 임계값 기반 알림 시스템 구축  
3. **트렌드 분석**: 데이터 증가 추세 모니터링
4. **최적화 검토**: 분석 결과 기반 스토리지 최적화
5. **다중 리전 관리**: 글로벌 환경에서 리전별 분석
6. **자동화**: 스크립트를 통한 분석 자동화

## 관련 명령어
- `fs export`: 상세 데이터 분석을 위한 데이터 내보내기
- `fs list`: Feature Group 현황 파악
- `fs create`: 새 Feature Group 생성 시 스토리지 계획
- `fs delete`: 불필요한 Feature Group 정리