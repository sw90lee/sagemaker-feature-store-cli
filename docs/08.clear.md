# 8. clear - 피처 그룹 데이터 삭제

## 개요
Feature Group의 Online Store 및/또는 Offline Store에서 모든 데이터를 삭제합니다. 선택적으로 백업을 생성하고, 조건부 삭제를 지원합니다.

⚠️ **경고**: 이 작업은 되돌릴 수 없습니다. 중요한 데이터는 반드시 백업하세요.

## 기본 사용법
```bash
fs clear FEATURE_GROUP_NAME [OPTIONS]
```

## 필수 인자
- `FEATURE_GROUP_NAME`: 데이터를 삭제할 Feature Group 이름

## 옵션
- `--online-only`: Online Store만 삭제
- `--offline-only`: Offline Store만 삭제 
- `--force`: 확인 없이 강제 삭제
- `--backup-s3 PATH`: 삭제 전 S3에 백업
- `--dry-run`: 실제 삭제 없이 계획만 확인

## 상세 사용 예시

### 1. 기본 전체 삭제 (확인 포함)
```bash
fs clear customer-profile
```

**실행 과정:**
1. 삭제 계획 표시
2. 사용자 확인 요청
3. Online/Offline Store 모두 삭제

### 2. Online Store만 삭제
```bash
fs clear customer-profile --online-only
```

### 3. Offline Store만 삭제
```bash
fs clear customer-profile --offline-only
```

### 4. 백업과 함께 삭제
```bash
fs clear customer-profile \
  --backup-s3 s3://my-backup-bucket/feature-backups/ \
  --force
```

### 5. 계획 확인 (실제 삭제 안함)
```bash
fs clear customer-profile --dry-run
```

**출력 예시:**
```
📋 삭제 계획: 'customer-profile'
==================================================
• 온라인 스토어: 모든 레코드 삭제
• 오프라인 스토어: S3 데이터 삭제

🔍 Dry-run 모드: 실제 삭제는 수행되지 않습니다.
```

## 고급 사용 시나리오

### 1. 안전한 삭제 워크플로우
```bash
#!/bin/bash
safe_clear_workflow() {
  local feature_group=$1
  local backup_location=$2
  
  echo "🛡️ 안전한 삭제 워크플로우 시작: $feature_group"
  
  # 1. 현재 상태 확인
  echo "📊 1단계: 현재 상태 확인"
  fs list | grep "$feature_group"
  
  if [ $? -ne 0 ]; then
    echo "❌ Feature Group을 찾을 수 없습니다: $feature_group"
    return 1
  fi
  
  # 2. 스키마 백업
  echo "📋 2단계: 스키마 백업"
  schema_backup_file="/tmp/${feature_group}_schema_backup.json"
  fs schema "$feature_group" -o json > "$schema_backup_file"
  echo "✅ 스키마 백업: $schema_backup_file"
  
  # 3. 데이터 샘플 백업 (검증용)
  echo "📦 3단계: 데이터 샘플 백업"
  sample_backup_file="/tmp/${feature_group}_sample_backup.json"
  fs export "$feature_group" "$sample_backup_file" \
    --limit 1000 \
    --format json || {
    echo "⚠️ 오프라인 스토어가 없거나 데이터가 없습니다."
  }
  
  # 4. 삭제 계획 확인
  echo "🔍 4단계: 삭제 계획 확인"
  fs clear "$feature_group" --dry-run
  
  # 5. 사용자 최종 확인
  echo ""
  echo "📋 백업 파일들:"
  ls -la /tmp/${feature_group}_*
  echo ""
  read -p "위 계획으로 진행하시겠습니까? (yes/no): " confirmation
  
  if [ "$confirmation" != "yes" ]; then
    echo "❌ 사용자가 취소했습니다."
    return 1
  fi
  
  # 6. 백업과 함께 삭제 실행
  echo "🗑️ 6단계: 백업과 함께 삭제 실행"
  if [ -n "$backup_location" ]; then
    fs clear "$feature_group" \
      --backup-s3 "$backup_location" \
      --force
  else
    fs clear "$feature_group" --force
  fi
  
  if [ $? -eq 0 ]; then
    echo "✅ 안전한 삭제가 완료되었습니다."
    echo "📁 로컬 백업: /tmp/${feature_group}_*"
    if [ -n "$backup_location" ]; then
      echo "☁️ S3 백업: $backup_location"
    fi
  else
    echo "❌ 삭제 중 오류가 발생했습니다."
    return 1
  fi
}

# 사용 예시
safe_clear_workflow "test-feature-group" "s3://my-backup-bucket/safety-backups/"
```

### 2. 배치 삭제 (여러 Feature Group)
```bash
#!/bin/bash
batch_clear_feature_groups() {
  local pattern=$1
  local backup_base_path=$2
  local force_mode=${3:-false}
  
  echo "🧹 배치 삭제 시작 (패턴: $pattern)"
  
  # 매칭되는 Feature Group 찾기
  matching_fgs=($(fs list -o json | jq -r --arg pattern "$pattern" \
    '.[] | select(.FeatureGroupName | test($pattern)) | .FeatureGroupName'))
  
  if [ ${#matching_fgs[@]} -eq 0 ]; then
    echo "❌ 패턴 '$pattern'에 매칭되는 Feature Group이 없습니다."
    return 1
  fi
  
  echo "📋 삭제 대상 Feature Groups (${#matching_fgs[@]}개):"
  printf '  - %s\n' "${matching_fgs[@]}"
  
  if [ "$force_mode" != "true" ]; then
    echo ""
    read -p "위 ${#matching_fgs[@]}개 Feature Group을 모두 삭제하시겠습니까? (yes/no): " confirmation
    if [ "$confirmation" != "yes" ]; then
      echo "❌ 배치 삭제가 취소되었습니다."
      return 1
    fi
  fi
  
  # 각 Feature Group 삭제
  success_count=0
  failure_count=0
  
  for fg in "${matching_fgs[@]}"; do
    echo ""
    echo "🗑️ 삭제 중: $fg"
    
    # 백업 경로 설정
    if [ -n "$backup_base_path" ]; then
      backup_path="${backup_base_path}/${fg}"
    else
      backup_path=""
    fi
    
    # 삭제 실행
    if [ -n "$backup_path" ]; then
      fs clear "$fg" --backup-s3 "$backup_path" --force
    else
      fs clear "$fg" --force
    fi
    
    if [ $? -eq 0 ]; then
      echo "✅ $fg 삭제 완료"
      success_count=$((success_count + 1))
    else
      echo "❌ $fg 삭제 실패"
      failure_count=$((failure_count + 1))
    fi
  done
  
  echo ""
  echo "📊 배치 삭제 결과:"
  echo "  성공: $success_count"
  echo "  실패: $failure_count"
  echo "  전체: ${#matching_fgs[@]}"
}

# 사용 예시
# 테스트 환경의 모든 Feature Group 삭제
batch_clear_feature_groups "^test-.*" "s3://backups/batch-clear/" false

# 개발 환경의 임시 Feature Group 삭제
batch_clear_feature_groups "^dev-temp-.*" "" true
```

### 3. 조건부 삭제 (데이터 나이 기반)
```bash
#!/bin/bash
conditional_clear_by_age() {
  local feature_group=$1
  local max_age_days=$2
  local backup_path=$3
  
  echo "📅 조건부 삭제: $max_age_days일 이상된 데이터만"
  
  # Feature Group 정보 확인
  fg_info=$(fs list -o json | jq --arg fg "$feature_group" \
    '.[] | select(.FeatureGroupName == $fg)')
  
  if [ -z "$fg_info" ]; then
    echo "❌ Feature Group을 찾을 수 없습니다: $feature_group"
    return 1
  fi
  
  # 생성 날짜 확인
  creation_time=$(echo "$fg_info" | jq -r '.CreationTime')
  creation_date=$(date -d "$creation_time" +%s)
  current_date=$(date +%s)
  age_days=$(( (current_date - creation_date) / 86400 ))
  
  echo "📊 Feature Group 정보:"
  echo "  생성일: $creation_time"
  echo "  현재 나이: $age_days일"
  echo "  삭제 기준: $max_age_days일"
  
  if [ $age_days -lt $max_age_days ]; then
    echo "ℹ️ Feature Group이 아직 $max_age_days일이 지나지 않았습니다."
    echo "   실제 나이: $age_days일"
    return 0
  fi
  
  echo "🗑️ 삭제 조건을 만족합니다. 삭제를 진행합니다..."
  
  # 백업과 함께 삭제
  if [ -n "$backup_path" ]; then
    fs clear "$feature_group" \
      --backup-s3 "$backup_path/${feature_group}_$(date +%Y%m%d)" \
      --force
  else
    fs clear "$feature_group" --force
  fi
  
  if [ $? -eq 0 ]; then
    echo "✅ 조건부 삭제 완료: $feature_group"
  else
    echo "❌ 조건부 삭제 실패: $feature_group"
    return 1
  fi
}

# 30일 이상된 Feature Group들을 찾아서 삭제
for fg in $(fs list -o json | jq -r '.[].FeatureGroupName'); do
  conditional_clear_by_age "$fg" 30 "s3://archive-bucket/old-feature-groups/"
done
```

### 4. 스토리지 비용 기반 삭제
```bash
#!/bin/bash
cost_based_clear() {
  local feature_group=$1
  local max_cost_threshold_gb=${2:-100}  # 100GB 기본값
  
  echo "💰 비용 기반 삭제 분석: $feature_group"
  
  # 스토리지 사용량 분석 (analyze 명령 사용)
  analysis_file="/tmp/${feature_group}_analysis.json"
  fs analyze "$feature_group" \
    --export "$analysis_file" \
    --output-format json
  
  if [ $? -ne 0 ] || [ ! -f "$analysis_file" ]; then
    echo "❌ 스토리지 분석에 실패했습니다."
    return 1
  fi
  
  # 총 크기 계산 (GB 단위)
  total_size_gb=$(python3 << EOF
import json

try:
    with open('$analysis_file', 'r') as f:
        data = json.load(f)
    
    total_bytes = data.get('total_size_bytes', 0)
    total_gb = total_bytes / (1024**3)
    print(f"{total_gb:.2f}")
except:
    print("0")
EOF
)
  
  echo "📊 스토리지 분석 결과:"
  echo "  총 크기: ${total_size_gb}GB"
  echo "  임계값: ${max_cost_threshold_gb}GB"
  
  # 비용 임계값 비교
  if (( $(echo "$total_size_gb > $max_cost_threshold_gb" | bc -l) )); then
    echo "💸 스토리지 비용이 임계값을 초과했습니다!"
    echo "🗑️ 비용 절약을 위해 삭제를 권장합니다."
    
    # 사용자 확인
    read -p "삭제를 진행하시겠습니까? (yes/no): " confirmation
    if [ "$confirmation" == "yes" ]; then
      # 비용 절약을 위해 offline-only 삭제 (online은 상대적으로 저렴)
      fs clear "$feature_group" \
        --offline-only \
        --backup-s3 "s3://cost-archive/${feature_group}_$(date +%Y%m%d)" \
        --force
      
      if [ $? -eq 0 ]; then
        echo "✅ 오프라인 스토어 삭제로 비용을 절약했습니다."
        echo "💡 필요시 백업에서 데이터를 복원할 수 있습니다."
      fi
    else
      echo "❌ 삭제가 취소되었습니다."
    fi
  else
    echo "✅ 스토리지 비용이 임계값 내에 있습니다."
  fi
  
  # 임시 파일 정리
  rm -f "$analysis_file"
}

# 비용 기반 삭제 실행
cost_based_clear "large-analytics-features" 500  # 500GB 임계값
```

### 5. 데이터 품질 기반 삭제
```bash
#!/bin/bash
quality_based_clear() {
  local feature_group=$1
  local min_quality_threshold=${2:-80}  # 80% 품질 기준
  
  echo "🔍 데이터 품질 기반 삭제 분석: $feature_group"
  
  # 데이터 품질 검사
  quality_report="/tmp/${feature_group}_quality.json"
  
  python3 << EOF > "$quality_report"
import json
import sys

# 간단한 데이터 품질 검사 시뮬레이션
# 실제로는 export를 통해 데이터를 샘플링하고 품질을 분석해야 함

try:
    # 샘플 데이터 가져오기 (실제 구현에서는 fs export 사용)
    quality_metrics = {
        "completeness": 85.5,  # 완성도 (누락값 없는 레코드 비율)
        "validity": 92.3,      # 유효성 (올바른 형식의 데이터 비율)
        "consistency": 78.9,   # 일관성 (중복 제거된 데이터 비율)
        "timeliness": 45.2,    # 시의성 (최근 데이터 비율)
        "accuracy": 88.7       # 정확성 (예상 범위 내 데이터 비율)
    }
    
    # 종합 품질 점수 계산
    overall_quality = sum(quality_metrics.values()) / len(quality_metrics)
    quality_metrics["overall_score"] = overall_quality
    
    print(json.dumps(quality_metrics, indent=2))

except Exception as e:
    print(f"{{\"error\": \"{e}\", \"overall_score\": 0}}", file=sys.stderr)
    sys.exit(1)
EOF
  
  if [ $? -ne 0 ]; then
    echo "❌ 품질 분석에 실패했습니다."
    return 1
  fi
  
  # 품질 점수 추출
  overall_score=$(jq -r '.overall_score' "$quality_report")
  
  echo "📊 데이터 품질 분석 결과:"
  jq -r 'to_entries[] | "  \(.key): \(.value)%"' "$quality_report" | head -5
  echo "  전체 점수: ${overall_score}%"
  echo "  품질 기준: ${min_quality_threshold}%"
  
  # 품질 기준 비교
  if (( $(echo "$overall_score < $min_quality_threshold" | bc -l) )); then
    echo "📉 데이터 품질이 기준 이하입니다!"
    echo "🧹 품질 향상을 위해 기존 데이터 삭제를 권장합니다."
    
    # 품질 세부 정보 표시
    echo ""
    echo "📋 품질 문제 분석:"
    jq -r 'to_entries[] | select(.value < 80 and .key != "overall_score") | 
           "  ⚠️ \(.key): \(.value)% (기준: 80%)"' "$quality_report"
    
    read -p "품질 개선을 위해 삭제하시겠습니까? (yes/no): " confirmation
    if [ "$confirmation" == "yes" ]; then
      # 품질 기준 미달 데이터 삭제
      fs clear "$feature_group" \
        --backup-s3 "s3://quality-archive/poor-quality/${feature_group}_$(date +%Y%m%d)" \
        --force
      
      if [ $? -eq 0 ]; then
        echo "✅ 품질 기준 미달 데이터가 삭제되었습니다."
        echo "💡 새로운 고품질 데이터로 Feature Group을 다시 구성하세요."
      fi
    else
      echo "❌ 삭제가 취소되었습니다."
    fi
  else
    echo "✅ 데이터 품질이 기준을 만족합니다."
  fi
  
  # 임시 파일 정리
  rm -f "$quality_report"
}

quality_based_clear "customer-behavior-features" 75
```

### 6. 복구 가능한 삭제 시스템
```bash
#!/bin/bash
setup_recoverable_clear() {
  local feature_group=$1
  local recovery_retention_days=${2:-30}
  
  echo "🛡️ 복구 가능한 삭제 시스템 설정"
  
  # 복구 메타데이터 저장
  recovery_base="/tmp/feature_group_recovery"
  mkdir -p "$recovery_base"
  
  recovery_metadata="$recovery_base/${feature_group}_recovery.json"
  
  # 삭제 전 상태 기록
  cat << EOF > "$recovery_metadata"
{
  "feature_group_name": "$feature_group",
  "deletion_timestamp": "$(date -Iseconds)",
  "recovery_expires": "$(date -d "+$recovery_retention_days days" -Iseconds)",
  "schema_backup": "${feature_group}_schema.json",
  "data_backup": "s3://recovery-vault/${feature_group}/$(date +%Y%m%d_%H%M%S)/",
  "deletion_reason": "Manual clear command",
  "can_recover": true
}
EOF
  
  # 스키마 백업
  fs schema "$feature_group" -o json > "$recovery_base/${feature_group}_schema.json"
  
  # 복구 스크립트 생성
  recovery_script="$recovery_base/recover_${feature_group}.sh"
  cat << EOF > "$recovery_script"
#!/bin/bash
# 자동 생성된 복구 스크립트: $feature_group
# 생성일: $(date)

echo "🔄 Feature Group 복구 시작: $feature_group"

# 복구 가능 여부 확인
recovery_metadata="$recovery_metadata"
if [ ! -f "\$recovery_metadata" ]; then
  echo "❌ 복구 메타데이터를 찾을 수 없습니다."
  exit 1
fi

# 복구 만료 확인
expires=\$(jq -r '.recovery_expires' "\$recovery_metadata")
if [ "\$(date -Iseconds)" > "\$expires" ]; then
  echo "❌ 복구 기간이 만료되었습니다 (만료: \$expires)"
  exit 1
fi

echo "✅ 복구 가능 상태 확인됨"

# Feature Group 재생성
echo "🏗️ Feature Group 재생성 중..."
schema_file="\$(jq -r '.schema_backup' \$recovery_metadata)"
backup_location="\$(jq -r '.data_backup' \$recovery_metadata)"

# 스키마 파일 경로 수정
full_schema_path="$recovery_base/\$schema_file"

# Feature Group 생성 (역할 ARN은 환경에 맞게 수정 필요)
# fs create $feature_group \\
#   --schema-file "\$full_schema_path" \\
#   --role-arn "arn:aws:iam::ACCOUNT:role/SageMakerRole" \\
#   --s3-uri "s3://feature-store-bucket/$feature_group/"

echo "⚠️ Feature Group 생성 명령어를 수동으로 실행하세요:"
echo "fs create $feature_group --schema-file '\$full_schema_path' --role-arn 'YOUR_ROLE_ARN' --s3-uri 'YOUR_S3_URI'"

echo "📦 데이터 복구는 백업 위치에서 수동으로 수행하세요:"
echo "백업 위치: \$backup_location"

echo "✅ 복구 안내가 완료되었습니다."
EOF
  
  chmod +x "$recovery_script"
  
  echo "📋 복구 가능한 삭제 준비 완료:"
  echo "  메타데이터: $recovery_metadata"
  echo "  복구 스크립트: $recovery_script"
  echo "  복구 만료일: $(date -d "+$recovery_retention_days days")"
  
  # 실제 삭제 실행
  echo ""
  read -p "복구 시스템이 준비되었습니다. 삭제를 진행하시겠습니까? (yes/no): " confirmation
  if [ "$confirmation" == "yes" ]; then
    fs clear "$feature_group" \
      --backup-s3 "s3://recovery-vault/${feature_group}/$(date +%Y%m%d_%H%M%S)/" \
      --force
    
    if [ $? -eq 0 ]; then
      echo "✅ 복구 가능한 삭제가 완료되었습니다."
      echo "🔄 복구 필요 시: bash $recovery_script"
    else
      echo "❌ 삭제에 실패했습니다. 복구 시스템은 그대로 유지됩니다."
    fi
  else
    echo "❌ 삭제가 취소되었습니다."
  fi
}

setup_recoverable_clear "important-feature-group" 30
```

## 성능 및 제한사항

### 1. Online Store 삭제 제한사항
- API 호출 제한으로 인해 대량의 레코드 삭제 시 시간이 소요됨
- 레코드별 개별 삭제 API 호출 필요
- 삭제 속도: 초당 약 100-500개 레코드 (환경에 따라 차이)

### 2. Offline Store 삭제 특징
- S3 객체 삭제로 상대적으로 빠름
- 배치 삭제 지원 (1000개씩)
- 파티션별 삭제 가능

### 3. 삭제 성능 최적화
```bash
#!/bin/bash
optimize_clear_performance() {
  local feature_group=$1
  
  echo "⚡ 삭제 성능 최적화 분석: $feature_group"
  
  # Feature Group 크기 추정
  fg_info=$(fs list -o json | jq --arg fg "$feature_group" \
    '.[] | select(.FeatureGroupName == $fg)')
  
  # 추정 레코드 수 (실제로는 더 정확한 방법 필요)
  estimated_records=100000  # 예시값
  
  # 삭제 시간 예측
  online_delete_time=$((estimated_records / 200))  # 초당 200개 가정
  offline_delete_time=30  # S3 삭제는 보통 30초 내외
  
  echo "📊 성능 예측:"
  echo "  예상 레코드 수: $estimated_records"
  echo "  Online 삭제 예상 시간: $online_delete_time초 ($(($online_delete_time / 60))분)"
  echo "  Offline 삭제 예상 시간: $offline_delete_time초"
  echo "  총 예상 시간: $((online_delete_time + offline_delete_time))초"
  
  # 최적화 제안
  echo ""
  echo "🚀 최적화 제안:"
  if [ $estimated_records -gt 50000 ]; then
    echo "  - 대량 데이터: offline-only 삭제 우선 고려"
    echo "  - 백그라운드에서 실행 권장"
    echo "  - 삭제 중 중단 방지를 위한 screen/tmux 사용"
  fi
  
  echo "  - 삭제 전 불필요한 AWS 리소스 사용 최소화"
  echo "  - 다른 API 사용을 최소화하여 rate limit 여유 확보"
}

optimize_clear_performance "large-dataset-features"
```

## 모니터링 및 로깅

### 1. 삭제 진행률 모니터링
```bash
#!/bin/bash
monitor_clear_progress() {
  local feature_group=$1
  local log_file="/tmp/clear_progress_${feature_group}.log"
  
  echo "📊 삭제 진행률 모니터링 시작: $feature_group"
  
  # 백그라운드에서 삭제 실행
  (
    fs clear "$feature_group" --force 2>&1 | tee "$log_file"
  ) &
  
  clear_pid=$!
  
  # 진행률 모니터링
  while kill -0 $clear_pid 2>/dev/null; do
    if [ -f "$log_file" ]; then
      # 로그에서 진행률 정보 추출
      online_progress=$(tail -20 "$log_file" | grep -o "온라인 레코드 삭제 중.*%" | tail -1)
      offline_progress=$(tail -20 "$log_file" | grep -o "S3 객체 삭제 중.*%" | tail -1)
      
      clear
      echo "🗑️ Feature Group 삭제 진행률: $feature_group"
      echo "$(date)"
      echo ""
      
      if [ -n "$online_progress" ]; then
        echo "🌐 Online Store: $online_progress"
      fi
      
      if [ -n "$offline_progress" ]; then
        echo "💾 Offline Store: $offline_progress"
      fi
      
      # 최근 로그 라인 표시
      echo ""
      echo "📝 최근 로그:"
      tail -3 "$log_file" | sed 's/^/  /'
    fi
    
    sleep 5
  done
  
  # 완료 상태 확인
  wait $clear_pid
  if [ $? -eq 0 ]; then
    echo "✅ 삭제가 완료되었습니다: $feature_group"
  else
    echo "❌ 삭제 중 오류가 발생했습니다: $feature_group"
  fi
  
  echo "📋 전체 로그: $log_file"
}

monitor_clear_progress "monitoring-test-fg"
```

## 오류 처리 및 문제 해결

### 1. 일반적인 오류 시나리오
```bash
# 권한 오류
❌ AWS API 오류: AccessDenied
# 해결: IAM 권한 확인 필요
# - sagemaker:DescribeFeatureGroup
# - sagemaker-featurestore-runtime:DeleteRecord
# - s3:DeleteObject, s3:ListBucket

# Athena 쿼리 오류
❌ Athena를 통한 레코드 ID 조회 실패: Query timeout
# 해결: 더 작은 배치로 나누어 처리

# S3 접근 오류
❌ 오프라인 스토어 삭제 실패: NoSuchBucket
# 해결: S3 버킷 존재 및 권한 확인
```

### 2. 복구 시나리오
```bash
#!/bin/bash
emergency_recovery() {
  local feature_group=$1
  
  echo "🚨 긴급 복구 프로세스: $feature_group"
  
  # 백업 위치 확인
  backup_locations=(
    "s3://recovery-vault/${feature_group}/"
    "s3://backup-bucket/feature-groups/${feature_group}/"
    "/tmp/${feature_group}_*"
  )
  
  echo "🔍 백업 위치 검색 중..."
  for location in "${backup_locations[@]}"; do
    if [[ "$location" == s3://* ]]; then
      # S3 백업 확인
      aws s3 ls "$location" && {
        echo "✅ S3 백업 발견: $location"
        echo "📋 복구 옵션:"
        echo "  1. 새 Feature Group 생성"
        echo "  2. 백업 데이터 복원"
        echo "  aws s3 sync $location /local/recovery/path/"
        return 0
      }
    else
      # 로컬 백업 확인
      ls $location 2>/dev/null && {
        echo "✅ 로컬 백업 발견: $location"
        return 0
      }
    fi
  done
  
  echo "❌ 사용 가능한 백업을 찾을 수 없습니다."
  echo "💡 다음 위치를 수동으로 확인하세요:"
  printf '  - %s\n' "${backup_locations[@]}"
}

emergency_recovery "accidentally-deleted-fg"
```

## 모범 사례

1. **백업 우선**: 중요한 데이터는 반드시 백업 후 삭제
2. **단계적 접근**: Online → Offline 순서로 단계적 삭제
3. **dry-run 활용**: 실제 삭제 전 계획 확인
4. **모니터링**: 대용량 삭제 시 진행률 모니터링
5. **복구 계획**: 삭제 전 복구 방법 준비
6. **권한 최소화**: 필요한 최소 권한만 부여

## 관련 명령어
- `fs create`: 새 Feature Group 생성 (복구용)
- `fs export`: 삭제 전 데이터 백업
- `fs analyze`: 스토리지 사용량 확인
- `fs list`: Feature Group 상태 확인