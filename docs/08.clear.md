# 8. clear - í”¼ì²˜ ê·¸ë£¹ ë°ì´í„° ì‚­ì œ

## ê°œìš”
Feature Groupì˜ Online Store ë°/ë˜ëŠ” Offline Storeì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. ì„ íƒì ìœ¼ë¡œ ë°±ì—…ì„ ìƒì„±í•˜ê³ , ì¡°ê±´ë¶€ ì‚­ì œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

âš ï¸ **ê²½ê³ **: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ë°±ì—…í•˜ì„¸ìš”.

## ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
fs clear FEATURE_GROUP_NAME [OPTIONS]
```

## í•„ìˆ˜ ì¸ì
- `FEATURE_GROUP_NAME`: ë°ì´í„°ë¥¼ ì‚­ì œí•  Feature Group ì´ë¦„

## ì˜µì…˜
- `--online-only`: Online Storeë§Œ ì‚­ì œ
- `--offline-only`: Offline Storeë§Œ ì‚­ì œ 
- `--force`: í™•ì¸ ì—†ì´ ê°•ì œ ì‚­ì œ
- `--backup-s3 PATH`: ì‚­ì œ ì „ S3ì— ë°±ì—…
- `--dry-run`: ì‹¤ì œ ì‚­ì œ ì—†ì´ ê³„íšë§Œ í™•ì¸

## ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ì „ì²´ ì‚­ì œ (í™•ì¸ í¬í•¨)
```bash
fs clear customer-profile
```

**ì‹¤í–‰ ê³¼ì •:**
1. ì‚­ì œ ê³„íš í‘œì‹œ
2. ì‚¬ìš©ì í™•ì¸ ìš”ì²­
3. Online/Offline Store ëª¨ë‘ ì‚­ì œ

### 2. Online Storeë§Œ ì‚­ì œ
```bash
fs clear customer-profile --online-only
```

### 3. Offline Storeë§Œ ì‚­ì œ
```bash
fs clear customer-profile --offline-only
```

### 4. ë°±ì—…ê³¼ í•¨ê»˜ ì‚­ì œ
```bash
fs clear customer-profile \
  --backup-s3 s3://my-backup-bucket/feature-backups/ \
  --force
```

### 5. ê³„íš í™•ì¸ (ì‹¤ì œ ì‚­ì œ ì•ˆí•¨)
```bash
fs clear customer-profile --dry-run
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“‹ ì‚­ì œ ê³„íš: 'customer-profile'
==================================================
â€¢ ì˜¨ë¼ì¸ ìŠ¤í† ì–´: ëª¨ë“  ë ˆì½”ë“œ ì‚­ì œ
â€¢ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´: S3 ë°ì´í„° ì‚­ì œ

ğŸ” Dry-run ëª¨ë“œ: ì‹¤ì œ ì‚­ì œëŠ” ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
```

## ê³ ê¸‰ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. ì•ˆì „í•œ ì‚­ì œ ì›Œí¬í”Œë¡œìš°
```bash
#!/bin/bash
safe_clear_workflow() {
  local feature_group=$1
  local backup_location=$2
  
  echo "ğŸ›¡ï¸ ì•ˆì „í•œ ì‚­ì œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: $feature_group"
  
  # 1. í˜„ì¬ ìƒíƒœ í™•ì¸
  echo "ğŸ“Š 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸"
  fs list | grep "$feature_group"
  
  if [ $? -ne 0 ]; then
    echo "âŒ Feature Groupì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $feature_group"
    return 1
  fi
  
  # 2. ìŠ¤í‚¤ë§ˆ ë°±ì—…
  echo "ğŸ“‹ 2ë‹¨ê³„: ìŠ¤í‚¤ë§ˆ ë°±ì—…"
  schema_backup_file="/tmp/${feature_group}_schema_backup.json"
  fs schema "$feature_group" -o json > "$schema_backup_file"
  echo "âœ… ìŠ¤í‚¤ë§ˆ ë°±ì—…: $schema_backup_file"
  
  # 3. ë°ì´í„° ìƒ˜í”Œ ë°±ì—… (ê²€ì¦ìš©)
  echo "ğŸ“¦ 3ë‹¨ê³„: ë°ì´í„° ìƒ˜í”Œ ë°±ì—…"
  sample_backup_file="/tmp/${feature_group}_sample_backup.json"
  fs export "$feature_group" "$sample_backup_file" \
    --limit 1000 \
    --format json || {
    echo "âš ï¸ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ê°€ ì—†ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
  }
  
  # 4. ì‚­ì œ ê³„íš í™•ì¸
  echo "ğŸ” 4ë‹¨ê³„: ì‚­ì œ ê³„íš í™•ì¸"
  fs clear "$feature_group" --dry-run
  
  # 5. ì‚¬ìš©ì ìµœì¢… í™•ì¸
  echo ""
  echo "ğŸ“‹ ë°±ì—… íŒŒì¼ë“¤:"
  ls -la /tmp/${feature_group}_*
  echo ""
  read -p "ìœ„ ê³„íšìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
  
  if [ "$confirmation" != "yes" ]; then
    echo "âŒ ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # 6. ë°±ì—…ê³¼ í•¨ê»˜ ì‚­ì œ ì‹¤í–‰
  echo "ğŸ—‘ï¸ 6ë‹¨ê³„: ë°±ì—…ê³¼ í•¨ê»˜ ì‚­ì œ ì‹¤í–‰"
  if [ -n "$backup_location" ]; then
    fs clear "$feature_group" \
      --backup-s3 "$backup_location" \
      --force
  else
    fs clear "$feature_group" --force
  fi
  
  if [ $? -eq 0 ]; then
    echo "âœ… ì•ˆì „í•œ ì‚­ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    echo "ğŸ“ ë¡œì»¬ ë°±ì—…: /tmp/${feature_group}_*"
    if [ -n "$backup_location" ]; then
      echo "â˜ï¸ S3 ë°±ì—…: $backup_location"
    fi
  else
    echo "âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    return 1
  fi
}

# ì‚¬ìš© ì˜ˆì‹œ
safe_clear_workflow "test-feature-group" "s3://my-backup-bucket/safety-backups/"
```

### 2. ë°°ì¹˜ ì‚­ì œ (ì—¬ëŸ¬ Feature Group)
```bash
#!/bin/bash
batch_clear_feature_groups() {
  local pattern=$1
  local backup_base_path=$2
  local force_mode=${3:-false}
  
  echo "ğŸ§¹ ë°°ì¹˜ ì‚­ì œ ì‹œì‘ (íŒ¨í„´: $pattern)"
  
  # ë§¤ì¹­ë˜ëŠ” Feature Group ì°¾ê¸°
  matching_fgs=($(fs list -o json | jq -r --arg pattern "$pattern" \
    '.[] | select(.FeatureGroupName | test($pattern)) | .FeatureGroupName'))
  
  if [ ${#matching_fgs[@]} -eq 0 ]; then
    echo "âŒ íŒ¨í„´ '$pattern'ì— ë§¤ì¹­ë˜ëŠ” Feature Groupì´ ì—†ìŠµë‹ˆë‹¤."
    return 1
  fi
  
  echo "ğŸ“‹ ì‚­ì œ ëŒ€ìƒ Feature Groups (${#matching_fgs[@]}ê°œ):"
  printf '  - %s\n' "${matching_fgs[@]}"
  
  if [ "$force_mode" != "true" ]; then
    echo ""
    read -p "ìœ„ ${#matching_fgs[@]}ê°œ Feature Groupì„ ëª¨ë‘ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
    if [ "$confirmation" != "yes" ]; then
      echo "âŒ ë°°ì¹˜ ì‚­ì œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
      return 1
    fi
  fi
  
  # ê° Feature Group ì‚­ì œ
  success_count=0
  failure_count=0
  
  for fg in "${matching_fgs[@]}"; do
    echo ""
    echo "ğŸ—‘ï¸ ì‚­ì œ ì¤‘: $fg"
    
    # ë°±ì—… ê²½ë¡œ ì„¤ì •
    if [ -n "$backup_base_path" ]; then
      backup_path="${backup_base_path}/${fg}"
    else
      backup_path=""
    fi
    
    # ì‚­ì œ ì‹¤í–‰
    if [ -n "$backup_path" ]; then
      fs clear "$fg" --backup-s3 "$backup_path" --force
    else
      fs clear "$fg" --force
    fi
    
    if [ $? -eq 0 ]; then
      echo "âœ… $fg ì‚­ì œ ì™„ë£Œ"
      success_count=$((success_count + 1))
    else
      echo "âŒ $fg ì‚­ì œ ì‹¤íŒ¨"
      failure_count=$((failure_count + 1))
    fi
  done
  
  echo ""
  echo "ğŸ“Š ë°°ì¹˜ ì‚­ì œ ê²°ê³¼:"
  echo "  ì„±ê³µ: $success_count"
  echo "  ì‹¤íŒ¨: $failure_count"
  echo "  ì „ì²´: ${#matching_fgs[@]}"
}

# ì‚¬ìš© ì˜ˆì‹œ
# í…ŒìŠ¤íŠ¸ í™˜ê²½ì˜ ëª¨ë“  Feature Group ì‚­ì œ
batch_clear_feature_groups "^test-.*" "s3://backups/batch-clear/" false

# ê°œë°œ í™˜ê²½ì˜ ì„ì‹œ Feature Group ì‚­ì œ
batch_clear_feature_groups "^dev-temp-.*" "" true
```

### 3. ì¡°ê±´ë¶€ ì‚­ì œ (ë°ì´í„° ë‚˜ì´ ê¸°ë°˜)
```bash
#!/bin/bash
conditional_clear_by_age() {
  local feature_group=$1
  local max_age_days=$2
  local backup_path=$3
  
  echo "ğŸ“… ì¡°ê±´ë¶€ ì‚­ì œ: $max_age_daysì¼ ì´ìƒëœ ë°ì´í„°ë§Œ"
  
  # Feature Group ì •ë³´ í™•ì¸
  fg_info=$(fs list -o json | jq --arg fg "$feature_group" \
    '.[] | select(.FeatureGroupName == $fg)')
  
  if [ -z "$fg_info" ]; then
    echo "âŒ Feature Groupì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $feature_group"
    return 1
  fi
  
  # ìƒì„± ë‚ ì§œ í™•ì¸
  creation_time=$(echo "$fg_info" | jq -r '.CreationTime')
  creation_date=$(date -d "$creation_time" +%s)
  current_date=$(date +%s)
  age_days=$(( (current_date - creation_date) / 86400 ))
  
  echo "ğŸ“Š Feature Group ì •ë³´:"
  echo "  ìƒì„±ì¼: $creation_time"
  echo "  í˜„ì¬ ë‚˜ì´: $age_daysì¼"
  echo "  ì‚­ì œ ê¸°ì¤€: $max_age_daysì¼"
  
  if [ $age_days -lt $max_age_days ]; then
    echo "â„¹ï¸ Feature Groupì´ ì•„ì§ $max_age_daysì¼ì´ ì§€ë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "   ì‹¤ì œ ë‚˜ì´: $age_daysì¼"
    return 0
  fi
  
  echo "ğŸ—‘ï¸ ì‚­ì œ ì¡°ê±´ì„ ë§Œì¡±í•©ë‹ˆë‹¤. ì‚­ì œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤..."
  
  # ë°±ì—…ê³¼ í•¨ê»˜ ì‚­ì œ
  if [ -n "$backup_path" ]; then
    fs clear "$feature_group" \
      --backup-s3 "$backup_path/${feature_group}_$(date +%Y%m%d)" \
      --force
  else
    fs clear "$feature_group" --force
  fi
  
  if [ $? -eq 0 ]; then
    echo "âœ… ì¡°ê±´ë¶€ ì‚­ì œ ì™„ë£Œ: $feature_group"
  else
    echo "âŒ ì¡°ê±´ë¶€ ì‚­ì œ ì‹¤íŒ¨: $feature_group"
    return 1
  fi
}

# 30ì¼ ì´ìƒëœ Feature Groupë“¤ì„ ì°¾ì•„ì„œ ì‚­ì œ
for fg in $(fs list -o json | jq -r '.[].FeatureGroupName'); do
  conditional_clear_by_age "$fg" 30 "s3://archive-bucket/old-feature-groups/"
done
```

### 4. ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ê¸°ë°˜ ì‚­ì œ
```bash
#!/bin/bash
cost_based_clear() {
  local feature_group=$1
  local max_cost_threshold_gb=${2:-100}  # 100GB ê¸°ë³¸ê°’
  
  echo "ğŸ’° ë¹„ìš© ê¸°ë°˜ ì‚­ì œ ë¶„ì„: $feature_group"
  
  # ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ë¶„ì„ (analyze ëª…ë ¹ ì‚¬ìš©)
  analysis_file="/tmp/${feature_group}_analysis.json"
  fs analyze "$feature_group" \
    --export "$analysis_file" \
    --output-format json
  
  if [ $? -ne 0 ] || [ ! -f "$analysis_file" ]; then
    echo "âŒ ìŠ¤í† ë¦¬ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # ì´ í¬ê¸° ê³„ì‚° (GB ë‹¨ìœ„)
  total_size_gb=$(python3 << EOF
import json

try:
    with open('$analysis_file', 'r') as f:
        data = json.load(f)
    
    total_bytes = data.get('total_size_bytes', 0)
    total_gb = total_bytes / (1024**3)
    print(f"{total_gb:.2f}")
except:
    print("0")
EOF
)
  
  echo "ğŸ“Š ìŠ¤í† ë¦¬ì§€ ë¶„ì„ ê²°ê³¼:"
  echo "  ì´ í¬ê¸°: ${total_size_gb}GB"
  echo "  ì„ê³„ê°’: ${max_cost_threshold_gb}GB"
  
  # ë¹„ìš© ì„ê³„ê°’ ë¹„êµ
  if (( $(echo "$total_size_gb > $max_cost_threshold_gb" | bc -l) )); then
    echo "ğŸ’¸ ìŠ¤í† ë¦¬ì§€ ë¹„ìš©ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤!"
    echo "ğŸ—‘ï¸ ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ ì‚­ì œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    # ì‚¬ìš©ì í™•ì¸
    read -p "ì‚­ì œë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
    if [ "$confirmation" == "yes" ]; then
      # ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ offline-only ì‚­ì œ (onlineì€ ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´)
      fs clear "$feature_group" \
        --offline-only \
        --backup-s3 "s3://cost-archive/${feature_group}_$(date +%Y%m%d)" \
        --force
      
      if [ $? -eq 0 ]; then
        echo "âœ… ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ì‚­ì œë¡œ ë¹„ìš©ì„ ì ˆì•½í–ˆìŠµë‹ˆë‹¤."
        echo "ğŸ’¡ í•„ìš”ì‹œ ë°±ì—…ì—ì„œ ë°ì´í„°ë¥¼ ë³µì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      fi
    else
      echo "âŒ ì‚­ì œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
    fi
  else
    echo "âœ… ìŠ¤í† ë¦¬ì§€ ë¹„ìš©ì´ ì„ê³„ê°’ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -f "$analysis_file"
}

# ë¹„ìš© ê¸°ë°˜ ì‚­ì œ ì‹¤í–‰
cost_based_clear "large-analytics-features" 500  # 500GB ì„ê³„ê°’
```

### 5. ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì‚­ì œ
```bash
#!/bin/bash
quality_based_clear() {
  local feature_group=$1
  local min_quality_threshold=${2:-80}  # 80% í’ˆì§ˆ ê¸°ì¤€
  
  echo "ğŸ” ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì‚­ì œ ë¶„ì„: $feature_group"
  
  # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
  quality_report="/tmp/${feature_group}_quality.json"
  
  python3 << EOF > "$quality_report"
import json
import sys

# ê°„ë‹¨í•œ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œë®¬ë ˆì´ì…˜
# ì‹¤ì œë¡œëŠ” exportë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ê³  í’ˆì§ˆì„ ë¶„ì„í•´ì•¼ í•¨

try:
    # ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” fs export ì‚¬ìš©)
    quality_metrics = {
        "completeness": 85.5,  # ì™„ì„±ë„ (ëˆ„ë½ê°’ ì—†ëŠ” ë ˆì½”ë“œ ë¹„ìœ¨)
        "validity": 92.3,      # ìœ íš¨ì„± (ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ ë°ì´í„° ë¹„ìœ¨)
        "consistency": 78.9,   # ì¼ê´€ì„± (ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨)
        "timeliness": 45.2,    # ì‹œì˜ì„± (ìµœê·¼ ë°ì´í„° ë¹„ìœ¨)
        "accuracy": 88.7       # ì •í™•ì„± (ì˜ˆìƒ ë²”ìœ„ ë‚´ ë°ì´í„° ë¹„ìœ¨)
    }
    
    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    overall_quality = sum(quality_metrics.values()) / len(quality_metrics)
    quality_metrics["overall_score"] = overall_quality
    
    print(json.dumps(quality_metrics, indent=2))

except Exception as e:
    print(f"{{\"error\": \"{e}\", \"overall_score\": 0}}", file=sys.stderr)
    sys.exit(1)
EOF
  
  if [ $? -ne 0 ]; then
    echo "âŒ í’ˆì§ˆ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    return 1
  fi
  
  # í’ˆì§ˆ ì ìˆ˜ ì¶”ì¶œ
  overall_score=$(jq -r '.overall_score' "$quality_report")
  
  echo "ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ê²°ê³¼:"
  jq -r 'to_entries[] | "  \(.key): \(.value)%"' "$quality_report" | head -5
  echo "  ì „ì²´ ì ìˆ˜: ${overall_score}%"
  echo "  í’ˆì§ˆ ê¸°ì¤€: ${min_quality_threshold}%"
  
  # í’ˆì§ˆ ê¸°ì¤€ ë¹„êµ
  if (( $(echo "$overall_score < $min_quality_threshold" | bc -l) )); then
    echo "ğŸ“‰ ë°ì´í„° í’ˆì§ˆì´ ê¸°ì¤€ ì´í•˜ì…ë‹ˆë‹¤!"
    echo "ğŸ§¹ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    # í’ˆì§ˆ ì„¸ë¶€ ì •ë³´ í‘œì‹œ
    echo ""
    echo "ğŸ“‹ í’ˆì§ˆ ë¬¸ì œ ë¶„ì„:"
    jq -r 'to_entries[] | select(.value < 80 and .key != "overall_score") | 
           "  âš ï¸ \(.key): \(.value)% (ê¸°ì¤€: 80%)"' "$quality_report"
    
    read -p "í’ˆì§ˆ ê°œì„ ì„ ìœ„í•´ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
    if [ "$confirmation" == "yes" ]; then
      # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ë°ì´í„° ì‚­ì œ
      fs clear "$feature_group" \
        --backup-s3 "s3://quality-archive/poor-quality/${feature_group}_$(date +%Y%m%d)" \
        --force
      
      if [ $? -eq 0 ]; then
        echo "âœ… í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
        echo "ğŸ’¡ ìƒˆë¡œìš´ ê³ í’ˆì§ˆ ë°ì´í„°ë¡œ Feature Groupì„ ë‹¤ì‹œ êµ¬ì„±í•˜ì„¸ìš”."
      fi
    else
      echo "âŒ ì‚­ì œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
    fi
  else
    echo "âœ… ë°ì´í„° í’ˆì§ˆì´ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤."
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -f "$quality_report"
}

quality_based_clear "customer-behavior-features" 75
```

### 6. ë³µêµ¬ ê°€ëŠ¥í•œ ì‚­ì œ ì‹œìŠ¤í…œ
```bash
#!/bin/bash
setup_recoverable_clear() {
  local feature_group=$1
  local recovery_retention_days=${2:-30}
  
  echo "ğŸ›¡ï¸ ë³µêµ¬ ê°€ëŠ¥í•œ ì‚­ì œ ì‹œìŠ¤í…œ ì„¤ì •"
  
  # ë³µêµ¬ ë©”íƒ€ë°ì´í„° ì €ì¥
  recovery_base="/tmp/feature_group_recovery"
  mkdir -p "$recovery_base"
  
  recovery_metadata="$recovery_base/${feature_group}_recovery.json"
  
  # ì‚­ì œ ì „ ìƒíƒœ ê¸°ë¡
  cat << EOF > "$recovery_metadata"
{
  "feature_group_name": "$feature_group",
  "deletion_timestamp": "$(date -Iseconds)",
  "recovery_expires": "$(date -d "+$recovery_retention_days days" -Iseconds)",
  "schema_backup": "${feature_group}_schema.json",
  "data_backup": "s3://recovery-vault/${feature_group}/$(date +%Y%m%d_%H%M%S)/",
  "deletion_reason": "Manual clear command",
  "can_recover": true
}
EOF
  
  # ìŠ¤í‚¤ë§ˆ ë°±ì—…
  fs schema "$feature_group" -o json > "$recovery_base/${feature_group}_schema.json"
  
  # ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
  recovery_script="$recovery_base/recover_${feature_group}.sh"
  cat << EOF > "$recovery_script"
#!/bin/bash
# ìë™ ìƒì„±ëœ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸: $feature_group
# ìƒì„±ì¼: $(date)

echo "ğŸ”„ Feature Group ë³µêµ¬ ì‹œì‘: $feature_group"

# ë³µêµ¬ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
recovery_metadata="$recovery_metadata"
if [ ! -f "\$recovery_metadata" ]; then
  echo "âŒ ë³µêµ¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
  exit 1
fi

# ë³µêµ¬ ë§Œë£Œ í™•ì¸
expires=\$(jq -r '.recovery_expires' "\$recovery_metadata")
if [ "\$(date -Iseconds)" > "\$expires" ]; then
  echo "âŒ ë³µêµ¬ ê¸°ê°„ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (ë§Œë£Œ: \$expires)"
  exit 1
fi

echo "âœ… ë³µêµ¬ ê°€ëŠ¥ ìƒíƒœ í™•ì¸ë¨"

# Feature Group ì¬ìƒì„±
echo "ğŸ—ï¸ Feature Group ì¬ìƒì„± ì¤‘..."
schema_file="\$(jq -r '.schema_backup' \$recovery_metadata)"
backup_location="\$(jq -r '.data_backup' \$recovery_metadata)"

# ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²½ë¡œ ìˆ˜ì •
full_schema_path="$recovery_base/\$schema_file"

# Feature Group ìƒì„± (ì—­í•  ARNì€ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
# fs create $feature_group \\
#   --schema-file "\$full_schema_path" \\
#   --role-arn "arn:aws:iam::ACCOUNT:role/SageMakerRole" \\
#   --s3-uri "s3://feature-store-bucket/$feature_group/"

echo "âš ï¸ Feature Group ìƒì„± ëª…ë ¹ì–´ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:"
echo "fs create $feature_group --schema-file '\$full_schema_path' --role-arn 'YOUR_ROLE_ARN' --s3-uri 'YOUR_S3_URI'"

echo "ğŸ“¦ ë°ì´í„° ë³µêµ¬ëŠ” ë°±ì—… ìœ„ì¹˜ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”:"
echo "ë°±ì—… ìœ„ì¹˜: \$backup_location"

echo "âœ… ë³µêµ¬ ì•ˆë‚´ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
EOF
  
  chmod +x "$recovery_script"
  
  echo "ğŸ“‹ ë³µêµ¬ ê°€ëŠ¥í•œ ì‚­ì œ ì¤€ë¹„ ì™„ë£Œ:"
  echo "  ë©”íƒ€ë°ì´í„°: $recovery_metadata"
  echo "  ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸: $recovery_script"
  echo "  ë³µêµ¬ ë§Œë£Œì¼: $(date -d "+$recovery_retention_days days")"
  
  # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
  echo ""
  read -p "ë³µêµ¬ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚­ì œë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): " confirmation
  if [ "$confirmation" == "yes" ]; then
    fs clear "$feature_group" \
      --backup-s3 "s3://recovery-vault/${feature_group}/$(date +%Y%m%d_%H%M%S)/" \
      --force
    
    if [ $? -eq 0 ]; then
      echo "âœ… ë³µêµ¬ ê°€ëŠ¥í•œ ì‚­ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
      echo "ğŸ”„ ë³µêµ¬ í•„ìš” ì‹œ: bash $recovery_script"
    else
      echo "âŒ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë³µêµ¬ ì‹œìŠ¤í…œì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤."
    fi
  else
    echo "âŒ ì‚­ì œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
  fi
}

setup_recoverable_clear "important-feature-group" 30
```

## ì„±ëŠ¥ ë° ì œí•œì‚¬í•­

### 1. Online Store ì‚­ì œ ì œí•œì‚¬í•­
- API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•´ ëŒ€ëŸ‰ì˜ ë ˆì½”ë“œ ì‚­ì œ ì‹œ ì‹œê°„ì´ ì†Œìš”ë¨
- ë ˆì½”ë“œë³„ ê°œë³„ ì‚­ì œ API í˜¸ì¶œ í•„ìš”
- ì‚­ì œ ì†ë„: ì´ˆë‹¹ ì•½ 100-500ê°œ ë ˆì½”ë“œ (í™˜ê²½ì— ë”°ë¼ ì°¨ì´)

### 2. Offline Store ì‚­ì œ íŠ¹ì§•
- S3 ê°ì²´ ì‚­ì œë¡œ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¦„
- ë°°ì¹˜ ì‚­ì œ ì§€ì› (1000ê°œì”©)
- íŒŒí‹°ì…˜ë³„ ì‚­ì œ ê°€ëŠ¥

### 3. ì‚­ì œ ì„±ëŠ¥ ìµœì í™”
```bash
#!/bin/bash
optimize_clear_performance() {
  local feature_group=$1
  
  echo "âš¡ ì‚­ì œ ì„±ëŠ¥ ìµœì í™” ë¶„ì„: $feature_group"
  
  # Feature Group í¬ê¸° ì¶”ì •
  fg_info=$(fs list -o json | jq --arg fg "$feature_group" \
    '.[] | select(.FeatureGroupName == $fg)')
  
  # ì¶”ì • ë ˆì½”ë“œ ìˆ˜ (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ë°©ë²• í•„ìš”)
  estimated_records=100000  # ì˜ˆì‹œê°’
  
  # ì‚­ì œ ì‹œê°„ ì˜ˆì¸¡
  online_delete_time=$((estimated_records / 200))  # ì´ˆë‹¹ 200ê°œ ê°€ì •
  offline_delete_time=30  # S3 ì‚­ì œëŠ” ë³´í†µ 30ì´ˆ ë‚´ì™¸
  
  echo "ğŸ“Š ì„±ëŠ¥ ì˜ˆì¸¡:"
  echo "  ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: $estimated_records"
  echo "  Online ì‚­ì œ ì˜ˆìƒ ì‹œê°„: $online_delete_timeì´ˆ ($(($online_delete_time / 60))ë¶„)"
  echo "  Offline ì‚­ì œ ì˜ˆìƒ ì‹œê°„: $offline_delete_timeì´ˆ"
  echo "  ì´ ì˜ˆìƒ ì‹œê°„: $((online_delete_time + offline_delete_time))ì´ˆ"
  
  # ìµœì í™” ì œì•ˆ
  echo ""
  echo "ğŸš€ ìµœì í™” ì œì•ˆ:"
  if [ $estimated_records -gt 50000 ]; then
    echo "  - ëŒ€ëŸ‰ ë°ì´í„°: offline-only ì‚­ì œ ìš°ì„  ê³ ë ¤"
    echo "  - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ê¶Œì¥"
    echo "  - ì‚­ì œ ì¤‘ ì¤‘ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ screen/tmux ì‚¬ìš©"
  fi
  
  echo "  - ì‚­ì œ ì „ ë¶ˆí•„ìš”í•œ AWS ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ìµœì†Œí™”"
  echo "  - ë‹¤ë¥¸ API ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ì—¬ rate limit ì—¬ìœ  í™•ë³´"
}

optimize_clear_performance "large-dataset-features"
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. ì‚­ì œ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
```bash
#!/bin/bash
monitor_clear_progress() {
  local feature_group=$1
  local log_file="/tmp/clear_progress_${feature_group}.log"
  
  echo "ğŸ“Š ì‚­ì œ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì‹œì‘: $feature_group"
  
  # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‚­ì œ ì‹¤í–‰
  (
    fs clear "$feature_group" --force 2>&1 | tee "$log_file"
  ) &
  
  clear_pid=$!
  
  # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
  while kill -0 $clear_pid 2>/dev/null; do
    if [ -f "$log_file" ]; then
      # ë¡œê·¸ì—ì„œ ì§„í–‰ë¥  ì •ë³´ ì¶”ì¶œ
      online_progress=$(tail -20 "$log_file" | grep -o "ì˜¨ë¼ì¸ ë ˆì½”ë“œ ì‚­ì œ ì¤‘.*%" | tail -1)
      offline_progress=$(tail -20 "$log_file" | grep -o "S3 ê°ì²´ ì‚­ì œ ì¤‘.*%" | tail -1)
      
      clear
      echo "ğŸ—‘ï¸ Feature Group ì‚­ì œ ì§„í–‰ë¥ : $feature_group"
      echo "$(date)"
      echo ""
      
      if [ -n "$online_progress" ]; then
        echo "ğŸŒ Online Store: $online_progress"
      fi
      
      if [ -n "$offline_progress" ]; then
        echo "ğŸ’¾ Offline Store: $offline_progress"
      fi
      
      # ìµœê·¼ ë¡œê·¸ ë¼ì¸ í‘œì‹œ
      echo ""
      echo "ğŸ“ ìµœê·¼ ë¡œê·¸:"
      tail -3 "$log_file" | sed 's/^/  /'
    fi
    
    sleep 5
  done
  
  # ì™„ë£Œ ìƒíƒœ í™•ì¸
  wait $clear_pid
  if [ $? -eq 0 ]; then
    echo "âœ… ì‚­ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: $feature_group"
  else
    echo "âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: $feature_group"
  fi
  
  echo "ğŸ“‹ ì „ì²´ ë¡œê·¸: $log_file"
}

monitor_clear_progress "monitoring-test-fg"
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤
```bash
# ê¶Œí•œ ì˜¤ë¥˜
âŒ AWS API ì˜¤ë¥˜: AccessDenied
# í•´ê²°: IAM ê¶Œí•œ í™•ì¸ í•„ìš”
# - sagemaker:DescribeFeatureGroup
# - sagemaker-featurestore-runtime:DeleteRecord
# - s3:DeleteObject, s3:ListBucket

# Athena ì¿¼ë¦¬ ì˜¤ë¥˜
âŒ Athenaë¥¼ í†µí•œ ë ˆì½”ë“œ ID ì¡°íšŒ ì‹¤íŒ¨: Query timeout
# í•´ê²°: ë” ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬

# S3 ì ‘ê·¼ ì˜¤ë¥˜
âŒ ì˜¤í”„ë¼ì¸ ìŠ¤í† ì–´ ì‚­ì œ ì‹¤íŒ¨: NoSuchBucket
# í•´ê²°: S3 ë²„í‚· ì¡´ì¬ ë° ê¶Œí•œ í™•ì¸
```

### 2. ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤
```bash
#!/bin/bash
emergency_recovery() {
  local feature_group=$1
  
  echo "ğŸš¨ ê¸´ê¸‰ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤: $feature_group"
  
  # ë°±ì—… ìœ„ì¹˜ í™•ì¸
  backup_locations=(
    "s3://recovery-vault/${feature_group}/"
    "s3://backup-bucket/feature-groups/${feature_group}/"
    "/tmp/${feature_group}_*"
  )
  
  echo "ğŸ” ë°±ì—… ìœ„ì¹˜ ê²€ìƒ‰ ì¤‘..."
  for location in "${backup_locations[@]}"; do
    if [[ "$location" == s3://* ]]; then
      # S3 ë°±ì—… í™•ì¸
      aws s3 ls "$location" && {
        echo "âœ… S3 ë°±ì—… ë°œê²¬: $location"
        echo "ğŸ“‹ ë³µêµ¬ ì˜µì…˜:"
        echo "  1. ìƒˆ Feature Group ìƒì„±"
        echo "  2. ë°±ì—… ë°ì´í„° ë³µì›"
        echo "  aws s3 sync $location /local/recovery/path/"
        return 0
      }
    else
      # ë¡œì»¬ ë°±ì—… í™•ì¸
      ls $location 2>/dev/null && {
        echo "âœ… ë¡œì»¬ ë°±ì—… ë°œê²¬: $location"
        return 0
      }
    fi
  done
  
  echo "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
  echo "ğŸ’¡ ë‹¤ìŒ ìœ„ì¹˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”:"
  printf '  - %s\n' "${backup_locations[@]}"
}

emergency_recovery "accidentally-deleted-fg"
```

## ëª¨ë²” ì‚¬ë¡€

1. **ë°±ì—… ìš°ì„ **: ì¤‘ìš”í•œ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ë°±ì—… í›„ ì‚­ì œ
2. **ë‹¨ê³„ì  ì ‘ê·¼**: Online â†’ Offline ìˆœì„œë¡œ ë‹¨ê³„ì  ì‚­ì œ
3. **dry-run í™œìš©**: ì‹¤ì œ ì‚­ì œ ì „ ê³„íš í™•ì¸
4. **ëª¨ë‹ˆí„°ë§**: ëŒ€ìš©ëŸ‰ ì‚­ì œ ì‹œ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
5. **ë³µêµ¬ ê³„íš**: ì‚­ì œ ì „ ë³µêµ¬ ë°©ë²• ì¤€ë¹„
6. **ê¶Œí•œ ìµœì†Œí™”**: í•„ìš”í•œ ìµœì†Œ ê¶Œí•œë§Œ ë¶€ì—¬

## ê´€ë ¨ ëª…ë ¹ì–´
- `fs create`: ìƒˆ Feature Group ìƒì„± (ë³µêµ¬ìš©)
- `fs export`: ì‚­ì œ ì „ ë°ì´í„° ë°±ì—…
- `fs analyze`: ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ í™•ì¸
- `fs list`: Feature Group ìƒíƒœ í™•ì¸