# read-parquet 명령어

Parquet 파일을 읽어서 상세 분석 정보를 JSON 형식으로 출력하는 명령어입니다. S3 경로와 로컬 파일 경로를 모두 지원합니다.

## 기본 사용법

```bash
fs read-parquet <file_path> [옵션]
```

## 매개변수

### 필수 매개변수
- `file_path`: 읽을 Parquet 파일 경로 (S3 경로 또는 로컬 파일 경로)

### 옵션
- `--save, -s`: 분석 결과를 저장할 파일 경로 (JSON 형식)

## 사용 예시

### 1. S3 Parquet 파일 읽기

```bash
# 기본 분석 (콘솔 출력)
fs read-parquet s3://my-bucket/data/records.parquet

# 분석 결과를 파일로 저장
fs read-parquet s3://my-bucket/data/records.parquet --save s3_analysis.json
```

### 2. 로컬 Parquet 파일 읽기

```bash
# 기본 분석
fs read-parquet /path/to/local/file.parquet

# 상대 경로 사용
fs read-parquet ./data/sample.parquet

# 분석 결과 저장
fs read-parquet ./data.parquet --save ./analysis_result.json
```

### 3. 다양한 저장 옵션

```bash
# 단축 옵션 사용
fs read-parquet s3://bucket/file.parquet -s analysis.json

# 특정 경로에 저장
fs read-parquet ./input.parquet --save /home/user/reports/parquet_analysis.json
```

## 출력 정보

명령어 실행 시 다음과 같은 정보를 JSON 형식으로 출력합니다:

### 1. 파일 정보 (file_info)
- 파일 경로
- 파일 크기 (MB)
- 전체 행 개수
- 전체 열 개수
- 모든 컬럼명 목록

### 2. 스키마 정보 (schema)
각 컬럼별로:
- 데이터 타입
- Null 값 개수 및 비율
- 고유값(unique) 개수
- Non-null 값 개수
- 샘플 값 (상위 3개)

### 3. Record ID 분석 (record_id_info)
- 기본키 후보 컬럼들 자동 식별
- 각 후보 컬럼의 상세 분석:
  - 고유값 개수
  - 고유값 비율
  - 기본키 가능성 판단
  - Null 값 존재 여부
  - 샘플 값들

### 4. 데이터 개요 (data_overview)
- 전체 메모리 사용량
- 중복 데이터 존재 여부 및 개수
- Null 값이 있는 컬럼 목록
- 숫자형 컬럼 목록
- 문자열 컬럼 목록
- 날짜/시간 컬럼 목록

### 5. 샘플 데이터 (sample_data)
- 처음 3개 레코드의 실제 데이터

### 6. 수치 통계 (numeric_statistics)
숫자형 컬럼들에 대한:
- 평균값 (mean)
- 표준편차 (std)
- 최솟값 (min)
- 최댓값 (max)
- 중앙값 (median)

## 출력 예시

```json
{
  "file_info": {
    "file_path": "s3://my-bucket/data/records.parquet",
    "file_size_mb": 15.7,
    "row_count": 10000,
    "column_count": 8,
    "columns": ["record_id", "user_id", "event_time", "feature1", "feature2", "status", "score", "region"]
  },
  "schema": {
    "record_id": {
      "data_type": "object",
      "null_count": 0,
      "null_percentage": 0.0,
      "unique_values": 10000,
      "non_null_count": 10000,
      "sample_values": ["rec_001", "rec_002", "rec_003"]
    },
    "score": {
      "data_type": "float64",
      "null_count": 150,
      "null_percentage": 1.5,
      "unique_values": 8850,
      "non_null_count": 9850,
      "sample_values": [85.2, 92.1, 78.5]
    }
  },
  "record_id_info": {
    "record_id_candidates": ["record_id", "user_id"],
    "details": {
      "record_id": {
        "unique_count": 10000,
        "total_count": 10000,
        "unique_ratio": 1.0,
        "is_likely_primary_key": true,
        "has_nulls": false,
        "sample_values": ["rec_001", "rec_002", "rec_003", "rec_004", "rec_005"]
      }
    }
  },
  "data_overview": {
    "total_memory_usage_mb": 15.7,
    "has_duplicates": false,
    "duplicate_count": 0,
    "columns_with_nulls": ["score", "status"],
    "all_numeric_columns": ["score"],
    "all_string_columns": ["record_id", "user_id", "status", "region"],
    "datetime_columns": ["event_time"]
  },
  "sample_data": {
    "first_3_records": [
      {
        "record_id": "rec_001",
        "user_id": "user_12345",
        "event_time": "2024-01-15T10:30:45Z",
        "feature1": "A",
        "feature2": "high",
        "status": "active",
        "score": 85.2,
        "region": "us-east-1"
      }
    ]
  },
  "numeric_statistics": {
    "score": {
      "mean": 82.5,
      "std": 12.3,
      "min": 45.1,
      "max": 99.9,
      "median": 83.2
    }
  }
}
```

## 에러 처리

### S3 관련 에러
- **NoSuchBucket**: 지정된 버킷을 찾을 수 없음
- **NoSuchKey**: 지정된 파일을 찾을 수 없음
- **AccessDenied**: 접근 권한이 없음
- **NoCredentialsError**: AWS 자격 증명을 찾을 수 없음

### 로컬 파일 관련 에러
- 파일이 존재하지 않음
- 파일 확장자가 .parquet가 아님
- 파일 읽기 권한이 없음

### Parquet 파일 관련 에러
- 손상된 Parquet 파일
- 지원하지 않는 Parquet 형식
- 메모리 부족

## 실무 사용 시나리오

### 1. Feature Store 데이터 검증
```bash
# Feature Store의 오프라인 스토어 데이터 분석
fs read-parquet s3://sagemaker-featurestore-us-east-1-123456789/offline-store/my-feature-group/ \
  --save feature_validation.json
```

### 2. 데이터 품질 체크
```bash
# 로컬 ETL 결과 파일 품질 체크
fs read-parquet ./etl_output/processed_data.parquet --save quality_report.json
```

### 3. 스키마 확인
```bash
# 새로운 데이터셋의 스키마와 샘플 데이터 확인
fs read-parquet s3://data-lake/raw/new_dataset.parquet
```

### 4. Record ID 무결성 검증
```bash
# 기본키 후보 컬럼들의 무결성 검증
fs read-parquet ./data/user_features.parquet --save record_integrity_check.json
```

## 성능 고려사항

- **큰 파일**: 매우 큰 Parquet 파일의 경우 메모리 사용량이 높을 수 있습니다
- **S3 비용**: S3 파일 읽기 시 데이터 전송 비용이 발생할 수 있습니다
- **네트워크**: S3에서 파일을 읽을 때 네트워크 속도에 따라 시간이 소요될 수 있습니다

## 권한 요구사항

### AWS 권한 (S3 파일 읽기 시)
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::your-bucket/*"
    }
  ]
}
```

### 로컬 파일 시스템
- 파일 읽기 권한 필요
- 결과 저장 시 쓰기 권한 필요

## 관련 명령어

- `fs analyze`: Feature Store 전체 분석
- `fs bulk-get`: 대량 레코드 조회
- `fs export`: 데이터 내보내기
- `fs schema`: 스키마 조회