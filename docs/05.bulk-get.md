# 5. bulk-get - ëŒ€ëŸ‰ ë ˆì½”ë“œ ì¡°íšŒ

## ê°œìš”
Feature Groupì—ì„œ ì—¬ëŸ¬ ë ˆì½”ë“œë¥¼ ì¼ê´„ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤. JSON ë˜ëŠ” CSV íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê¸°ë³¸ ì‚¬ìš©ë²•
```bash
fs bulk-get FEATURE_GROUP_NAME INPUT_FILE [OPTIONS]
```

## í•„ìˆ˜ ì¸ì
- `FEATURE_GROUP_NAME`: ì¡°íšŒí•  Feature Group ì´ë¦„
- `INPUT_FILE`: ì¡°íšŒí•  ë ˆì½”ë“œ IDë“¤ì´ í¬í•¨ëœ íŒŒì¼ (JSON ë˜ëŠ” CSV)

## ì˜µì…˜
- `--feature-names TEXT`: ì¡°íšŒí•  í”¼ì²˜ ì´ë¦„ë“¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)
- `--output-file, -o PATH`: ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
- `--current-time, -c`: Time í•„ë“œë¥¼ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ êµì²´
- `--max-workers INTEGER`: ìµœëŒ€ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 10)
- `--batch-size INTEGER`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)

## ì…ë ¥ íŒŒì¼ í˜•ì‹

### JSON í˜•ì‹ ì…ë ¥
```json
[
  {"record_id": "customer_001"},
  {"record_id": "customer_002"},
  {"record_id": "customer_003"},
  {"record_id": "customer_004"},
  {"record_id": "customer_005"}
]
```

### CSV í˜•ì‹ ì…ë ¥
```csv
record_id
customer_001
customer_002
customer_003
customer_004
customer_005
```

## ìƒì„¸ ì‚¬ìš© ì˜ˆì‹œ

### 1. ê¸°ë³¸ ëŒ€ëŸ‰ ì¡°íšŒ (JSON ì…ë ¥)
```bash
fs bulk-get customer-profile customer_ids.json
```

**ì…ë ¥ íŒŒì¼ (customer_ids.json):**
```json
[
  {"record_id": "cust_12345"},
  {"record_id": "cust_67890"},
  {"record_id": "cust_11111"},
  {"record_id": "cust_22222"}
]
```

### 2. CSV íŒŒì¼ì„ ì‚¬ìš©í•œ ì¡°íšŒ
```bash
fs bulk-get customer-profile customer_ids.csv
```

**ì…ë ¥ íŒŒì¼ (customer_ids.csv):**
```csv
record_id
cust_12345
cust_67890
cust_11111
cust_22222
```

### 3. ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
```bash
fs bulk-get customer-profile customer_ids.json --output-file results.json
```

### 4. íŠ¹ì • í”¼ì²˜ë§Œ ì¡°íšŒ
```bash
fs bulk-get customer-profile customer_ids.json \
  --feature-names "customer_id,age,balance,category" \
  --output-file filtered_results.json
```

### 5. Time í•„ë“œë¥¼ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ êµì²´
```bash
fs bulk-get real-time-features user_ids.json \
  --current-time \
  --output-file current_features.json
```

### 6. ì„±ëŠ¥ ìµœì í™” ì„¤ì •
```bash
fs bulk-get large-feature-group massive_ids.json \
  --max-workers 20 \
  --batch-size 200 \
  --output-file massive_results.json
```

## ê³ ê¸‰ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### 1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```bash
#!/bin/bash
FEATURE_GROUP="customer-analytics"
INPUT_DIR="/data/batch_requests"
OUTPUT_DIR="/data/results"
BATCH_SIZE=500
MAX_WORKERS=15

# ì—¬ëŸ¬ ì…ë ¥ íŒŒì¼ì„ ìˆœì°¨ ì²˜ë¦¬
for input_file in "$INPUT_DIR"/*.json; do
  filename=$(basename "$input_file" .json)
  output_file="$OUTPUT_DIR/${filename}_results.json"
  
  echo "Processing $input_file..."
  fs bulk-get $FEATURE_GROUP "$input_file" \
    --output-file "$output_file" \
    --batch-size $BATCH_SIZE \
    --max-workers $MAX_WORKERS \
    --feature-names "customer_id,lifetime_value,risk_score,segment"
  
  if [ $? -eq 0 ]; then
    echo "Successfully processed $filename"
  else
    echo "Failed to process $filename"
    exit 1
  fi
done
```

### 2. ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤ëƒ…ìƒ· ìƒì„±
```bash
#!/bin/bash
create_realtime_snapshot() {
  local feature_group=$1
  local user_list=$2
  local output_file=$3
  
  # í˜„ì¬ ì‹œê°„ ìŠ¤íƒ¬í”„ë¡œ íŒŒì¼ëª… ìƒì„±
  timestamp=$(date +"%Y%m%d_%H%M%S")
  snapshot_file="${output_file}_${timestamp}.json"
  
  echo "Creating real-time snapshot: $snapshot_file"
  fs bulk-get $feature_group "$user_list" \
    --current-time \
    --output-file "$snapshot_file" \
    --feature-names "user_id,session_count,last_activity,current_balance"
  
  echo "Snapshot created successfully"
}

create_realtime_snapshot "user-sessions" "active_users.json" "realtime_snapshot"
```

### 3. A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ë³„ í”¼ì²˜ ìˆ˜ì§‘
```bash
#!/bin/bash
EXPERIMENT_DIR="/experiments/ab_test_001"
FEATURE_GROUP="user-features"

# A ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘
echo "Collecting A group features..."
fs bulk-get $FEATURE_GROUP "$EXPERIMENT_DIR/group_a_users.json" \
  --output-file "$EXPERIMENT_DIR/group_a_features.json" \
  --feature-names "user_id,age,location,preference_score,engagement_rate"

# B ê·¸ë£¹ ë°ì´í„° ìˆ˜ì§‘  
echo "Collecting B group features..."
fs bulk-get $FEATURE_GROUP "$EXPERIMENT_DIR/group_b_users.json" \
  --output-file "$EXPERIMENT_DIR/group_b_features.json" \
  --feature-names "user_id,age,location,preference_score,engagement_rate"

# ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦
python << 'EOF'
import json

def validate_feature_collection(file_path, expected_features):
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    if not data:
        print(f"Warning: No data found in {file_path}")
        return False
    
    sample_record = data[0]
    missing_features = [f for f in expected_features if f not in sample_record]
    
    if missing_features:
        print(f"Missing features in {file_path}: {missing_features}")
        return False
    
    print(f"âœ“ {file_path}: {len(data)} records with all required features")
    return True

expected = ["user_id", "age", "location", "preference_score", "engagement_rate"]
validate_feature_collection("/experiments/ab_test_001/group_a_features.json", expected)
validate_feature_collection("/experiments/ab_test_001/group_b_features.json", expected)
EOF
```

### 4. ì‹œê³„ì—´ ë°ì´í„° ë°°ì¹˜ ìˆ˜ì§‘
```bash
#!/bin/bash
collect_timeseries_features() {
  local feature_group=$1
  local date_range_start=$2
  local date_range_end=$3
  
  # ë‚ ì§œë³„ ì‚¬ìš©ì ID íŒŒì¼ë“¤ ì²˜ë¦¬
  current_date=$date_range_start
  while [[ "$current_date" <= "$date_range_end" ]]; do
    date_str=$(date -d "$current_date" +"%Y-%m-%d")
    input_file="/data/daily_users/users_${date_str}.json"
    output_file="/data/features/features_${date_str}.json"
    
    if [[ -f "$input_file" ]]; then
      echo "Collecting features for $date_str..."
      fs bulk-get $feature_group "$input_file" \
        --output-file "$output_file" \
        --batch-size 300 \
        --max-workers 12 \
        --feature-names "user_id,daily_activity,transaction_count,app_usage_minutes"
      
      # ê²°ê³¼ ì••ì¶•í•˜ì—¬ ìŠ¤í† ë¦¬ì§€ ì ˆì•½
      gzip "$output_file"
      echo "âœ“ Completed and compressed: ${output_file}.gz"
    else
      echo "âš  Input file not found: $input_file"
    fi
    
    current_date=$(date -d "$current_date + 1 day" +"%Y-%m-%d")
  done
}

collect_timeseries_features "user-daily-features" "2024-01-01" "2024-01-31"
```

### 5. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ í”¼ì²˜ ìˆ˜ì§‘
```bash
#!/bin/bash
prepare_inference_features() {
  local model_name=$1
  local inference_batch_id=$2
  local input_file=$3
  
  case $model_name in
    "recommendation_model")
      feature_group="user-preferences"
      features="user_id,age,category_preferences,purchase_history,browsing_behavior"
      ;;
    "fraud_detection")
      feature_group="transaction-features" 
      features="user_id,transaction_patterns,device_fingerprint,location_history,risk_indicators"
      ;;
    "customer_lifetime_value")
      feature_group="customer-analytics"
      features="customer_id,purchase_frequency,average_order_value,tenure_months,support_interactions"
      ;;
    *)
      echo "Unknown model: $model_name"
      exit 1
      ;;
  esac
  
  output_file="/ml_inference/${model_name}/batch_${inference_batch_id}_features.json"
  
  echo "Preparing features for $model_name (batch: $inference_batch_id)..."
  fs bulk-get $feature_group "$input_file" \
    --output-file "$output_file" \
    --feature-names "$features" \
    --batch-size 250 \
    --max-workers 15 \
    --current-time
    
  # ì¶”ë¡  ì¤€ë¹„ ì™„ë£Œ ë§ˆì»¤ íŒŒì¼ ìƒì„±
  touch "/ml_inference/${model_name}/batch_${inference_batch_id}_ready.marker"
  
  echo "âœ“ Features ready for inference: $output_file"
}

# ì‚¬ìš© ì˜ˆì‹œ
prepare_inference_features "recommendation_model" "20240115_001" "users_for_recommendation.json"
prepare_inference_features "fraud_detection" "20240115_002" "suspicious_transactions.json"
```

### 6. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê³¼ í•¨ê»˜ ìˆ˜ì§‘
```bash
#!/bin/bash
bulk_get_with_validation() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  local required_features=$4
  
  # ì„ì‹œ íŒŒì¼ ìƒì„±
  temp_file="${output_file}.tmp"
  
  echo "Starting bulk get with validation..."
  fs bulk-get $feature_group "$input_file" \
    --output-file "$temp_file" \
    --feature-names "$required_features"
  
  if [ $? -ne 0 ]; then
    echo "âŒ Bulk get failed"
    rm -f "$temp_file"
    exit 1
  fi
  
  # Pythonì„ ì‚¬ìš©í•œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
  python << EOF
import json
import sys

def validate_data_quality(file_path, required_features):
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Failed to load JSON: {e}")
        return False
    
    if not data:
        print("âŒ No data returned")
        return False
    
    total_records = len(data)
    valid_records = 0
    issues = []
    
    required_features_list = required_features.split(',')
    
    for i, record in enumerate(data):
        record_valid = True
        
        # í•„ìˆ˜ í”¼ì²˜ í™•ì¸
        missing_features = [f for f in required_features_list if f not in record or record[f] is None]
        if missing_features:
            issues.append(f"Record {i}: Missing features {missing_features}")
            record_valid = False
        
        # ë¹ˆ ê°’ í™•ì¸
        empty_features = [f for f in required_features_list if f in record and str(record[f]).strip() == '']
        if empty_features:
            issues.append(f"Record {i}: Empty features {empty_features}")
            record_valid = False
            
        if record_valid:
            valid_records += 1
    
    quality_score = valid_records / total_records * 100
    
    print(f"ğŸ“Š Data Quality Report:")
    print(f"  Total records: {total_records}")
    print(f"  Valid records: {valid_records}")
    print(f"  Quality score: {quality_score:.2f}%")
    
    if issues:
        print(f"  Issues found: {len(issues)}")
        for issue in issues[:10]:  # ì²« 10ê°œ ì´ìŠˆë§Œ í‘œì‹œ
            print(f"    - {issue}")
        if len(issues) > 10:
            print(f"    ... and {len(issues) - 10} more issues")
    
    # 90% ì´ìƒ í’ˆì§ˆì¼ ë•Œë§Œ ì„±ê³µ
    return quality_score >= 90.0

if validate_data_quality("$temp_file", "$required_features"):
    print("âœ… Data quality validation passed")
    sys.exit(0)
else:
    print("âŒ Data quality validation failed")
    sys.exit(1)
EOF

  if [ $? -eq 0 ]; then
    mv "$temp_file" "$output_file"
    echo "âœ… Bulk get completed with validation: $output_file"
  else
    rm -f "$temp_file"
    echo "âŒ Data quality validation failed, output file not created"
    exit 1
  fi
}

# ì‚¬ìš© ì˜ˆì‹œ
bulk_get_with_validation "customer-profile" "customer_ids.json" "validated_customers.json" "customer_id,age,balance,category"
```

### 7. ë‹¤ì¤‘ Feature Group ë³‘ë ¬ ì¡°íšŒ
```bash
#!/bin/bash
parallel_multi_feature_group_get() {
  local user_ids_file=$1
  local output_dir=$2
  
  # Feature Group ëª©ë¡ê³¼ í•´ë‹¹ í”¼ì²˜ë“¤
  declare -A feature_groups
  feature_groups["user-profile"]="user_id,age,location,subscription_type"
  feature_groups["user-activity"]="user_id,last_login,session_count,page_views"
  feature_groups["user-preferences"]="user_id,favorite_categories,notification_settings"
  
  # ë³‘ë ¬ë¡œ ê° Feature Groupì—ì„œ ë°ì´í„° ìˆ˜ì§‘
  for fg in "${!feature_groups[@]}"; do
    output_file="$output_dir/${fg//-/_}_data.json"
    echo "Starting collection from $fg..."
    
    (
      fs bulk-get "$fg" "$user_ids_file" \
        --output-file "$output_file" \
        --feature-names "${feature_groups[$fg]}" \
        --batch-size 200 \
        --max-workers 10
      
      if [ $? -eq 0 ]; then
        echo "âœ… Completed: $fg -> $output_file"
      else
        echo "âŒ Failed: $fg"
      fi
    ) &
  done
  
  # ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
  wait
  
  # ê²°ê³¼ ë³‘í•©
  echo "Merging results..."
  python << 'EOF'
import json
import os
from pathlib import Path

output_dir = "$output_dir"
merged_data = {}

# ê° í”¼ì²˜ ê·¸ë£¹ì˜ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
for file_path in Path(output_dir).glob("*_data.json"):
    if file_path.stat().st_size == 0:
        continue
        
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        for record in data:
            user_id = record.get('user_id')
            if user_id:
                if user_id not in merged_data:
                    merged_data[user_id] = {}
                merged_data[user_id].update(record)
    except Exception as e:
        print(f"Error processing {file_path}: {e}")

# ë³‘í•©ëœ ê²°ê³¼ ì €ì¥
merged_list = list(merged_data.values())
with open(f"{output_dir}/merged_features.json", 'w') as f:
    json.dump(merged_list, f, indent=2)

print(f"âœ… Merged {len(merged_list)} user records")
EOF
  
  echo "ğŸ‰ Multi-feature group collection completed: $output_dir/merged_features.json"
}

parallel_multi_feature_group_get "user_ids.json" "/tmp/user_features"
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ í¬ê¸° ì¡°ì •
```bash
# ì‘ì€ ë ˆì½”ë“œ (< 1KB): í° ë°°ì¹˜ í¬ê¸°
fs bulk-get small-features ids.json --batch-size 500

# í° ë ˆì½”ë“œ (> 10KB): ì‘ì€ ë°°ì¹˜ í¬ê¸°  
fs bulk-get large-features ids.json --batch-size 50

# ì ì‘ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •
record_count=$(jq length input_file.json)
if [ $record_count -gt 10000 ]; then
  batch_size=200
elif [ $record_count -gt 1000 ]; then
  batch_size=300
else
  batch_size=500
fi

fs bulk-get feature-group input_file.json --batch-size $batch_size
```

### 2. ì›Œì»¤ ìˆ˜ ìµœì í™”
```bash
#!/bin/bash
# CPU ì½”ì–´ ìˆ˜ì— ê¸°ë°˜í•œ ì›Œì»¤ ìˆ˜ ì„¤ì •
cpu_cores=$(nproc)
optimal_workers=$((cpu_cores * 2))

# ìµœëŒ€ 20ê°œë¡œ ì œí•œ (API ì œí•œ ê³ ë ¤)
if [ $optimal_workers -gt 20 ]; then
  optimal_workers=20
fi

echo "Using $optimal_workers workers based on $cpu_cores CPU cores"

fs bulk-get feature-group large_input.json \
  --max-workers $optimal_workers \
  --batch-size 150
```

### 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
```bash
#!/bin/bash
process_large_file_chunked() {
  local input_file=$1
  local feature_group=$2
  local chunk_size=1000
  
  # í° íŒŒì¼ì„ ì²­í¬ë¡œ ë¶„í•  ì²˜ë¦¬
  total_records=$(jq length "$input_file")
  chunks=$(((total_records + chunk_size - 1) / chunk_size))
  
  echo "Processing $total_records records in $chunks chunks of $chunk_size"
  
  for ((i=0; i<chunks; i++)); do
    start=$((i * chunk_size))
    chunk_file="/tmp/chunk_${i}.json"
    result_file="/tmp/result_${i}.json"
    
    # ì²­í¬ ìƒì„±
    jq ".[$start:$start+$chunk_size]" "$input_file" > "$chunk_file"
    
    # ì²­í¬ ì²˜ë¦¬
    fs bulk-get "$feature_group" "$chunk_file" \
      --output-file "$result_file" \
      --batch-size 200 \
      --max-workers 10
    
    echo "Completed chunk $((i+1))/$chunks"
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    rm "$chunk_file"
  done
  
  # ê²°ê³¼ íŒŒì¼ë“¤ ë³‘í•©
  echo "Merging results..."
  jq -s 'add' /tmp/result_*.json > final_result.json
  rm /tmp/result_*.json
  
  echo "âœ… Large file processing completed: final_result.json"
}

process_large_file_chunked "huge_input.json" "feature-group"
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¬¸ì œ í•´ê²°

### 1. ë ˆì½”ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì²˜ë¦¬
```bash
#!/bin/bash
bulk_get_with_missing_handling() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # ì„ì‹œ ê²°ê³¼ íŒŒì¼
  temp_result="/tmp/bulk_get_temp.json"
  
  fs bulk-get $feature_group "$input_file" \
    --output-file "$temp_result"
  
  # ëˆ„ë½ëœ ë ˆì½”ë“œ ì‹ë³„
  python << EOF
import json

# ìš”ì²­í•œ IDë“¤ ë¡œë“œ
with open('$input_file', 'r') as f:
    requested_data = json.load(f)
requested_ids = {item.get('record_id') for item in requested_data}

# ê²°ê³¼ ë°ì´í„° ë¡œë“œ  
try:
    with open('$temp_result', 'r') as f:
        result_data = json.load(f)
    returned_ids = {item.get('record_id') for item in result_data if item.get('record_id')}
except:
    returned_ids = set()

# ëˆ„ë½ëœ IDë“¤ ì°¾ê¸°
missing_ids = requested_ids - returned_ids

if missing_ids:
    print(f"âš  Missing records: {len(missing_ids)}")
    with open('missing_records.json', 'w') as f:
        json.dump(list(missing_ids), f, indent=2)
    print("Missing record IDs saved to: missing_records.json")

print(f"âœ… Retrieved: {len(returned_ids)}/{len(requested_ids)} records")
EOF
  
  mv "$temp_result" "$output_file"
}

bulk_get_with_missing_handling "customer-profile" "customer_ids.json" "results.json"
```

### 2. ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
```bash
#!/bin/bash
bulk_get_with_retry() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  local max_retries=${4:-3}
  
  for ((attempt=1; attempt<=max_retries; attempt++)); do
    echo "Attempt $attempt/$max_retries..."
    
    if fs bulk-get $feature_group "$input_file" --output-file "$output_file"; then
      echo "âœ… Bulk get succeeded on attempt $attempt"
      return 0
    else
      echo "âŒ Attempt $attempt failed"
      if [ $attempt -eq $max_retries ]; then
        echo "ğŸ’¥ All $max_retries attempts failed"
        return 1
      fi
      
      # ì§€ìˆ˜ì  ë°±ì˜¤í”„
      sleep_time=$((2 ** attempt))
      echo "Waiting ${sleep_time}s before retry..."
      sleep $sleep_time
    fi
  done
}

bulk_get_with_retry "feature-group" "input.json" "output.json" 3
```

### 3. ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë³µêµ¬
```bash
#!/bin/bash
robust_bulk_get() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
  temp_dir=$(mktemp -d)
  
  # ì…ë ¥ íŒŒì¼ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
  split_input() {
    python << EOF
import json
import math

with open('$input_file', 'r') as f:
    data = json.load(f)

chunk_size = 100
chunks = math.ceil(len(data) / chunk_size)

for i in range(chunks):
    start = i * chunk_size
    end = min(start + chunk_size, len(data))
    chunk_data = data[start:end]
    
    with open('$temp_dir/chunk_{:03d}.json'.format(i), 'w') as f:
        json.dump(chunk_data, f)

print(f"Split into {chunks} chunks")
EOF
  }
  
  split_input
  
  # ê° ì²­í¬ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
  success_count=0
  total_chunks=$(ls "$temp_dir"/chunk_*.json | wc -l)
  
  for chunk_file in "$temp_dir"/chunk_*.json; do
    chunk_name=$(basename "$chunk_file" .json)
    result_file="$temp_dir/${chunk_name}_result.json"
    
    if bulk_get_with_retry "$feature_group" "$chunk_file" "$result_file" 3; then
      success_count=$((success_count + 1))
      echo "âœ… $chunk_name completed ($success_count/$total_chunks)"
    else
      echo "âŒ $chunk_name failed permanently"
    fi
  done
  
  # ì„±ê³µí•œ ê²°ê³¼ë“¤ ë³‘í•©
  if [ $success_count -gt 0 ]; then
    echo "Merging $success_count successful chunks..."
    jq -s 'add' "$temp_dir"/*_result.json > "$output_file"
    echo "âœ… Merged results saved to: $output_file"
  else
    echo "âŒ No chunks succeeded"
    exit 1
  fi
  
  # ì„ì‹œ íŒŒì¼ ì •ë¦¬
  rm -rf "$temp_dir"
}

robust_bulk_get "feature-group" "large_input.json" "robust_output.json"
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. ìƒì„¸ ì§„í–‰ë¥  í‘œì‹œ
```bash
#!/bin/bash
bulk_get_with_progress() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # ì´ ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
  total_records=$(jq length "$input_file")
  
  echo "ğŸ“Š Starting bulk get for $total_records records..."
  start_time=$(date +%s)
  
  # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤
  (
    while true; do
      if [[ -f "$output_file" ]]; then
        completed=$(jq length "$output_file" 2>/dev/null || echo 0)
        percentage=$(( completed * 100 / total_records ))
        echo -ne "\râ³ Progress: $completed/$total_records ($percentage%) completed..."
      fi
      sleep 2
    done
  ) &
  progress_pid=$!
  
  # ì‹¤ì œ bulk get ì‹¤í–‰
  fs bulk-get "$feature_group" "$input_file" --output-file "$output_file"
  result=$?
  
  # ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨
  kill $progress_pid 2>/dev/null
  
  end_time=$(date +%s)
  duration=$((end_time - start_time))
  
  if [ $result -eq 0 ]; then
    final_count=$(jq length "$output_file")
    throughput=$((final_count / duration))
    echo -e "\nâœ… Completed: $final_count records in ${duration}s (${throughput} records/sec)"
  else
    echo -e "\nâŒ Failed after ${duration}s"
  fi
  
  return $result
}
```

### 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```bash
#!/bin/bash
benchmark_bulk_get() {
  local feature_group=$1
  local input_file=$2
  local test_name=${3:-"default"}
  
  # ë©”íŠ¸ë¦­ íŒŒì¼ ì´ˆê¸°í™”
  metrics_file="bulk_get_metrics_${test_name}.json"
  
  echo "ğŸ” Starting benchmark: $test_name"
  
  # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
  cpu_cores=$(nproc)
  memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  input_records=$(jq length "$input_file")
  
  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  start_time=$(date +%s.%N)
  fs bulk-get "$feature_group" "$input_file" --output-file "benchmark_${test_name}.json"
  end_time=$(date +%s.%N)
  
  duration=$(echo "$end_time - $start_time" | bc)
  output_records=$(jq length "benchmark_${test_name}.json" 2>/dev/null || echo 0)
  throughput=$(echo "scale=2; $output_records / $duration" | bc)
  
  # ë©”íŠ¸ë¦­ ì €ì¥
  cat << EOF > "$metrics_file"
{
  "test_name": "$test_name",
  "timestamp": "$(date -Iseconds)",
  "system": {
    "cpu_cores": $cpu_cores,
    "memory_gb": $memory_gb
  },
  "input": {
    "feature_group": "$feature_group",
    "record_count": $input_records
  },
  "performance": {
    "duration_seconds": $duration,
    "records_retrieved": $output_records,
    "throughput_records_per_sec": $throughput,
    "success_rate": $(echo "scale=4; $output_records / $input_records" | bc)
  }
}
EOF
  
  echo "ğŸ“ˆ Benchmark completed: $metrics_file"
  echo "   Duration: ${duration}s"
  echo "   Throughput: ${throughput} records/sec"
  echo "   Success rate: $(echo "scale=2; $output_records / $input_records * 100" | bc)%"
}

# ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ë²¤ì¹˜ë§ˆí‚¹
benchmark_bulk_get "small-features" "small_ids.json" "small_records"
benchmark_bulk_get "large-features" "large_ids.json" "large_records"
```

## ëª¨ë²” ì‚¬ë¡€

1. **ì…ë ¥ íŒŒì¼ ê²€ì¦**: ì²˜ë¦¬ ì „ JSON/CSV í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬
2. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: ë ˆì½”ë“œ í¬ê¸°ì™€ ë„¤íŠ¸ì›Œí¬ ìƒí™©ì— ë§ì¶˜ ìµœì í™”
3. **ì˜¤ë¥˜ ì²˜ë¦¬**: ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¶”ì  ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
4. **ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§**: ëŒ€ìš©ëŸ‰ ì‘ì—… ì‹œ ì§„í–‰ ìƒí™© ì¶”ì 
5. **ê²°ê³¼ ê²€ì¦**: ì¶œë ¥ ë°ì´í„° í’ˆì§ˆ í™•ì¸
6. **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„ì‹œ íŒŒì¼ ì •ë¦¬

## ê´€ë ¨ ëª…ë ¹ì–´
- `fs get`: ë‹¨ì¼ ë ˆì½”ë“œ ì¡°íšŒ
- `fs bulk-put`: ëŒ€ëŸ‰ ë ˆì½”ë“œ ì €ì¥
- `fs list`: Feature Group ëª©ë¡ í™•ì¸
- `fs schema`: ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ