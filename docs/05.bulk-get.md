# 5. bulk-get - 대량 레코드 조회

## 개요
Feature Group에서 여러 레코드를 일괄적으로 조회합니다. JSON 또는 CSV 파일을 입력으로 받아 대량의 데이터를 효율적으로 조회할 수 있습니다.

## 기본 사용법
```bash
fs bulk-get FEATURE_GROUP_NAME INPUT_FILE [OPTIONS]
```

## 필수 인자
- `FEATURE_GROUP_NAME`: 조회할 Feature Group 이름
- `INPUT_FILE`: 조회할 레코드 ID들이 포함된 파일 (JSON 또는 CSV)

## 옵션
- `--feature-names TEXT`: 조회할 피처 이름들 (쉼표로 구분)
- `--output-file, -o PATH`: 결과를 저장할 파일 경로
- `--current-time, -c`: Time 필드를 현재 시간으로 교체
- `--max-workers INTEGER`: 최대 워커 수 (기본값: 10)
- `--batch-size INTEGER`: 배치 크기 (기본값: 100)

## 입력 파일 형식

### JSON 형식 입력
```json
[
  {"record_id": "customer_001"},
  {"record_id": "customer_002"},
  {"record_id": "customer_003"},
  {"record_id": "customer_004"},
  {"record_id": "customer_005"}
]
```

### CSV 형식 입력
```csv
record_id
customer_001
customer_002
customer_003
customer_004
customer_005
```

## 상세 사용 예시

### 1. 기본 대량 조회 (JSON 입력)
```bash
fs bulk-get customer-profile customer_ids.json
```

**입력 파일 (customer_ids.json):**
```json
[
  {"record_id": "cust_12345"},
  {"record_id": "cust_67890"},
  {"record_id": "cust_11111"},
  {"record_id": "cust_22222"}
]
```

### 2. CSV 파일을 사용한 조회
```bash
fs bulk-get customer-profile customer_ids.csv
```

**입력 파일 (customer_ids.csv):**
```csv
record_id
cust_12345
cust_67890
cust_11111
cust_22222
```

### 3. 결과를 파일로 저장
```bash
fs bulk-get customer-profile customer_ids.json --output-file results.json
```

### 4. 특정 피처만 조회
```bash
fs bulk-get customer-profile customer_ids.json \
  --feature-names "customer_id,age,balance,category" \
  --output-file filtered_results.json
```

### 5. Time 필드를 현재 시간으로 교체
```bash
fs bulk-get real-time-features user_ids.json \
  --current-time \
  --output-file current_features.json
```

### 6. 성능 최적화 설정
```bash
fs bulk-get large-feature-group massive_ids.json \
  --max-workers 20 \
  --batch-size 200 \
  --output-file massive_results.json
```

## 고급 사용 시나리오

### 1. 대용량 데이터 처리 파이프라인
```bash
#!/bin/bash
FEATURE_GROUP="customer-analytics"
INPUT_DIR="/data/batch_requests"
OUTPUT_DIR="/data/results"
BATCH_SIZE=500
MAX_WORKERS=15

# 여러 입력 파일을 순차 처리
for input_file in "$INPUT_DIR"/*.json; do
  filename=$(basename "$input_file" .json)
  output_file="$OUTPUT_DIR/${filename}_results.json"
  
  echo "Processing $input_file..."
  fs bulk-get $FEATURE_GROUP "$input_file" \
    --output-file "$output_file" \
    --batch-size $BATCH_SIZE \
    --max-workers $MAX_WORKERS \
    --feature-names "customer_id,lifetime_value,risk_score,segment"
  
  if [ $? -eq 0 ]; then
    echo "Successfully processed $filename"
  else
    echo "Failed to process $filename"
    exit 1
  fi
done
```

### 2. 실시간 데이터 스냅샷 생성
```bash
#!/bin/bash
create_realtime_snapshot() {
  local feature_group=$1
  local user_list=$2
  local output_file=$3
  
  # 현재 시간 스탬프로 파일명 생성
  timestamp=$(date +"%Y%m%d_%H%M%S")
  snapshot_file="${output_file}_${timestamp}.json"
  
  echo "Creating real-time snapshot: $snapshot_file"
  fs bulk-get $feature_group "$user_list" \
    --current-time \
    --output-file "$snapshot_file" \
    --feature-names "user_id,session_count,last_activity,current_balance"
  
  echo "Snapshot created successfully"
}

create_realtime_snapshot "user-sessions" "active_users.json" "realtime_snapshot"
```

### 3. A/B 테스트 그룹별 피처 수집
```bash
#!/bin/bash
EXPERIMENT_DIR="/experiments/ab_test_001"
FEATURE_GROUP="user-features"

# A 그룹 데이터 수집
echo "Collecting A group features..."
fs bulk-get $FEATURE_GROUP "$EXPERIMENT_DIR/group_a_users.json" \
  --output-file "$EXPERIMENT_DIR/group_a_features.json" \
  --feature-names "user_id,age,location,preference_score,engagement_rate"

# B 그룹 데이터 수집  
echo "Collecting B group features..."
fs bulk-get $FEATURE_GROUP "$EXPERIMENT_DIR/group_b_users.json" \
  --output-file "$EXPERIMENT_DIR/group_b_features.json" \
  --feature-names "user_id,age,location,preference_score,engagement_rate"

# 수집된 데이터 검증
python << 'EOF'
import json

def validate_feature_collection(file_path, expected_features):
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    if not data:
        print(f"Warning: No data found in {file_path}")
        return False
    
    sample_record = data[0]
    missing_features = [f for f in expected_features if f not in sample_record]
    
    if missing_features:
        print(f"Missing features in {file_path}: {missing_features}")
        return False
    
    print(f"✓ {file_path}: {len(data)} records with all required features")
    return True

expected = ["user_id", "age", "location", "preference_score", "engagement_rate"]
validate_feature_collection("/experiments/ab_test_001/group_a_features.json", expected)
validate_feature_collection("/experiments/ab_test_001/group_b_features.json", expected)
EOF
```

### 4. 시계열 데이터 배치 수집
```bash
#!/bin/bash
collect_timeseries_features() {
  local feature_group=$1
  local date_range_start=$2
  local date_range_end=$3
  
  # 날짜별 사용자 ID 파일들 처리
  current_date=$date_range_start
  while [[ "$current_date" <= "$date_range_end" ]]; do
    date_str=$(date -d "$current_date" +"%Y-%m-%d")
    input_file="/data/daily_users/users_${date_str}.json"
    output_file="/data/features/features_${date_str}.json"
    
    if [[ -f "$input_file" ]]; then
      echo "Collecting features for $date_str..."
      fs bulk-get $feature_group "$input_file" \
        --output-file "$output_file" \
        --batch-size 300 \
        --max-workers 12 \
        --feature-names "user_id,daily_activity,transaction_count,app_usage_minutes"
      
      # 결과 압축하여 스토리지 절약
      gzip "$output_file"
      echo "✓ Completed and compressed: ${output_file}.gz"
    else
      echo "⚠ Input file not found: $input_file"
    fi
    
    current_date=$(date -d "$current_date + 1 day" +"%Y-%m-%d")
  done
}

collect_timeseries_features "user-daily-features" "2024-01-01" "2024-01-31"
```

### 5. 머신러닝 모델 추론을 위한 피처 수집
```bash
#!/bin/bash
prepare_inference_features() {
  local model_name=$1
  local inference_batch_id=$2
  local input_file=$3
  
  case $model_name in
    "recommendation_model")
      feature_group="user-preferences"
      features="user_id,age,category_preferences,purchase_history,browsing_behavior"
      ;;
    "fraud_detection")
      feature_group="transaction-features" 
      features="user_id,transaction_patterns,device_fingerprint,location_history,risk_indicators"
      ;;
    "customer_lifetime_value")
      feature_group="customer-analytics"
      features="customer_id,purchase_frequency,average_order_value,tenure_months,support_interactions"
      ;;
    *)
      echo "Unknown model: $model_name"
      exit 1
      ;;
  esac
  
  output_file="/ml_inference/${model_name}/batch_${inference_batch_id}_features.json"
  
  echo "Preparing features for $model_name (batch: $inference_batch_id)..."
  fs bulk-get $feature_group "$input_file" \
    --output-file "$output_file" \
    --feature-names "$features" \
    --batch-size 250 \
    --max-workers 15 \
    --current-time
    
  # 추론 준비 완료 마커 파일 생성
  touch "/ml_inference/${model_name}/batch_${inference_batch_id}_ready.marker"
  
  echo "✓ Features ready for inference: $output_file"
}

# 사용 예시
prepare_inference_features "recommendation_model" "20240115_001" "users_for_recommendation.json"
prepare_inference_features "fraud_detection" "20240115_002" "suspicious_transactions.json"
```

### 6. 데이터 품질 검증과 함께 수집
```bash
#!/bin/bash
bulk_get_with_validation() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  local required_features=$4
  
  # 임시 파일 생성
  temp_file="${output_file}.tmp"
  
  echo "Starting bulk get with validation..."
  fs bulk-get $feature_group "$input_file" \
    --output-file "$temp_file" \
    --feature-names "$required_features"
  
  if [ $? -ne 0 ]; then
    echo "❌ Bulk get failed"
    rm -f "$temp_file"
    exit 1
  fi
  
  # Python을 사용한 데이터 품질 검증
  python << EOF
import json
import sys

def validate_data_quality(file_path, required_features):
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"❌ Failed to load JSON: {e}")
        return False
    
    if not data:
        print("❌ No data returned")
        return False
    
    total_records = len(data)
    valid_records = 0
    issues = []
    
    required_features_list = required_features.split(',')
    
    for i, record in enumerate(data):
        record_valid = True
        
        # 필수 피처 확인
        missing_features = [f for f in required_features_list if f not in record or record[f] is None]
        if missing_features:
            issues.append(f"Record {i}: Missing features {missing_features}")
            record_valid = False
        
        # 빈 값 확인
        empty_features = [f for f in required_features_list if f in record and str(record[f]).strip() == '']
        if empty_features:
            issues.append(f"Record {i}: Empty features {empty_features}")
            record_valid = False
            
        if record_valid:
            valid_records += 1
    
    quality_score = valid_records / total_records * 100
    
    print(f"📊 Data Quality Report:")
    print(f"  Total records: {total_records}")
    print(f"  Valid records: {valid_records}")
    print(f"  Quality score: {quality_score:.2f}%")
    
    if issues:
        print(f"  Issues found: {len(issues)}")
        for issue in issues[:10]:  # 첫 10개 이슈만 표시
            print(f"    - {issue}")
        if len(issues) > 10:
            print(f"    ... and {len(issues) - 10} more issues")
    
    # 90% 이상 품질일 때만 성공
    return quality_score >= 90.0

if validate_data_quality("$temp_file", "$required_features"):
    print("✅ Data quality validation passed")
    sys.exit(0)
else:
    print("❌ Data quality validation failed")
    sys.exit(1)
EOF

  if [ $? -eq 0 ]; then
    mv "$temp_file" "$output_file"
    echo "✅ Bulk get completed with validation: $output_file"
  else
    rm -f "$temp_file"
    echo "❌ Data quality validation failed, output file not created"
    exit 1
  fi
}

# 사용 예시
bulk_get_with_validation "customer-profile" "customer_ids.json" "validated_customers.json" "customer_id,age,balance,category"
```

### 7. 다중 Feature Group 병렬 조회
```bash
#!/bin/bash
parallel_multi_feature_group_get() {
  local user_ids_file=$1
  local output_dir=$2
  
  # Feature Group 목록과 해당 피처들
  declare -A feature_groups
  feature_groups["user-profile"]="user_id,age,location,subscription_type"
  feature_groups["user-activity"]="user_id,last_login,session_count,page_views"
  feature_groups["user-preferences"]="user_id,favorite_categories,notification_settings"
  
  # 병렬로 각 Feature Group에서 데이터 수집
  for fg in "${!feature_groups[@]}"; do
    output_file="$output_dir/${fg//-/_}_data.json"
    echo "Starting collection from $fg..."
    
    (
      fs bulk-get "$fg" "$user_ids_file" \
        --output-file "$output_file" \
        --feature-names "${feature_groups[$fg]}" \
        --batch-size 200 \
        --max-workers 10
      
      if [ $? -eq 0 ]; then
        echo "✅ Completed: $fg -> $output_file"
      else
        echo "❌ Failed: $fg"
      fi
    ) &
  done
  
  # 모든 백그라운드 작업 완료 대기
  wait
  
  # 결과 병합
  echo "Merging results..."
  python << 'EOF'
import json
import os
from pathlib import Path

output_dir = "$output_dir"
merged_data = {}

# 각 피처 그룹의 결과 파일 로드
for file_path in Path(output_dir).glob("*_data.json"):
    if file_path.stat().st_size == 0:
        continue
        
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        for record in data:
            user_id = record.get('user_id')
            if user_id:
                if user_id not in merged_data:
                    merged_data[user_id] = {}
                merged_data[user_id].update(record)
    except Exception as e:
        print(f"Error processing {file_path}: {e}")

# 병합된 결과 저장
merged_list = list(merged_data.values())
with open(f"{output_dir}/merged_features.json", 'w') as f:
    json.dump(merged_list, f, indent=2)

print(f"✅ Merged {len(merged_list)} user records")
EOF
  
  echo "🎉 Multi-feature group collection completed: $output_dir/merged_features.json"
}

parallel_multi_feature_group_get "user_ids.json" "/tmp/user_features"
```

## 성능 최적화

### 1. 배치 크기 조정
```bash
# 작은 레코드 (< 1KB): 큰 배치 크기
fs bulk-get small-features ids.json --batch-size 500

# 큰 레코드 (> 10KB): 작은 배치 크기  
fs bulk-get large-features ids.json --batch-size 50

# 적응적 배치 크기 설정
record_count=$(jq length input_file.json)
if [ $record_count -gt 10000 ]; then
  batch_size=200
elif [ $record_count -gt 1000 ]; then
  batch_size=300
else
  batch_size=500
fi

fs bulk-get feature-group input_file.json --batch-size $batch_size
```

### 2. 워커 수 최적화
```bash
#!/bin/bash
# CPU 코어 수에 기반한 워커 수 설정
cpu_cores=$(nproc)
optimal_workers=$((cpu_cores * 2))

# 최대 20개로 제한 (API 제한 고려)
if [ $optimal_workers -gt 20 ]; then
  optimal_workers=20
fi

echo "Using $optimal_workers workers based on $cpu_cores CPU cores"

fs bulk-get feature-group large_input.json \
  --max-workers $optimal_workers \
  --batch-size 150
```

### 3. 메모리 효율적인 처리
```bash
#!/bin/bash
process_large_file_chunked() {
  local input_file=$1
  local feature_group=$2
  local chunk_size=1000
  
  # 큰 파일을 청크로 분할 처리
  total_records=$(jq length "$input_file")
  chunks=$(((total_records + chunk_size - 1) / chunk_size))
  
  echo "Processing $total_records records in $chunks chunks of $chunk_size"
  
  for ((i=0; i<chunks; i++)); do
    start=$((i * chunk_size))
    chunk_file="/tmp/chunk_${i}.json"
    result_file="/tmp/result_${i}.json"
    
    # 청크 생성
    jq ".[$start:$start+$chunk_size]" "$input_file" > "$chunk_file"
    
    # 청크 처리
    fs bulk-get "$feature_group" "$chunk_file" \
      --output-file "$result_file" \
      --batch-size 200 \
      --max-workers 10
    
    echo "Completed chunk $((i+1))/$chunks"
    
    # 임시 파일 정리
    rm "$chunk_file"
  done
  
  # 결과 파일들 병합
  echo "Merging results..."
  jq -s 'add' /tmp/result_*.json > final_result.json
  rm /tmp/result_*.json
  
  echo "✅ Large file processing completed: final_result.json"
}

process_large_file_chunked "huge_input.json" "feature-group"
```

## 오류 처리 및 문제 해결

### 1. 레코드가 존재하지 않는 경우 처리
```bash
#!/bin/bash
bulk_get_with_missing_handling() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # 임시 결과 파일
  temp_result="/tmp/bulk_get_temp.json"
  
  fs bulk-get $feature_group "$input_file" \
    --output-file "$temp_result"
  
  # 누락된 레코드 식별
  python << EOF
import json

# 요청한 ID들 로드
with open('$input_file', 'r') as f:
    requested_data = json.load(f)
requested_ids = {item.get('record_id') for item in requested_data}

# 결과 데이터 로드  
try:
    with open('$temp_result', 'r') as f:
        result_data = json.load(f)
    returned_ids = {item.get('record_id') for item in result_data if item.get('record_id')}
except:
    returned_ids = set()

# 누락된 ID들 찾기
missing_ids = requested_ids - returned_ids

if missing_ids:
    print(f"⚠ Missing records: {len(missing_ids)}")
    with open('missing_records.json', 'w') as f:
        json.dump(list(missing_ids), f, indent=2)
    print("Missing record IDs saved to: missing_records.json")

print(f"✅ Retrieved: {len(returned_ids)}/{len(requested_ids)} records")
EOF
  
  mv "$temp_result" "$output_file"
}

bulk_get_with_missing_handling "customer-profile" "customer_ids.json" "results.json"
```

### 2. 재시도 로직 구현
```bash
#!/bin/bash
bulk_get_with_retry() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  local max_retries=${4:-3}
  
  for ((attempt=1; attempt<=max_retries; attempt++)); do
    echo "Attempt $attempt/$max_retries..."
    
    if fs bulk-get $feature_group "$input_file" --output-file "$output_file"; then
      echo "✅ Bulk get succeeded on attempt $attempt"
      return 0
    else
      echo "❌ Attempt $attempt failed"
      if [ $attempt -eq $max_retries ]; then
        echo "💥 All $max_retries attempts failed"
        return 1
      fi
      
      # 지수적 백오프
      sleep_time=$((2 ** attempt))
      echo "Waiting ${sleep_time}s before retry..."
      sleep $sleep_time
    fi
  done
}

bulk_get_with_retry "feature-group" "input.json" "output.json" 3
```

### 3. 네트워크 오류 복구
```bash
#!/bin/bash
robust_bulk_get() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # 임시 디렉토리 생성
  temp_dir=$(mktemp -d)
  
  # 입력 파일을 작은 청크로 분할
  split_input() {
    python << EOF
import json
import math

with open('$input_file', 'r') as f:
    data = json.load(f)

chunk_size = 100
chunks = math.ceil(len(data) / chunk_size)

for i in range(chunks):
    start = i * chunk_size
    end = min(start + chunk_size, len(data))
    chunk_data = data[start:end]
    
    with open('$temp_dir/chunk_{:03d}.json'.format(i), 'w') as f:
        json.dump(chunk_data, f)

print(f"Split into {chunks} chunks")
EOF
  }
  
  split_input
  
  # 각 청크를 개별적으로 처리
  success_count=0
  total_chunks=$(ls "$temp_dir"/chunk_*.json | wc -l)
  
  for chunk_file in "$temp_dir"/chunk_*.json; do
    chunk_name=$(basename "$chunk_file" .json)
    result_file="$temp_dir/${chunk_name}_result.json"
    
    if bulk_get_with_retry "$feature_group" "$chunk_file" "$result_file" 3; then
      success_count=$((success_count + 1))
      echo "✅ $chunk_name completed ($success_count/$total_chunks)"
    else
      echo "❌ $chunk_name failed permanently"
    fi
  done
  
  # 성공한 결과들 병합
  if [ $success_count -gt 0 ]; then
    echo "Merging $success_count successful chunks..."
    jq -s 'add' "$temp_dir"/*_result.json > "$output_file"
    echo "✅ Merged results saved to: $output_file"
  else
    echo "❌ No chunks succeeded"
    exit 1
  fi
  
  # 임시 파일 정리
  rm -rf "$temp_dir"
}

robust_bulk_get "feature-group" "large_input.json" "robust_output.json"
```

## 모니터링 및 로깅

### 1. 상세 진행률 표시
```bash
#!/bin/bash
bulk_get_with_progress() {
  local feature_group=$1
  local input_file=$2
  local output_file=$3
  
  # 총 레코드 수 계산
  total_records=$(jq length "$input_file")
  
  echo "📊 Starting bulk get for $total_records records..."
  start_time=$(date +%s)
  
  # 진행률 모니터링을 위한 백그라운드 프로세스
  (
    while true; do
      if [[ -f "$output_file" ]]; then
        completed=$(jq length "$output_file" 2>/dev/null || echo 0)
        percentage=$(( completed * 100 / total_records ))
        echo -ne "\r⏳ Progress: $completed/$total_records ($percentage%) completed..."
      fi
      sleep 2
    done
  ) &
  progress_pid=$!
  
  # 실제 bulk get 실행
  fs bulk-get "$feature_group" "$input_file" --output-file "$output_file"
  result=$?
  
  # 진행률 모니터링 중단
  kill $progress_pid 2>/dev/null
  
  end_time=$(date +%s)
  duration=$((end_time - start_time))
  
  if [ $result -eq 0 ]; then
    final_count=$(jq length "$output_file")
    throughput=$((final_count / duration))
    echo -e "\n✅ Completed: $final_count records in ${duration}s (${throughput} records/sec)"
  else
    echo -e "\n❌ Failed after ${duration}s"
  fi
  
  return $result
}
```

### 2. 성능 메트릭 수집
```bash
#!/bin/bash
benchmark_bulk_get() {
  local feature_group=$1
  local input_file=$2
  local test_name=${3:-"default"}
  
  # 메트릭 파일 초기화
  metrics_file="bulk_get_metrics_${test_name}.json"
  
  echo "🔍 Starting benchmark: $test_name"
  
  # 시스템 정보 수집
  cpu_cores=$(nproc)
  memory_gb=$(free -g | awk '/^Mem:/{print $2}')
  input_records=$(jq length "$input_file")
  
  # 성능 테스트 실행
  start_time=$(date +%s.%N)
  fs bulk-get "$feature_group" "$input_file" --output-file "benchmark_${test_name}.json"
  end_time=$(date +%s.%N)
  
  duration=$(echo "$end_time - $start_time" | bc)
  output_records=$(jq length "benchmark_${test_name}.json" 2>/dev/null || echo 0)
  throughput=$(echo "scale=2; $output_records / $duration" | bc)
  
  # 메트릭 저장
  cat << EOF > "$metrics_file"
{
  "test_name": "$test_name",
  "timestamp": "$(date -Iseconds)",
  "system": {
    "cpu_cores": $cpu_cores,
    "memory_gb": $memory_gb
  },
  "input": {
    "feature_group": "$feature_group",
    "record_count": $input_records
  },
  "performance": {
    "duration_seconds": $duration,
    "records_retrieved": $output_records,
    "throughput_records_per_sec": $throughput,
    "success_rate": $(echo "scale=4; $output_records / $input_records" | bc)
  }
}
EOF
  
  echo "📈 Benchmark completed: $metrics_file"
  echo "   Duration: ${duration}s"
  echo "   Throughput: ${throughput} records/sec"
  echo "   Success rate: $(echo "scale=2; $output_records / $input_records * 100" | bc)%"
}

# 다양한 시나리오로 벤치마킹
benchmark_bulk_get "small-features" "small_ids.json" "small_records"
benchmark_bulk_get "large-features" "large_ids.json" "large_records"
```

## 모범 사례

1. **입력 파일 검증**: 처리 전 JSON/CSV 형식 유효성 검사
2. **배치 크기 조정**: 레코드 크기와 네트워크 상황에 맞춘 최적화
3. **오류 처리**: 실패한 레코드 추적 및 재시도 로직 구현
4. **진행률 모니터링**: 대용량 작업 시 진행 상황 추적
5. **결과 검증**: 출력 데이터 품질 확인
6. **리소스 관리**: 메모리 사용량 및 임시 파일 정리

## 관련 명령어
- `fs get`: 단일 레코드 조회
- `fs bulk-put`: 대량 레코드 저장
- `fs list`: Feature Group 목록 확인
- `fs schema`: 스키마 정보 조회